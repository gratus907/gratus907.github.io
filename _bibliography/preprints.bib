@inproceedings{TriCL22,
  abbr      = {Under Review},
  author    = {MinGyu Choi and Wonseok Shin and Yijingxiu Lu and Sun Kim},
  title     = {{Triangular Contrastive Learning on Molecular Graphs}},
  booktitle = {Under review.},
  year      = {2022},
  arxiv     = {2205.13279}, 
  blog      = {/TriCL/},
  abstract  = {Recent contrastive learning methods have shown to be effective in various tasks, learning generalizable representations invariant to data augmentation thereby leading to state of the art performances. Regarding the multifaceted nature of large unlabeled data used in self-supervised learning while majority of real-word downstream tasks use single format of data, a multimodal framework that can train single modality to learn diverse perspectives from other modalities is an important challenge. In this paper, we propose TriCL (Triangular Contrastive Learning), a universal framework for trimodal contrastive learning. TriCL takes advantage of Triangular Area Loss, a novel intermodal contrastive loss that learns the angular geometry of the embedding space through simultaneously contrasting the area of positive and negative triplets. Systematic observation on embedding space in terms of alignment and uniformity showed that Triangular Area Loss can address the line-collapsing problem by discriminating modalities by angle. Our experimental results also demonstrate the outperformance of TriCL on downstream task of molecular property prediction which implies that the advantages of the embedding space indeed benefits the performance on downstream tasks.},
}

@inproceedings{TriSCLR22,
  abbr      = {Under Review},
  author    = {MinGyu Choi and Yijingxiu Lu and Wonseok Shin and Sun Kim},
  title     = {{Supervised Contrastive Learning for Multimodal Sentiment Analysis with Continuous Labels}},
  booktitle = {Under review.},
  year      = {2022},
  abstract  = {Understanding paraverbal and nonverbal information is a prime challenge for analyzing the sentiment of human communication. Recent methods addressed this issue by generating a synthesized representation of text, acoustics, and visual embeddings. However, these cannot be applied to general downstream tasks lacking one or mmore modalities. Moreover, dealing with continuous labels annotated to speech data is a challenge in distilling paraverbal/nonverbal information to a text encoder via supervised contrastive learning, which requires discrete labels. Here, we propose TriSCLR, which stands for Triangular Supervised Contrastive Learning framework for Regression. We devise novel RegCon loss and RegTAL, generalized NT- Xent loss and Triangular Area Loss, suitable for unimodal and trimodal contrastive regressions, respectively. State-of-the-art performances on two benchmark datasets using a single text encoder validated the effectiveness of TriSCLR for general multimodal sentiment analysis.},
}