---
layout: single
title: CNN Architecture - AlexNet (2012)
categories: deep-learning-study
parlinks: []
comments : true
---
<div id="toc">
Contents
</div>
* TOC
{:toc}
----------

AlexNet은 2012년 Imagenet challenge를 큰 격차로 우승하면서, image classification task를 통해 Deep neural network & GPU-computing의 시대를 연 모델이라는 평가를 받는 그런 아키텍쳐입니다. 이번 포스팅에서는 그런 AlexNet의 원본 논문을 따라가면서, 메인 아이디어들에 대해 살펴봅니다.

## Architecture
![picture 1](../../images/8190f2b2db0f7eb370af157be64544adbe4c13e88488ef86e8d2bf9a60d90be2.png)  

기본적으로 AlexNet의 구조는 LeNet과 많이 다르지 않습니다. 보다 큰 이미지를 처리하기 위해 좀더 깊어진 구조라고 생각할 수 있는데요. Convolution layer 5개와 Fully connected layer 2개로 구성되어 있습니다. LeNet에서 본것처럼 Convolution layer들이 주변을 보면서 feature를 추출하고, 그 추출한 특징들을 마지막 linear layer에서 어떻게 합칠지를 고민해서 하나의 결과를 낸다고 생각할 수 있겠습니다.

### 2-GPU Training
AlexNet은 몇가지 특이한 점이 있는데, 눈에 보이는 가장 큰 특징 중 하나는 위 그림에서 보듯 네트워크 전체를 두개로 나눠서 구현해놨다는 점입니다. 이는 당시 (2012) GPU 메모리가 3GB정도로 현재에 비해 부족했기 때문에 GPU 2개에 네트워크를 반씩 나눠서 돌리면서, 필요한 때만 서로간에 communicate하도록 한 것인데요. 지금에 와서는 GPU의 성능이 비약적으로 향상됨에 따라 굳이 이렇게 두개로 나누지 않아도 충분히 구현할 수 있습니다. 

### Usage of ReLU
LeNet에서도 그렇고, 이전까지의 많은 Neural network들은 activation function으로 $\tanh$ 나 sigmoid (어차피 거의 비슷하므로 sigmoid로 통칭하겠습니다) 같은 smooth한 함수를 사용했습니다. 그러나 AlexNet에서는 ReLU를 사용하면 훨씬 빠르게 training이 가능함을 주장하고 있으며, 이를 실험을 통해 확인하였습니다. 이부분에 대해서는 다양한 이야기와 생각해볼 이슈들이 있는데, ReLU는 계산 자체가 빠르게 가능한데다 양쪽에서 vanishing gradient 문제가 발생하는 sigmoid에 비해 이런 문제들이 덜하다는 장점을 직관적으로 생각해 볼 수 있습니다.

AlexNet 논문에서는 이를 non-saturating이라고 부르고 있으며, 이후 많은 논문에서도 ReLU를 사용하여 이러한 이점을 얻고자 하고 있습니다.

### Local Response Normalization
Sigmoid의 경우 각 뉴런의 입력이 0 주위로 모아져야 learning의 효율이 발휘되기 때문에 (끝쪽으로 갈수록 미분계수가 0에 가까워서 아무 일도 일어나지 않음), 많은 네트워크들이 input normalization을 통해 이를 맞춰주려고 했습니다. ReLU는 0 이하만 아니라면 입력값에 따라 미분계수가 줄어들거나 하지는 않으므로 이게 꼭 필요하지는 않지만, AlexNet 논문에서는 Local normalization이라는 방법을 적용할 때 효율이 좋았다고 합니다. 다만, 이 방법은 이후 Batch normalization 등 다양한 방법들이 제시되고 이러한 방법들의 효율이 더욱 우수함이 밝혀짐에 따라 이후의 연구에서 더이상 계승되지 않았기 때문에 자세히 다루지는 않겠습니다.

간단하게만 말하자면, convolution layer 한번이 필터를 여러개 쓰는 상황에서 적용하는 normalization입니다. 예를 들어 보자면 총 96개의 필터 (커널) 을 쓰는 상황에서 뭐 17번 필터의 결과값이 있을텐데, 이 값을 13번, 14번, ..., 21번까지의 필터의 결과값을 이용하여 normalize하는 것입니다 (좌우로 4개씩 쓰는건 그냥 임의로 정한 값입니다) 이 방법은 실제 뇌에서의 신경생리학에 있어서 측면 억제 (lateral inhibition) 로부터 motivation을 얻은 방법이라고 합니다.

### Overlapping Pooling
AlexNet에서는 일부 pooling을 서로 살짝 겹치게 수행하는데, 이 방법을 통해 overfitting을 크게 줄일 수 있다고 합니다. 

## Training
### Regularization Techniques
Regularization에 사용되는 여러 방법들에 대해서는 따로 포스팅했습니다. [Regularization methods](/deep-learning-study/regularization)을 참고해주세요 :) 

AlexNet은 이전 LeNet에 비해 훨씬 더 큰 모델로, 파라미터의 개수가 훨씬 더 많기 때문에 overfitting의 우려가 큽니다. 이에 대응하기 위해, LeNet과 비교해 보면 훨씬 Regularization에 공을 들이는 것을 알 수 있습니다.
- 2012년 당시에는 최신 테크닉이었던 (그러나, 이후에는 Batch normalization등의 활용으로 인해 그 효용이 많이 줄어든) dropout을 적용합니다. 구체적으로 fully-connected layer 중 앞 두 칸에 $p = 0.5$입니다.
- SGD에 Weight decay 0.0005를 적용합니다. 논문에서는 그 이유를 명확하게 밝히고 있지는 않으나, 단순히 regularization일 뿐 아니라 실제로 training에 반드시 필요하다고 주장하고 있습니다.

### Optimization
- SGD with Momentum 0.9, weight decay 0.0005.
- Learning rate는 0.01로 시작해서, loss가 줄어들지 않는 것 같아 보일때마다 1/10으로 줄이는 방법을 사용했습니다. 
- "Adjusted Manually"...

## Code
[AlexNet으로 CIFAR10 풀어보기](/deep-learning-study/alexnet-cifar10) 로 이어집니다