I"›W<div id="toc">
Contents
</div>
<ul id="markdown-toc">
  <li><a href="#problem--dataset" id="markdown-toc-problem--dataset">Problem / Dataset</a></li>
  <li><a href="#softmax-regression" id="markdown-toc-softmax-regression">Softmax Regression</a></li>
  <li><a href="#multi-layer-perceptron" id="markdown-toc-multi-layer-perceptron">Multi-Layer Perceptron</a></li>
</ul>
<hr />

<p><strong>ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ìˆ˜í•™ì  ê¸°ì´ˆ</strong> 5ê°•, 6ê°• (9ì›” 16ì¼, 23ì¼) ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ì´ë²ˆ ë‚´ìš©ì€ ëŒ€ë¶€ë¶„ì´ ì½”ë“œì— ëŒ€í•œ ë‚´ìš©ì´ë¼ì„œ, $\LaTeX$ ë…¸íŠ¸ë¥¼ ë³€í™˜í•˜ì§€ ì•Šê³  ì—¬ê¸°ì— ë°”ë¡œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.</p>

<p>ì•„ì§ ì½ì§€ ì•Šì•˜ë‹¤ë©´, ìµœì†Œí•œ <a href="/deep-learning-study/softmax-regression">Softmax(ë§í¬)</a>ì™€ <a href="/deep-learning-study/multilayer-perceptron">MLP(ë§í¬)</a>ì— ëŒ€í•œ í¬ìŠ¤íŒ… ì„, ë˜ë„ë¡ <a href="/deep-learning-study/">ë§í¬</a> ì— ìˆëŠ” í¬ìŠ¤íŒ… ì¤‘ shallow-nnê³¼ SVM, LRì— ëŒ€í•œ ë‚´ìš©ì„ ì½ìœ¼ë©´ ì´ë¡ ì  ë°°ê²½ì´ ì¶©ë¶„í•  ê²ƒìœ¼ë¡œ ìƒê°í•©ë‹ˆë‹¤ (ì œê°€ ì´ ë‚´ìš©ì„ ê³µë¶€í•´ì„œ ì´í•´í•œëŒ€ë¡œ ì •ë¦¬í–ˆìœ¼ë‹ˆê¹Œìš”..?)</p>

<h2 id="problem--dataset">Problem / Dataset</h2>
<p>ì´ë²ˆì— í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œëŠ”, MNISTë¼ëŠ” ë§¤ìš° ìœ ëª…í•œ ë°ì´í„°ì…‹ì„ ì´ìš©í•©ë‹ˆë‹¤. MNISTëŠ” Hand-written ìˆ«ìë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ, ë„ë¦¬ ì•Œë ¤ì§„ CNN ëª¨ë¸ì„ ì²˜ìŒ ì œì‹œí•œ LeCunì˜ ì—°êµ¬ì— ì‚¬ìš©ë˜ì—ˆë˜ ë°ì´í„°ì…‹ì´ê¸°ë„ í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ë”¥ ëŸ¬ë‹ì„ ë§Œë“¤ê¸°ì—ëŠ” ë„ˆë¬´ ì‘ì€ ë°ì´í„°ì…‹ì´ì§€ë§Œ ê³µë¶€í•˜ëŠ” ëª©ì ìœ¼ë¡œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.</p>

<p>ê° ì´ë¯¸ì§€ëŠ” 28 by 28 grayscale imageë¡œ, í¸ì˜ìƒ $\R^{28 \times 28}$ ìœ¼ë¡œ ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.</p>

<p>ê°€ì¥ ë¨¼ì € í•´ì•¼í•  ì¼ì€ pytorch moduleì„ importí•˜ê³ , ë°ì´í„°ë¥¼ ë°›ì•„ì˜¤ê³  ì •ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.<br />
Pytorchì—ì„œëŠ” <code class="language-plaintext highlighter-rouge">DataLoader</code>ë¼ëŠ” ëª¨ë“ˆì„ ì´ìš©í•˜ì—¬, í¸í•˜ê²Œ ë°ì´í„°ë¥¼ Batchë¡œ ë¨¹ì¸ë‹¤ê±°ë‚˜ í•˜ëŠ” ì‘ì—…ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="n">train_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./mnist_data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'./mnist_data/'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">device =</code> ë¶€ë¶„ì€ ê°€ëŠ¥í•˜ë‹¤ë©´ cuda GPUë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ëŠ” ë¶€ë¶„ì¸ë°, ì‚¬ì‹¤ ì´ë²ˆ íƒœìŠ¤í¬ëŠ” ë„ˆë¬´ ì‘ê¸° ë•Œë¬¸ì— GPUë¥¼ ì“°ë©´ ì´ë“ì´ ì—†ê±°ë‚˜ ì˜¤íˆë ¤ ë” ëŠë ¤ì§ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.</p>

<h2 id="softmax-regression">Softmax Regression</h2>
<p>ë¨¼ì €, ìš°ë¦¬ëŠ” Softmax regressionì„ ì‹œë„í•´ ë³´ê² ìŠµë‹ˆë‹¤. Pytorchì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.</p>
<ul>
  <li>Model ì •ì˜ : ì…ë ¥ $x$ë¥¼ ì–´ë–¤ ê³¼ì •ì„ ê±°ì³ ì¶œë ¥ê°’ìœ¼ë¡œ ë§Œë“¤ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì—ëŠ” í›ˆë ¨ê°€ëŠ¥í•œ parameterê°€ ìˆìŠµë‹ˆë‹¤.</li>
  <li>Loss function ì •ì˜ : ëª¨ë¸ì´ ì–´ë–¤ ë°©ë²•ìœ¼ë¡œ í˜„ì¬ ì •í™•ë„ë¥¼ ì¸¡ì •í• ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.</li>
  <li>í•™ìŠµ : Training dataë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.</li>
</ul>

<p>ì´ì œ, Modelì„ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. Modelì€ ê¸°ë³¸ì ìœ¼ë¡œ softmax regressionì—ì„œ ê³µë¶€í–ˆë˜ ëª¨ë¸ë¡œ, $Ax + b$ ë¥¼ 10ê°œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.</p>

<p>ì—¬ê¸°ì„œ, <code class="language-plaintext highlighter-rouge">__init__</code> ì„ ì´ìš©í•˜ì—¬ ì´ ëª¨ë¸ì—ì„œ ì“¸ Layerë“¤ì„ ì •ì˜í•˜ê³ , ê·¸ Layerë“¤ì„ <code class="language-plaintext highlighter-rouge">forward</code> ë©”ì„œë“œë¥¼ í†µí•´ ì‚¬ìš©í•´ì„œ ë„˜ê²¨ì£¼ë©´ ë©ë‹ˆë‹¤.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SoftMax</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SoftMax</span><span class="p">()</span>
</code></pre></div></div>
<p>Softmax Regressionì€ ì´ë ‡ê²Œ ì •ì˜ëœ modelì— ë‹¤ìŒê³¼ ê°™ì€ ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. 
\(\underset{a \in \R^{k \times n}, b \in \R^k}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N}  \left(-(a_{Y_i}^T X_i + b_{Y_i} + \log\left(\sum_{j = 1}^{k} e^{a_j^TX_i + b}\right)\right)\)</p>

<p>pytorchì—ëŠ” $Ax + b$ì˜ ê²°ê³¼ë§Œ ë°›ì•„ë‚´ë©´ ì´ë¥¼ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜ê°€ ìˆìœ¼ë¯€ë¡œ, ëª¨ë¸ì€ $Ax + b$ë§Œ í•´ì£¼ë©´ ë©ë‹ˆë‹¤.<br />
ëŒ€ì‹  Loss functionì„ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ <code class="language-plaintext highlighter-rouge">lr</code> ì€ learning rate, $\alpha$ ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ìµœì í™” ìì²´ì—ëŠ” Stochastic Gradient Descentë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_function</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>    
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>   
</code></pre></div></div>

<p>ì´ì œ, ìš°ë¦¬ì—ê²Œ í•„ìš”í•œ ëª¨ë¸ì˜ í›ˆë ¨ ê³¼ì •ì€ ì´ë ‡ê²Œ ì§„í–‰ë©ë‹ˆë‹¤. Epochë¥¼ ë§ì´ ëŒë¦´ìˆ˜ë¡ ì •í™•í•´ì§€ê¸´ í•˜ì§€ë§Œ, íˆ¬ì…í•˜ëŠ” ì‹œê°„ ëŒ€ë¹„ ì–¼ë§Œí¼ì˜ íš¨ìœ¨ì´ ìˆëŠ”ì§€ëŠ” ìƒí™©ë§ˆë‹¤ ë‹¤ë¥´ë¯€ë¡œ ì ë‹¹íˆ íŒë‹¨í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì›Œë‚™ ë‹¨ìˆœí•œ ëª¨ë¸ì´ë¼ í•™ìŠµí• ê²Œ ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” epoch=2 (ì¦‰, ì „ì²´ ë°ì´í„°ë¥¼ ë‘ë°”í€´ ëŒë¦½ë‹ˆë‹¤) ë§Œ ëŒë¦¬ê² ìŠµë‹ˆë‹¤.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">NUM_EPOCH</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_EPOCH</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span> <span class="p">:</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">train_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>ì´ë¥¼ ëœ¯ì–´ë³´ë©´,</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">optimizer.zero_grad()</code> ë¡œ ê¸°ì¡´ MLP ëª¨ë¸ì— ë‚¨ì•„ìˆë˜ gradient ê°’ë“¤ì„ ë‹¤ ë‚ ë¦¬ê³ </li>
  <li><code class="language-plaintext highlighter-rouge">train_loss</code> ëŠ” í˜„ì¬ ì‹œì ì— ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì„œ ì¶”ì¸¡ì„ í•´ë³´ê³  ê·¸ loss function ê°’ì„ í™•ì¸í•˜ê³ ,</li>
  <li><code class="language-plaintext highlighter-rouge">.backward()</code> ë¡œ í˜„ì¬ ì‹œì ì˜ gradientë¥¼ ê³„ì‚°í•˜ê³ </li>
  <li><code class="language-plaintext highlighter-rouge">optimizer.step()</code> ìœ¼ë¡œ ì‹¤ì œ optimization (ì—¬ê¸°ì„  SGD)ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.</li>
</ul>

<p>ê·¸ë ‡ë‹¤ë©´, ì´ ëª¨ë¸ì€ ì–¼ë§ˆë‚˜ ì •í™•í• ê¹Œìš”?</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span> <span class="p">:</span>
    <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">label</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">label</span><span class="p">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'''[Test set]</span><span class="se">\n</span><span class="s">Average loss: </span><span class="si">{</span><span class="n">test_loss</span> <span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, 
Accuracy: </span><span class="si">{</span><span class="n">correct</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span><span class="si">}</span><span class="s"> (</span><span class="si">{</span><span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%)'''</span><span class="p">)</span>
</code></pre></div></div>
<p>ì´ ì½”ë“œëŠ” í…ŒìŠ¤íŠ¸ì…‹ì„ ëª¨ë‘ í•œë°”í€´ ëŒë¦¬ë©´ì„œ, test lossì˜ ê°’ê³¼ ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.</p>

<p>ì´ˆê¸°í™” ìƒí™© ë“±ì— ë”°ë¼ ì¡°ê¸ˆ ë‹¬ë¼ì§ˆìˆ˜ëŠ” ìˆì„í…ë°, ì €ëŠ” 2ë²ˆì˜ epochë¡œ 92.11%ì˜ ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>

<h2 id="multi-layer-perceptron">Multi-Layer Perceptron</h2>
<p>PytorchëŠ” êµ‰ì¥íˆ ì“°ê¸° í¸í•œ ëª¨ë“ˆì¸ë°, ìœ„ ì½”ë“œì—ì„œ ì •ë§ <code class="language-plaintext highlighter-rouge">model</code>ë§Œ ë°”ê¾¸ë©´ ë°”ë¡œ MLPë¥¼ ì‚¬ìš©í•´ë³¼ìˆ˜ ìˆìŠµë‹ˆë‹¤. MLPë¥¼ ìˆ˜í–‰í•˜ë©´ì„œ ì ì  ê°œìˆ˜ê°€ ì¤„ì–´ë“¤ì–´ì•¼ í•˜ëŠ”ë°, ì—¬ê¸°ì„œëŠ” í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê·¸ëƒ¥ í¸ì˜ìƒ ì ì  ì¤„ì–´ë“œëŠ” ì´ìœ ê°’ì„ ëª‡ê°œ ì ì–´ë„£ê² ìŠµë‹ˆë‹¤.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,)</span> <span class="p">:</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">linear3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
    
<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>
<p>ì´ ëª¨ë¸ì€ activation functionìœ¼ë¡œ ReLUë¥¼ ì“°ëŠ” depth 4ì§œë¦¬ MLPì¸ë°, 784 -&gt; 256 -&gt; 128 -&gt; 64 -&gt; 10ìœ¼ë¡œ ë‹¨ê³„ì ìœ¼ë¡œ ê°œìˆ˜ë¥¼ ì¤„ì—¬ë‚˜ê°‘ë‹ˆë‹¤. ìˆœì„œëŒ€ë¡œ Linear-&gt;ReLU-&gt;â€¦-&gt;Linearë¡œ ëë‚©ë‹ˆë‹¤.</p>

<p>ìœ„ ì½”ë“œì—ì„œ <code class="language-plaintext highlighter-rouge">loss_function</code> ì •ì˜ë¶€í„°ëŠ” ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤. ì €ëŠ” ë¡œì»¬ì—ì„œ ì •í™•ë„ê°€ 96% ì •ë„ ë‚˜ì˜¤ê³ , ì°¸ì„ì„±ì„ ê°–ê³  Epochë¥¼ 10ìœ¼ë¡œ ë°”ê¿¨ì„ ë•ŒëŠ” 98%ì˜ ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>

<p>Q. ì™œ 4ë‹¨ê³„ layerë¥¼ (3ë‹¨ê³„, 5ë‹¨ê³„ê°€ ì•„ë‹ˆë¼â€¦) ì“°ë‚˜ìš”?<br />
-&gt; ì¼ë°˜ì ìœ¼ë¡œ Layerê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ íŒŒë¼ë¯¸í„°ê°€ ë§ì•„ì ¸ì„œ training ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³  capacityê°€ ì»¤ì§‘ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ, layerê°€ ì–•ìœ¼ë©´ non-linearityë¥¼ ì¶©ë¶„íˆ ì£¼ì§€ ëª»í•´ì„œ underfittingí•  ìš°ë ¤ê°€ ìˆìŠµë‹ˆë‹¤. 
ê·¸ë ‡ë‹¤ê³  í•´ì„œ ë¬¸ì œë¡œë¶€í„° ë°”ë¡œ ëª‡ Layerì§œë¦¬ MLPë¥¼ ì“¸ì§€ ê²°ì •í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. í•´ë³´ë‹ˆê¹Œ ì €ëŠ” 4 Layer ì •ë„ê°€ ê°€ì¥ ì ë‹¹í•´ ë³´ì˜€ìŠµë‹ˆë‹¤.</p>
:ET