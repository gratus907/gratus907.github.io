I"<div id="toc">
Contents
</div>
<ul id="markdown-toc">
  <li><a href="#motivation" id="markdown-toc-motivation">Motivation</a></li>
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#neuron-model" id="markdown-toc-neuron-model">Neuron Model</a></li>
</ul>
<hr />

<h3 id="motivation">Motivation</h3>
<ul>
  <li>Complex, nonlinear hypothesis
    <ul>
      <li>많은 수의 polynomial feature을 쓸 수도 있겠지만… 여러 개의 feature를 가진 문제에 적용하기는 어렵다.</li>
      <li>100개의 feature가 있다면? 그 이상이라면? 적절한 고차항을 쓰기는 매우 어려운 일.</li>
    </ul>
  </li>
  <li>ex) Computer Vision. <strong>이 이미지는 차량인가?</strong>
    <ul>
      <li>Pixel intensity matrix를 보고 원래의 이미지를 인식할 수 있는가?</li>
      <li>Classification problem.</li>
      <li>Feature size = 픽셀의 수 (x3 if RGB)
        <ul>
          <li>이렇게 많은 feature로는 logistic regression같은 알고리즘을 쓸 수가 없다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Goal : Algorithm that mimics brain.</li>
</ul>

<h3 id="background">Background</h3>
<ul>
  <li>80년대-90년대 초에 크게 유행했으나, 90년대 말에는 별로..</li>
  <li>Computationally expensive.</li>
  <li>현대에는 이정도 자원은 사용할 만 하다 =&gt; STATE OF THE ART</li>
  <li>뇌는 수많은 기능을 처리한다 -&gt; 이 많은 기능 (언어, 시각, 청각…) 을 각각 구현해야 하는가?</li>
  <li>NO. <code class="language-plaintext highlighter-rouge">ONE LEARNING ALGORITHM HYPOTHESIS</code>에 의하면, 실제로 뇌에서 작동하는 것은 단 하나의 학습 알고리즘.
    <ul>
      <li>청각 관련 기능을 끊고 시각 관련 부분에 이를 연결하면, 뇌가 알아서 잘 매핑해서 작동하더라.</li>
      <li>Brain rewiring experiment</li>
      <li>뇌에 다른 센서도 잘 연결하면 (direction 등) 대략 잘 작동하더라.</li>
      <li>아마도 각각의 기능은 별개의 sw가 아닐것이다.</li>
    </ul>
  </li>
  <li>Neuron : 신경계를 구성하는 기본 세포.
    <ul>
      <li>Inputs (Dendrites)</li>
      <li>Outputs (Axons)</li>
      <li>I/O를 가진 기본적인 계산 단위처럼 생각할 수 있다.</li>
    </ul>
  </li>
</ul>

<h3 id="neuron-model">Neuron Model</h3>
<ul>
  <li>Logistic Unit : $x_1, x_2, x_3$ 을 입력받아서 $h_\theta(x)$를 compute하는 neuron을 생각.</li>
  <li>Layer structure (Neural Network) : Neuron들의 output을 다시 받아서 새로운 값을 계산하는 neuron간의 layer를 쌓는 느낌.</li>
  <li>Layer 1 (Input Layer) - 맨 끝 (Output Layer) 사이에 Hidden layer들이 위치하는 구조.</li>
  <li>Bias unit 같은 추가 테크닉들 사용.</li>
  <li>$a_i^{(j)}$ : “Activation” of unit $i$ in layer $j$</li>
  <li>$\Theta^(j)$ : Matrix of weights, 다음 layer로 넘어가는 값들.  <br />
<img src="../../images/604f33448a52a8789feb7a13bf3141a4fad0ac1759963e4497c23feeffe8c7f4.png" alt="picture 1" /></li>
  <li>Forward Propagation은 Vectorize를 통해 비교적 효율적으로 연산 가능하다.</li>
  <li>이 방법이 왜 좋은가?
    <ul>
      <li>맨 끝 Layer (Output Layer) 는 일종의 Logistic regression</li>
      <li>그 이전의 Hidden layer는 그 자체가 Learning된 결과물. 즉, feature 자체가 학습을 통해 발전한다.</li>
      <li>Flexible한 방법.</li>
    </ul>
  </li>
</ul>
:ET