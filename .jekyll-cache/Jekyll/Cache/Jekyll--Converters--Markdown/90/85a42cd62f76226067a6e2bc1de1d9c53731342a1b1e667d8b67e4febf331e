I"§<div id="toc">
Contents
</div>
<ul id="markdown-toc">
  <li><a href="#linear-layer" id="markdown-toc-linear-layer">Linear Layer</a></li>
  <li><a href="#multi-layer-perceptron" id="markdown-toc-multi-layer-perceptron">Multi Layer Perceptron</a></li>
  <li><a href="#weight-initialization" id="markdown-toc-weight-initialization">Weight Initialization</a></li>
  <li><a href="#gradient-computation--back-propagation" id="markdown-toc-gradient-computation--back-propagation">Gradient Computation : Back propagation</a></li>
</ul>
<hr />

<p><strong>ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ìˆ˜í•™ì  ê¸°ì´ˆ</strong> 6ê°• (9ì›” 23ì¼) ì— ê¸°ë°˜í•©ë‹ˆë‹¤.</p>

<p>ì´ ê¸€ì€ SVMê³¼ Logistic Regression <a href="/deep-learning-study/svm-and-lr">ë§í¬</a>, Softmax Regression <a href="/deep-learning-study/softmax-regression">ë§í¬</a> ì— ì´ì–´ì§€ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.</p>

<p>ë‚˜ì¤‘ì— ì„¤ëª…ì„ ë³´ê°•í•´ì„œ ë‹¤ì‹œ ì‘ì„±ë  ì˜ˆì •ì…ë‹ˆë‹¤.</p>

<hr />

<p>Logistic regression ê°™ì€ $f_\theta(x) = a^T x + b$ caseë¥¼ 1-layer (linear layer) neural networkë¡œ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>Softmax Regression ì„¤ëª… ë§ˆì§€ë§‰ì— í–ˆë˜ ê²ƒì²˜ëŸ¼, ì ì ˆí•œ loss function $\ell$ ì„ ë„ì…í•œ ë‹¤ìŒ, $\ell(f_\theta(x), y)$ ë¥¼ ìµœì í™”í•˜ëŠ” ê²½ìš°ë¥¼ ìƒê°í•˜ì. Logistic regressionì€ ì—¬ê¸°ì— $\ell$ë¡œ logistic lossë¥¼, $f_\theta$ ìë¦¬ì— linear modelì„ ë„£ì€ íŠ¹ìˆ˜í•œ ì¼€ì´ìŠ¤ì´ë‹¤. ì´ë¥¼ ì¢€ë” ì—„ë°€í•˜ê²Œ ìƒê°í•˜ê¸° ìœ„í•´, Linear layerë¥¼ ìƒê°í•˜ì.</p>

<h3 id="linear-layer">Linear Layer</h3>
<p>ì…ë ¥ìœ¼ë¡œ $X \in \R^{B \x n}$, where $B = $ batch size, $n = $ ì…ë ¥ í¬ê¸°ë¥¼ ë°›ì•„ì„œ, ì¶œë ¥ $Y \in \R^{B \x m}$ í¬ê¸°ì˜ í…ì„œë¥¼ ì¶œë ¥í•˜ëŠ”ë°,
\(Y_{k, i} = \sum_{j = 1}^{n} A_{i, j} X_{k, j} + b_i\)
ì´ì™€ ê°™ì´ ì‘ë™í•˜ëŠ” layer ë¥¼, batchì˜ ê° ë²¡í„° $x_k$ ì— ëŒ€í•´ $y_k = A x_k + b$ í˜•íƒœì˜ ì„ í˜•ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤ëŠ” ì˜ë¯¸ì—ì„œ linear layerë¼ í•œë‹¤. ì´ë•Œ $A$ í–‰ë ¬ì„ weight, $b$ ë²¡í„°ë¥¼ biasë¼ í•œë‹¤.</p>

<p>ë”°ë¼ì„œ, Logistic Regressionì´ë€, í•˜ë‚˜ì˜ Linear layerë¥¼ ì´ìš©í•˜ê³ , loss functionìœ¼ë¡œ logistic loss (KL-divergence with logistic probability) ë¥¼ ì‚¬ìš©í•˜ëŠ” Shallow neural network ë¼ê³  ë‹¤ì‹œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.</p>

<h3 id="multi-layer-perceptron">Multi Layer Perceptron</h3>
<p>Multi-Layer (Deep Network) ë¥¼ ìƒê°í•˜ë©´, linear functionì˜ ê¹Šì€ ê²°í•©ì€ ì–´ì°¨í”¼ linearí•˜ë¯€ë¡œ ì•„ë¬´ ì˜ë¯¸ê°€ ì—†ë‹¤.</p>

<p>ê·¸ëŸ¬ë‚˜, ì ë‹¹í•œ non-linear activation function $\sigma$ ë¥¼ ë„ì…í•˜ì—¬, ë‹¤ìŒê³¼ ê°™ì€ layerë¥¼ êµ¬ì¶•í•˜ë©´ ì˜ë¯¸ê°€ ìˆê²Œ ëœë‹¤.</p>

<p><img src="../../images/ff57f23d2850f3930f519254ad5691f7b02dee930010d30ffa0e4a39b57b8d93.png" alt="picture 1" /></p>

<p>ì¦‰, ì´ë¥¼ ì‹ìœ¼ë¡œ ì“°ë©´â€¦
\(\begin{align*}
    y_L &amp;= W_L y_{L-1} + b_L \\
    y_{L - 1} &amp;= \sigma(W_{L-1} y_{L - 2} + b_{L - 1}) \\
    \cdots &amp; \cdots \\
    y_2 &amp;= \sigma (W_2 y_1 + b_2) \\
    y_1 &amp;= \sigma (W_1 x + b_1)
\end{align*}\)
where $x \in \R^{n_0}, W_l \in \R^{n_l \x n_{l-1}}, n_L = 1$. (Binary classificationë§Œ ì ê¹ ìƒê°í•˜ê¸°ë¡œ í•˜ì)</p>

<ul>
  <li>ì£¼ë¡œ $\sigma$ ë¡œëŠ” ReLU $ = \max(z, 0)$, Sigmoid $\frac{1}{1 + e^{-z}}$, Hyperbolic tangent $\frac{1 - e^{-2z}}{1 + e^{-2z}}$ ë¥¼ ì“´ë‹¤.</li>
  <li>ê´€ë¡€ì ìœ¼ë¡œ ë§ˆì§€ë§‰ layerì—ëŠ” $\sigma$ë¥¼ ë„£ì§€ ì•ŠëŠ” ê²½í–¥ì´ ìˆë‹¤.</li>
</ul>

<p>ì´ ëª¨ë¸ì„ <strong>MultiLayer Perceptron (MLP)</strong> ë˜ëŠ” Fully connected neural network ë¼ í•œë‹¤.</p>

<h3 id="weight-initialization">Weight Initialization</h3>
<p>SGD $\theta^{k + 1} = \theta^k - \alpha g^k$ ì—ì„œ, $\theta^0$ ì€ convex optimizationì—ì„œëŠ” ì–´ë–¤ ì ì„ ê³¨ë¼ë„ global solutionìœ¼ë¡œ ìˆ˜ë ´í•˜ë¯€ë¡œ ì˜ë¯¸ê°€ ì—†ì§€ë§Œ, deep learningì—ì„œëŠ” $\theta^0$ ì„ ì˜ ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ë¬¸ì œê°€ ëœë‹¤.</p>

<p>ë‹¨ìˆœí•˜ê²Œ $\theta^0 = 0$ ì„ ì“°ë©´, vanishing gradient ì˜ ë¬¸ì œê°€ ë°œìƒí•œë‹¤. Pytorchì—ì„œëŠ” ë”°ë¡œ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì´ ìˆìŒ.</p>

<h3 id="gradient-computation--back-propagation">Gradient Computation : Back propagation</h3>
<p>ë‹¤ì‹œ ì ê¹ logistic regressionì„ ìƒê°í•˜ë©´, loss functionì„ ë‹¤ ì…‹ì—…í•œ ë‹¤ìŒ ê²°êµ­ ë§ˆì§€ë§‰ì—ëŠ” stochastic gradient descent ê°™ì€ ë°©ë²•ì„ ì´ìš©í•´ì„œ ìµœì í™”í•  ê³„íšìœ¼ë¡œ ì§„í–‰í–ˆë‹¤. ê·¸ë ‡ë‹¤ëŠ” ë§ì€, ê²°êµ­ ì–´ë–»ê²Œë“  ë­”ê°€ ì € loss functionì˜ gradientë¥¼ ê³„ì‚°í•  ë°©ë²•ì´ ìˆê¸°ëŠ” í•´ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ê°€ ëœë‹¤. ì¦‰, ê° layerì˜ weightë“¤ê³¼ biasë“¤ì˜ ê° ì›ì†Œë“¤ $A_{i, j, k}$ì— ëŒ€í•´, $\pdv{y_L}{A_{i, j, k}}$ ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.</p>

<p>MLPì—ì„œëŠ” ì´ gradient ê³„ì‚°ì´ ì§ì ‘ ìˆ˜í–‰í•˜ê¸°ì—ëŠ” ë§¤ìš° ì–´ë µê¸° ë•Œë¬¸ì—, ì´ë¥¼ pytorchì—ì„œëŠ” autograd í•¨ìˆ˜ë¡œ ì œê³µí•œë‹¤. ë‹¤ë§Œ ê¸°ë³¸ì ì¸ ì›ë¦¬ëŠ” vector calculusì˜ chain ruleì— ê¸°ë°˜í•œë‹¤. ë‚˜ì¤‘ì— ì´ë¥¼ ë”°ë¡œ ë‹¤ë£¬ë‹¤.</p>

:ET