I"·<div id="toc">
Contents
</div>
<ul id="markdown-toc">
  <li><a href="#softmax-regression" id="markdown-toc-softmax-regression">Softmax Regression</a></li>
</ul>
<hr />

<p><strong>ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ìˆ˜í•™ì  ê¸°ì´ˆ</strong> 6ê°• (9ì›” 23ì¼) ì— ê¸°ë°˜í•©ë‹ˆë‹¤.</p>

<p>ì´ ê¸€ì€ SVMê³¼ Logistic Regression <a href="/deep-learning-study/svm-and-lr">ë§í¬</a> ì— ì´ì–´ì§€ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.</p>

<p>ë‚˜ì¤‘ì— ì„¤ëª…ì„ ë³´ê°•í•´ì„œ ë‹¤ì‹œ ì‘ì„±ë  ì˜ˆì •ì…ë‹ˆë‹¤.</p>

<hr />

<p>ë°ì´í„° $X_1, \dots X_n \in \mathcal{X}$ì´ ìˆê³ , ì´ì— ëŒ€í•œ ì •ë‹µ ë¼ë²¨
$Y_1, \dots Y_n \in \mathcal{Y}$ì´ ì£¼ì–´ì§„ ê²½ìš°ë¥¼ ìƒê°í•´ ë³´ì. ì´ë²ˆì—ëŠ” ê·¸ëŸ°ë°, $Y_i$ ê°€ $-1$ ê³¼ $1$ ì¤‘ì—ì„œ ê³ ë¥´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ $\Set{1, 2, \dots k}$ ì¤‘ í•˜ë‚˜ì´ë‹¤.</p>

<h2 id="softmax-regression">Softmax Regression</h2>
<p>Logistic Regressionì˜ í™•ì¥ëœ ë²„ì „ìœ¼ë¡œ, multi-class classificationì„ í•˜ê³  ì‹¶ë‹¤. ì—¬ì „íˆ empirical distributionì˜ ê°œë…ì„ ì‚¬ìš©í•œë‹¤. $\mathcal{P}(y)$ ëŠ” í¬ê¸° $k$ì˜ ë²¡í„°ë¡œ, one-hot encoding ëœ ê²ƒìœ¼ë¡œ ë³´ì.</p>

<p>Softmax í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬, $\argmax$ ë¥¼ in some sense smooth- í•œë‹¤. Define $\mu : \R^k \to \R^k$ as
\(\mu(z)_i = \frac{e^{z_i}}{\sum_{j = 1}^{k} e^{z_j}}\)</p>

<p>ì´ í•¨ìˆ˜ê°’ì˜ ëª¨ë“  index $i$ì— ëŒ€í•œ í•©ì´ 1ì´ê¸° ë•Œë¬¸ì—, $\mu(z)_i$ ë¥¼ ì¼ì¢…ì˜ confidence í™•ë¥ ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.</p>

<p>ì´ì œ, ëª¨ë¸ $\mu(f_{A, b}(x)) = \mu(Ax + b)$ ë¥¼ íƒí•˜ì. ì´ë•Œ, $x \in \R^n$ ì— ëŒ€í•´, $A$ì˜ ê° row vector ë¥¼ $a_i^T$ ë¼ í•˜ë©´, $f_{A, b}(x)$ ëŠ” ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì´ $(f_{A, b}(x))_i = (a_i^Tx + b_i)$ ì¸ í¬ê¸° $k$ì˜ ë²¡í„°ê°€ ë˜ê³ , $\mu$ ë¥¼ ë¶™ì´ë©´ ê° indexì— softmaxë¥¼ ì“´ ê²°ê³¼ê°€ ëœë‹¤. ê²°êµ­ì€ ì–´ë–¤ í–‰ë ¬ê³±ì„ í•´ì„œ ë²¡í„°ë¥¼ ì–»ì€ ë‹¤ìŒ, ê·¸ ë²¡í„°ì—ë‹¤ê°€ softmaxë¥¼ ë¶™ì¸ ì…ˆ.</p>

<p>ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•œë‹¤.
\(\underset{A \in \R^{k \x n}, b \in \R^k}{\minimize}\ \sum_{i = 1}^{N} \DKL{\mathcal{P}(Y_i)}{\mu(f_{a, b}(X_i))}\)
ì´ ì‹ì„ ì •ë¦¬í•˜ë©´, Logistic regression ë•Œì²˜ëŸ¼ ë‹¤ìŒ ë¬¸ì œì™€ ë™ì¹˜ì„ì„ ì•ˆë‹¤.
\(\underset{A \in \R^{k \x n}, b \in \R^k}{\minimize}\ \sum_{i = 1}^{N} H(\mathcal{P}(Y), \mu(f_{a, b}(X)))\)
ì´ì œ, ë‹¤ì‹œ cross entropy í•­ì„ ì „ê°œí•˜ì—¬ ì •ë¦¬í•œë‹¤.
\(\underset{A \in \R^{k \x n}, b \in \R^k}{\minimize}\ -\sum_{i = 1}^{N} \sum_{j = 1}^{k} \mathcal{P}(Y_i)_j \log (\mu(f_{A, b}(X_i))_j)\)
ì—¬ê¸°ì„œ $\mathcal{P}(Y_i)_j$ ëŠ” $j = Y_i$ ì¼ ë•Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì´ë¯€ë¡œ,
\(\underset{A \in \R^{k \x n}, b \in \R^k}{\minimize}\ -\sum_{i = 1}^{N} \log \mu(f_{A, b}(X_{i}))_{Y_i} =
\underset{A \in \R^{k \x n}, b \in \R^k}{\minimize}\ -\sum_{i = 1}^{N} \log \left(
\frac{e^{a_{Y_i}^T X_i + b_{Y_i}}}{\sum_{j = 1}^{k} e^{a_j^TX_i + b_j}}\right)\)
ì´ ì‹ì„ ì •ë¦¬í•˜ì—¬,
\(\underset{A \in \R^{k \x n}, b \in \R^k}{\minimize}\ \sum_{i = 1}^{N} \left(-(a_{Y_i}^T X_i + b_{Y_i}) + \log\left(\sum_{j = 1}^{k} e^{a_j^TX_i + b_j}\right)\right)\)</p>

<p><strong>Interesting fact</strong> : Softmax regressionì€ ì˜ ë³´ë©´ ê²°ê³¼ ì‹ì´ ì‚¬ì‹¤ convexí•˜ë‹¤. ë˜í•œ, $n = 2$ ì¼ ë•Œ, ì´ ì‹ì€ Logistic regressionê³¼ ë™ì¹˜ì´ë‹¤.</p>

<p>ì´ë¥¼ í¸í•˜ê²Œ ì“°ê¸° ìœ„í•´, Cross Entropy Loss ë¼ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.
\(\ell^{\text{CE}} (f, y) = - \log\left(\frac{e^{f_y}}{\sum_{j = 1}^{k} e^{f_j}}\right)\)
ì´ì œ, ì´ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì‰½ê²Œ Softmax Regressionì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.
\(\underset{A \in \R^{k \x n}, b \in \R^k}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell^{\text{CE}}(f_{A, b}(X_i), Y_i)\)</p>

<p>ì´ëŠ” ì¦‰, Softmax Regressionì„ ì •ì˜í•˜ëŠ” ë° ìˆì–´ì„œâ€¦</p>
<ul>
  <li>ë‹¨ìˆœí•œ Linear modelì„ Cross Entropy Lossë¡œ ìµœì í™”í•˜ê¸°</li>
  <li>Softmax-ed Linear modelì˜ KL Divergenceë¡œ ìµœì í™”í•˜ê¸°</li>
</ul>

<p>ê²°êµ­ì€ ë‘˜ì´ ê°™ì€ ë§ì´ì§€ë§Œ (CE Lossê°€ ê²°êµ­ softmaxì²˜ë¦¬í•œ í™•ë¥ ë¶„í¬ë¥¼ ê³ ë ¤í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ), ì „ìì˜ í‘œí˜„ì´ ì¢€ë” ì¼ë°˜í™”ê°€ ì‰½ë‹¤.</p>

<p>ì „ìì˜ í‘œí˜„ì„ ì´ìš©í•˜ì—¬ SRì„ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥í•˜ë©´, linear model $f_{A, b}$ ëŒ€ì‹  ì–´ë–¤ ì„ì˜ì˜ model $f_\theta(X_i)$ ì™€ì˜ cross entropy lossë¥¼ minimizeí•˜ëŠ” ê²ƒì²˜ëŸ¼ ìƒê°í•´ ë³¼ ìˆ˜ë„ ìˆë‹¤.
\(\underset{A \in \R^{k \x n}, b \in \R^k}{\minimize}\ \frac{1}{N}\sum_{i = 1}^{N} \ell^{\text{CE}}(f_{\theta}(X_i), Y_i)\)</p>

<p>ì´ëŠ” cross entropy lossê°€ ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì–´ë–¤ arg-max ìŠ¤ëŸ¬ìš´ (by softmax) choiceë¥¼ í•´ì„œ ê·¸ ê²°ê³¼ê°’ì˜ empirical distributionê³¼ì˜ KL-Divergenceë¥¼ minimizeí•˜ëŠ” ê°œë…ìœ¼ë¡œ ì ìš©ë˜ê¸° ë•Œë¬¸.</p>
:ET