I":<div id="toc">
Contents
</div>
<ul id="markdown-toc">
  <li><a href="#gradient-descent" id="markdown-toc-gradient-descent">Gradient Descent</a></li>
  <li><a href="#stochastic-gradient-descent" id="markdown-toc-stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
</ul>
<hr />

<ul>
  <li>많은 딥러닝 문헌에서 $x^i$를 $i$번째 데이터를 의미하는데 쓰고, $x_i$는 $x$벡터의 $i$번째 인덱스를 의미하는데 씁니다. 그러나 이렇게하면 제곱이랑 헷갈린다는 문제가 있어서, 여기서는 최대한 $i$번째 데이터를 $x_i$로 쓰고, 그 벡터의 원소는 $x_i(j)$ 로 쓰겠습니다.</li>
</ul>

<p>딥러닝의 문제는 수학적 관점에서 환원하면 결국 다음과 같이 요약할 수 있습니다.</p>
<ul>
  <li>미지의 함수 $f$에 대해 알고자 하는데,</li>
  <li>모든 지점이 아닌 어떤 지점 $x_i$ 들에서만 그 값 $f(x_i) = y_i$ 를 알고 있고,</li>
  <li>그래서 어떤 페널티 $\ell$ 을 정의해서, $\sum_i \ell(f(x_i), g(x_i))$가 작은 $g$를 $f$의 근사-함수로 생각하고 싶습니다.</li>
  <li>그런데 이 $g$를 모든 함수의 공간에서 최적화하는 것은 일반적으로 가능하지 않으므로,</li>
  <li>어떤 parameter $\theta$ 에 의해 표현되는 함수공간의 부분집합 $g_\theta$만을 생각하며,</li>
  <li>$\minimize \sum_i \ell(f(x_i), g_\theta(x_i))$ by moving $\theta$로 생각합니다.</li>
</ul>

<h2 id="gradient-descent">Gradient Descent</h2>

<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
:ET