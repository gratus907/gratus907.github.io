I"Í<div id="toc">
Contents
</div>
<ul id="markdown-toc">
  <li><a href="#linear-regression" id="markdown-toc-linear-regression">Linear Regression</a></li>
  <li><a href="#cost-funtion" id="markdown-toc-cost-funtion">Cost funtion</a></li>
  <li><a href="#gradient-descent" id="markdown-toc-gradient-descent">Gradient Descent</a></li>
  <li><a href="#gradient-descent-on-linear-regression" id="markdown-toc-gradient-descent-on-linear-regression">Gradient Descent on Linear Regression</a></li>
</ul>
<hr />

<p>ê°•ì˜ ê¸°ì¤€ 1ì£¼ì°¨ Linear regression - Gradient Descent.</p>

<h3 id="linear-regression">Linear Regression</h3>
<ul>
  <li>Regression Problemì— ëŒ€í•œ <strong>Linear Fitting</strong></li>
  <li>$m$ê°œì˜ <strong>Training data</strong> ë“¤ì´ $(x_i, y_i)$ í˜•íƒœë¡œ ì£¼ì–´ì§€ê³ , ì´ dataë¡œë¶€í„° <strong>best-predicting line</strong> ì°¾ê¸°.</li>
  <li>Learning Algorithmì„ ë§Œë“¤ì–´ì„œ, hypothesis $h$ ë¥¼ ë§Œë“ ë‹¤. ì´ $h$ëŠ”, $x$ë¥¼ ë°›ì•„ì„œ Estimated value $y$ë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜.</li>
  <li>Hypothesisë¡œ Linear function $y = \theta_0 + \theta_1 x$ ë¥¼ ì“°ëŠ” ê²ƒì´ <code class="language-plaintext highlighter-rouge">Linear Regression</code>.</li>
  <li>ì´ Parameterë¥¼ ì–´ë–»ê²Œ ê²°ì •í•  ê²ƒì¸ê°€? $h_\theta (x)$ê°€ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” $m$ê°œì˜ dataë“¤ì— ê°€ê¹ê²Œ í•˜ê³  ì‹¶ë‹¤.</li>
</ul>

<h3 id="cost-funtion">Cost funtion</h3>
<ul>
  <li>Minimizeí•  <strong>ëª©í‘œ</strong> ë¥¼ ì¡ì•„ì„œ parameterë¥¼ ë§ì¶”ë ¤ê³  í•œë‹¤.
\(\text{minimize }_{\theta_0, \theta_1} \quad \frac{1}{2m}\sum_{i = 1}^{m} \ (h_\theta(x_i) - y_i)^2\)</li>
  <li>ê° Data pointê¹Œì§€ì˜ ì œê³± ê±°ë¦¬ì˜ í‰ê· ì„ minimizeí•˜ë ¤ëŠ” ëª©í‘œ.</li>
  <li>ì´ í•¨ìˆ˜ë¥¼ $J(\theta_0, \theta_1)$ ë¡œ ì¨ì„œ, $\text{minimize }_{\theta_0, \theta_1} \ J(\theta_0, \theta_1)$ ì²˜ëŸ¼ ì“°ê³  square error functionì´ë¼ê³  ë¶€ë¥´ì.</li>
  <li>Why square? ê°€ì¥ ì¼ë°˜ì ì¸ regressionì—ì„œì˜ reasonable choice.</li>
  <li>ê²°êµ­ì€ ì´ë³€ìˆ˜í•¨ìˆ˜ $J(\theta_0, \theta_1)$ì˜ Minimumì„ ì–»ëŠ” $\theta_0, \theta_1$ì„ ì°¾ëŠ” ë¬¸ì œ.</li>
</ul>

<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
  <li>ì–´ë–»ê²Œ ê·¸ <strong>minimum</strong> ì„ ì°¾ì„ ê²ƒì¸ê°€?</li>
  <li>ì‘ì€ stepì„ ë°Ÿë˜, <strong>ì§€ê¸ˆ ì´ ìˆœê°„ ê°€ì¥ ê°€íŒŒë¥¸ ê²½ì‚¬ë¥¼ íƒ€ê³  ë‚´ë ¤ê°ˆ ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ</strong> ì¡°ê¸ˆì”© ë‚´ë ¤ê°„ë‹¤ë©´ ë‚˜ë¦„ëŒ€ë¡œ ë¹ ë¥´ê²Œ local minimumìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?</li>
  <li>ì´ëŸ° ë°©ë²•ìœ¼ë¡œ ë‹¤ë³€ìˆ˜í•¨ìˆ˜ì˜ gradientë¥¼ íƒ€ê³  ê°€ëŠ” ê²ƒì´ <strong>Gradient Descent</strong>.</li>
  <li>ì´ˆê¸°ì ì´ ì•„ì£¼ ì‚´ì§ ë‹¬ë¼ì¡Œì„ ë•Œ, gradientë¥¼ íƒ€ê³  ê°€ë‹¤ ë³´ë©´ ë‹¤ë¥¸ local optimumì— ë„ì°©í•  ìˆ˜ë„ ìˆë‹¤.</li>
</ul>

<p>ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.</p>

<p>\(\theta_j := \theta_j - \alpha \pdv{}{\theta_j} J(\theta_0, \theta_1)\)
(ì‹¤ì œë¡œëŠ”, ì´ë¥¼ <strong>simulatneous update</strong> ë¡œ êµ¬í˜„í•´ì•¼ í•œë‹¤.)</p>

<ul>
  <li>$\alpha$ ëŠ” <strong>Learning rate</strong> ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ, $\alpha$ ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ gradient descent ìì²´ê°€ ëŠë¦¬ê²Œ ìˆ˜ë ´í•  ê²ƒì´ê³ , $\alpha$ê°€ ë„ˆë¬´ í¬ë©´ overshoot (minimumì„ ì§€ë‚˜ì³ì„œ ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” ê²½ìš°) ë°œìƒ ê°€ëŠ¥ì„±ì´ ìˆë‹¤.</li>
</ul>

<h3 id="gradient-descent-on-linear-regression">Gradient Descent on Linear Regression</h3>
<ul>
  <li>Linear Regression ë¬¸ì œì— Gradient Descentë¥¼ ì ìš©í•´ ë³´ì.</li>
</ul>

\[\begin{aligned} \pdv{}{\theta_0}  \frac{1}{2m}\sum_{i = 1}^{m} \ (\theta_0 + \theta_1 x_i - y_i)^2 &amp;= \frac{1}{m} \sum_{i = 1}^{m} (\theta_0 + \theta_1 x_i - y_i)\\ \pdv{}{\theta_1}  \frac{1}{2m}\sum_{i = 1}^{m} \ (\theta_0 + \theta_1 x_i - y_i)^2 &amp;= \frac{1}{m} \sum_{i = 1}^{m} (\theta_0 + \theta_1 x_i - y_i) \cdot x_i \end{aligned}\]

<p>ì´ë¥¼ ê·¸ëƒ¥ ì ìš©í•´ì„œ simulatneous update í•˜ë©´ ë.</p>

<ul>
  <li>Linear Regressionì€ <strong>convex objective function</strong>ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, global optima ì™¸ì˜ local optimaë¥¼ ê°–ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ, Gradient descentê°€ local optimaë¡œ ë¹ ì§€ì§€ ì•Šê³  í•­ìƒ ì˜ ì‘ë™í•œë‹¤.</li>
  <li>ì´ ë¬¸ì œì˜ ê²½ìš°, ëª¨ë“  training set ì„ ì´ìš©í•´ì„œ í•œ ìŠ¤í…ì˜ gradient descentë¥¼ ë°Ÿì•˜ëŠ”ë° ì´ë¥¼ <strong>Batch</strong> Gradient Descent ë¼ê³  ë¶€ë¥¸ë‹¤. í•œ ìŠ¤í…œì—ì„œ ëª¨ë“  test setì„ í†µì§¸ë¡œ í™•ì¸í•œë‹¤ëŠ” ì˜ë¯¸.</li>
</ul>
:ET