<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-12-22T10:09:32+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Gratus907’s Study Note</title><subtitle>Portfolio / Study note by Wonseok Shin (Gratus907), SNU CSE
</subtitle><entry><title type="html">Principal Component Analysis (주 성분 분석)</title><link href="http://localhost:4000/deep-learning-study/principal-component-analysis/" rel="alternate" type="text/html" title="Principal Component Analysis (주 성분 분석)" /><published>2021-12-22T00:00:00+09:00</published><updated>2021-12-22T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning-study/principal-component-analysis</id><content type="html" xml:base="http://localhost:4000/deep-learning-study/principal-component-analysis/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#principal-component-analysis&quot; id=&quot;markdown-toc-principal-component-analysis&quot;&gt;Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#변동성의-설명&quot; id=&quot;markdown-toc-변동성의-설명&quot;&gt;변동성의 설명&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#분산-최대화-vs-오차-최소화&quot; id=&quot;markdown-toc-분산-최대화-vs-오차-최소화&quot;&gt;분산 최대화 vs 오차 최소화&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pca-계산과정-유도&quot; id=&quot;markdown-toc-pca-계산과정-유도&quot;&gt;PCA 계산과정 유도&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#code-space가-다차원인-경우&quot; id=&quot;markdown-toc-code-space가-다차원인-경우&quot;&gt;Code space가 다차원인 경우&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;오늘 알아볼 주제는 루트에서 아주 약간 벗어나서, 딥 러닝 시대 이전의 머신 러닝 알고리즘을 하나 보려고 합니다.&lt;/p&gt;

&lt;p&gt;Goodfellow, Bengio, Courville의 Deep learning book 에서도 이 알고리즘을 거의 맨 앞에 다루고 있는데, 사실 그 유도 방법이 수식이 꽤 많고 복잡합니다. 아마도 선형대수와 벡터미적분의 이론을 최대한 덜 휘두르면서 나아가려다 보니 그런 것 같은데, 이미 한학기의 선형대수와 벡터미적분학 수업을 들었다면 굳이 그렇게 힘든길을 걷지 않아도 같은 결론에 도달할 수 있을 뿐 아니라 색다른 insight를 제공하기 때문에 소개하려 합니다.&lt;/p&gt;

&lt;p&gt;이 글의 유도 방법은 wikipedia에서의 유도와 가장 비슷하며 (앞에서 좀 다릅니다), 후반부는 Goodfellow et al.의 책 5장의 방법을 따라갔습니다.&lt;/p&gt;

&lt;h2 id=&quot;principal-component-analysis&quot;&gt;Principal Component Analysis&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Principal Component Analysis (주성분 분석)&lt;/strong&gt;, 이하 PCA 는 다음과 같은 문제를 해결합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;어떤 $n$차원 데이터 데이터 $x_1, \dots x_m$이 주어졌을 때, 이들을 적절히 &lt;strong&gt;손실있는 압축&lt;/strong&gt; 함수 $f$ 와 그 decoder $g$ 를 생각해서 
$y_i = f(x_i)$, $x_i \approx g(y_i) = g(f(x_i))$ 이게 하고 싶습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그런데, 여기서 가급적 결과물 $f(x_i)$가 좋은 성질을 갖기를 원합니다. 좋은 성질이란,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;데이터의 원본이 갖는 &lt;strong&gt;변동성&lt;/strong&gt; 을 최대한 많이 설명하고 싶고&lt;/li&gt;
  &lt;li&gt;계산하기 쉬웠으면 좋겠습니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이를 위해서 새로운 parameter로 표현되는 mlp, cnn 등 뉴럴 네트워크로 $f, g$를 만들어서 학습시키는 방법을 &lt;strong&gt;autoencoder&lt;/strong&gt; 라고 합니다. Autoencoder에 대해서는 &lt;a href=&quot;/deep-learning-study/autoencoders&quot;&gt;포스팅 링크&lt;/a&gt; 에 소개한 적이 있는데, 오늘 알아볼 방법은 좀더 근본있게(?) 수학적으로 논증을 전개합니다..&lt;/p&gt;

&lt;h2 id=&quot;변동성의-설명&quot;&gt;변동성의 설명&lt;/h2&gt;
&lt;p&gt;이 방법에 대한 설명은 다양하게 있었지만, 저는&lt;/p&gt;

&lt;p&gt;미지의 데이터들이 주어졌을 때, 이들을 어떤 &lt;strong&gt;타원체&lt;/strong&gt; 로 이해하려고 생각한다고 가정해 봅시다. 타원체는 그 축이 되는 선분들을 모두 찾으면 그것으로 충분하며, 다시 말해 &lt;strong&gt;중심&lt;/strong&gt; 과 &lt;strong&gt;축의 방향, 길이&lt;/strong&gt; 들을 찾아야 합니다. 중심은 뭔가 당연하게 모든 데이터의 벡터로서의 평균이어야 할것 같습니다. 예를 들어 아래 그림에서와 같이 찾으면 뭔가 좋은 설명이 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/f2777e975bbd3a0496d4f94bb0835d5ac6be6d8d7dfd2db80bf2a1428590be15.png&quot; width=&quot;40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이때, 우리가 데이터를 설명하는 &lt;strong&gt;단 하나의 축&lt;/strong&gt; 을 골라야 한다면, 모든 데이터를 그 축에 대해 정사영했을 때 결과물이 최대한 원본 데이터의 성질을 잘 설명해 주는 축을 고르는 편이 바람직할 것입니다. 이를 수식으로 설명하자면, 다음을 최소화하면 좋겠습니다. 
\(\sum_{i = 1}^{n} \norm{x_i - y_i}^2\)
여기서 제곱을 이용하기 때문에 PCA를 $L^2$ norm을 사용한다고 말합니다. (직관적으로는 거리를 최소화하는게 자연스럽겠지만, 거리의 제곱의 합도 뭔가 최소화되면 좋을것 같으므로 이부분의 정당성은 넘어가겠습니다. 사실 통계학적 방법으로 PCA를 유도하면 제곱이어야 하는 이유가 좀더 자연스럽습니다) 당연히 $L^1$이나 다른 norm을 사용할 수도 있겠으나, 그렇게 하게 되면 이후 계산과정이 달라지고 다른 알고리즘이 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;분산-최대화-vs-오차-최소화&quot;&gt;분산 최대화 vs 오차 최소화&lt;/h3&gt;
&lt;p&gt;먼저 대상이 되는 차원이 1차원인, 즉 $y$가 1차원인 문제를 풀어 봅시다. 어떤 데이터 $x_i$를 축 $w$에 정사영한 결과 $y_i$를 생각할 때, 정사영의 기하적인 성질을 생각하면 $y_i$와 $x_i - y_i$가 수직이므로 피타고라스 정리에 의해, 다음이 성립하게 됩니다. 
\(\norm{x_i}^2 = \norm{y_i}^2 + \norm{x_i - y_i}^2\)&lt;/p&gt;

&lt;p&gt;그런데 $\norm{x_i}^2$는 이미 주어진 데이터이므로 사실 상수입니다. 따라서, 앞서 오차를 최소화한다는 것은 사실 $\norm{y_i}^2$을 최대화하는 것과 같은 문제를 풀고 있는 셈이 됩니다.&lt;/p&gt;

&lt;p&gt;편의상 모든 데이터를 평행이동해서, 평균이 0이 되게 맞추었다고 생각하겠습니다. 또한 편의상 $w$의 방향만 정하면 되므로 $w$를 unit vector가 되도록 강제하겠습니다.&lt;/p&gt;

&lt;p&gt;$y_i$라는 값이 정사영 대상축 $w$에 대해, $(x \cdot w) w$를 만족하므로, 이제 $\norm{y_i}^2$ 는 $(\mathbf{x_i} \cdot \mathbf{w})^2$ 가 됩니다. 따라서, 우리는 다음과 같은 문제를 해결하는 셈이 됩니다.
\(\underset{\norm{\mathbf{w}} = 1}{\maximize}\ \sum_{i = 1}^{n} (\mathbf{x_i} \cdot \mathbf{w})^2\)&lt;/p&gt;

&lt;p&gt;이 문제를 다르게 해석하는 방법은, $y$값들의 &lt;strong&gt;분산&lt;/strong&gt;을 최대화한다고 보는 관점입니다. 고등학교 때 배운 공식 
\(\variance{X} = \expect{X^2} - \expect{X}^2\) 
위 공식에서, 사실 $\expect{X}$ 가 0이므로 앞부분만 계산하면 되므로… 결국 우리는 $y_i$들의 분산을 최대화하고 있는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;pca-계산과정-유도&quot;&gt;PCA 계산과정 유도&lt;/h3&gt;
&lt;p&gt;우리는 잘 formulate된 최적화 문제를 들고 있으므로, 이를 해결해 보고자 합니다. 
\(\underset{\norm{\mathbf{w}} = 1}{\maximize}\ \sum_{i = 1}^{n} (\mathbf{x_i} \cdot \mathbf{w})^2\)
식을 좀 조작하면, matrix form으로 한번에 $x_i$들을 묶어서 쓸 수 있습니다. 
\(\underset{\norm{\mathbf{w}} = 1}{\maximize}\  \norm{X \mathbf{w}}^2 = \underset{\norm{\mathbf{w}} = 1}{\maximize}\  w X^T X w\)
이제, $X^T X$라는 행렬을 생각해 보면, 이 행렬은 $X$가 각 데이터를 행으로 들고있는 행렬이므로 $m \times n$ 행렬이고, $X^T X$는 $n \times n$ 행렬입니다. 만약 이 행렬이 가역행렬이 아니라면 애당초 데이터 행렬 $X$의 rank가 $n$이 채 안된다는 의미이므로 그 데이터가 (선형 종속성을 제거했을때) $n$차원이 아니었다는 말이 됩니다. 따라서, 편의상 이를 가역행렬로 두더라도 의미가 크게 손상되지 않습니다. 가역행렬이면서 대칭행렬인 $X^T X$는 orthonormal eigenbasis를 가진다는 사실이 잘 알려져 있으므로, 고윳값(eigenvalue) $\lambda_1, \dots \lambda_n$과 그 고유벡터 (eigenvector) $v_1, \dots v_n$에 대하여, 일반성을 잃지 않고 $\lambda_i$가 큰 것부터 나열되어 있다고 하면 $w = \sum c_i v_i, \sum \abs{c_i}^2 = 1$ 에 대해
\(\underset{\norm{\mathbf{c}} = 1}{\maximize}\ \sum \abs{c_i}^2 \lambda_i\)&lt;/p&gt;

&lt;p&gt;(원래는 $\abs{\lambda_i}$ 를 이용해서 논증해야 하지만, $X^T X$ 형태의 행렬은 언제나 positive definite하므로 상관 없습니다)&lt;/p&gt;

&lt;p&gt;이 값의 최대는 결국 당연히 $w = v_1$ 일 때 $\lambda_1$이 최대가 됩니다. 결국, 이렇게 얻은 최적화 문제의 해가 의미하는 바는&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;데이터행렬 $X$로부터 &lt;strong&gt;최대한의 정보를 뽑아내는&lt;/strong&gt; 축 $w$는 $X^TX$의 가장 큰 eigenvalue에 대응하는 eigenvector이다&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이렇게 요약할 수 있겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;code-space가-다차원인-경우&quot;&gt;Code space가 다차원인 경우&lt;/h3&gt;
&lt;p&gt;다시 문제를 원본으로 되돌려서, 여러 축을 이용하는 경우를 생각해 봅시다. $d$차원에 대한 문제를 해결하는 방법은, 1차원 문제를 $d$번 연속해서 풀고자 합니다. 즉, 위와 같이 축 하나를 뽑아낸 다음, 그 축방향의 모든 분산을 제거한 행렬 
\(\hat{X} = X - X w w^T\)
이 $\hat{X}$에 대해 다시 주성분분석한 벡터를 하나 찾는 방식으로 진행합니다.&lt;/p&gt;

&lt;p&gt;그런데, $\hat{X}^T \hat{X}$라는 행렬을 생각하면, 원본 $X^T X$의 eigenvector $v$에 대해 
\(\hat{X}^T \hat{X}v = (X - Xww^T)^T (X-Xww^T) v = (X^T - ww^T X^T) (Xv - Xww^T v)\)
여기서 $v$가 $w$일 때를 제외하고, 나머지 경우들에서는 eigenvector들이 orthogonal하게 뽑혔다고 생각할 수 있으므로 $w^T v = 0$ 이고, 
\((X^T - ww^T X^T) (Xv) = X^T X v - ww^T X^T X v = \lambda v - \lambda ww^T v = \lambda v\)
즉, $\hat{X}^T \hat{X}$ 들은 나머지 모든 eigenvalue와 그 eigenvector를 그대로 보존한 채로, eigenvalue 하나만 0이 되고 그에 해당하는 eigenvector $w$를 갖는 행렬입니다. 따라서 이번에는 같은 논증에 의해, 두번째로 큰 eigenvalue와 그 eigenvector를 뽑게 됩니다.&lt;/p&gt;

&lt;p&gt;귀납적으로 반복하면, 결국 &lt;strong&gt;주성분&lt;/strong&gt; 들은 eigenvector들임을 알 수 있습니다.&lt;/p&gt;</content><author><name></name></author><category term="deep-learning-study" /><summary type="html">Contents</summary></entry><entry><title type="html">Batch Normalization</title><link href="http://localhost:4000/deep-learning-study/batch-normalization/" rel="alternate" type="text/html" title="Batch Normalization" /><published>2021-12-20T00:00:00+09:00</published><updated>2021-12-20T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning-study/batch-normalization</id><content type="html" xml:base="http://localhost:4000/deep-learning-study/batch-normalization/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#internal-covariate-shift&quot; id=&quot;markdown-toc-internal-covariate-shift&quot;&gt;Internal Covariate Shift&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#batch-normalization&quot; id=&quot;markdown-toc-batch-normalization&quot;&gt;Batch Normalization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experimental-result&quot; id=&quot;markdown-toc-experimental-result&quot;&gt;Experimental Result&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#후속연구--real-effect-of-bn&quot; id=&quot;markdown-toc-후속연구--real-effect-of-bn&quot;&gt;후속연구 : Real effect of BN?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;이 글은 ICML 2015에서 발표된 &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;ICML 2015에서 발표된 Ioffe, Szegedy의 논문&lt;/a&gt; 과 제가 수강했던 심층신경망 강의에 기반합니다.&lt;/p&gt;

&lt;p&gt;ICML 2015 논문에서는 딥러닝에서 training을 가속하기 위한 방법으로, Batch normalization이라는 기법을 제시했습니다. 이 글에서는 그 기법에 대해 살펴봅니다.&lt;/p&gt;

&lt;h2 id=&quot;internal-covariate-shift&quot;&gt;Internal Covariate Shift&lt;/h2&gt;
&lt;p&gt;언제나, 어떤 새로운 기법이 제시되기 위해서는 그 이전 방법 (또는 그 기법이 없을 때) 에 비해 무언가가 나아져야 하고, 그러기 위해서는 어떤 문제가 있는지를 알아야 합니다. 깊은 신경망 네트워크에서 가장 흔히 발생하는 문제 중 하나는 &lt;strong&gt;Vanishing Gradient&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;Vanishing Gradient란, sigmoid 같은 함수를 activation function으로 쓸 때, 입력값이 굉장히 크면 기울기가 너무 작아져서 training이 정상적으로 진행되지 않는 상황을 말합니다. 이 문제 때문에 중간에 한번이라도 값이 튀어서 $x$의 절댓값이 지나치게 커지면, 그 위치에서 아무리 gradient를 구해도 그 값이 너무 작기 때문에 빠져나오지 못하고 갇혀버리게 됩니다. 이 문제를 해결하기 위해 크게 두 가지가 제시되었습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sigmoid같은 함수를 쓰지 말고, ReLU를 쓰자. ReLU는 $x$가 커져도 기울기가 0이 되지 않고, 대신에 음수가 되면 0이 된다는 단점이 있습니다. 이를 보완할 Leaky ReLU같은 activation function을 고려할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;만약 입력값이 계속 -1~1, 또는 0~1 정도 범위 사이를 왔다갔다 한다는 것을 보장할 수 있다면, 사실 sigmoid를 써도 됩니다. 즉, 어떤 식으로든 입력을 stabilize할 수 있으면 좋을 것 같습니다.
    &lt;ul&gt;
      &lt;li&gt;글로 다룬 적은 없지만, 이 문제 때문에 딥러닝은 (Convex한 함수의 최적화와는 다르게) initialization을 잘 해야 합니다. Initialization을 어떻게 할지에 대해서도 많은 논문들이 있습니다.&lt;/li&gt;
      &lt;li&gt;오늘 다룰 방법도 이 관점에서 문제를 살펴봅니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ioffe와 Szegedy는 (이하, 저자들은) 초기에 입력이 좋은 상태 (-1~1 정도 구간 사이에 존재) 하는 상황으로 시작하더라도 나중에 뉴런들을 거치다 보면 이 분포가 변화함을 관찰하였으며, 이를 &lt;strong&gt;Internal Covariate Shift&lt;/strong&gt; 라고 불렀습니다. 즉, 초기에는 평균 0, 분산 1인 아름다운 데이터를 들고 시작하더라도 중간에 평균과 분산이 틀어지며, 이를 교정하고 싶다는 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;
&lt;p&gt;통계학과 기존의 머신러닝에서는 이미 &lt;strong&gt;서로 다른 범위의 데이터를&lt;/strong&gt; 다룰 때, 정규화라는 기법을 이용해 왔습니다. 예를들어 어떤 동물종 A와 B의 개체수에 따른 회귀모형을 만들고자 할 때, A는 1~100 정도 분포하고 B는 1~10,000 정도 분포한다면 B의 차이에 비해 A의 차이가 너무 작아져서 A에 따른 변화가 올바르게 반영되지 못할 것입니다. 이를 막기 위해, 보통은 평균이 0이고 분산이 1이 되도록 전체 값을 맞춰준다거나 하는 정규화를 수행합니다. 이때, 데이터 $x_1, \dots x_n$에 대해 정규화는 다음과 같이 쉽게 수행할 수 있습니다.
\(y_i = \frac{x_i - \expect{x}}{\sqrt{\var[x]}}\)
이 방법은 쉬운 정규화지만, 약간 문제가 있습니다. 정규화 때문에 혹시 네트워크가 표현가능한 함수의 집합 (이를 Representation power라 합니다) 이 줄어들지 않느냐는 것입니다. 예를 들어 sigmoid가 -1, 1 구간에서 거의 linear하고 양쪽부분에서 nonlinear한데 모든 중간 레이어값을 어거지로 이렇게 중간에 밀어넣는게 올바르냐? 는 말에 선뜻 답하기가 어렵습니다.&lt;/p&gt;

&lt;p&gt;그래서 저자들은 새로운 방법으로, 다음과 같은 normalization을 제안합니다. 
\(y_i = \gamma_i \frac{x_i - \expect{x}}{\sqrt{\var[x]}} + \beta_i\)
이때, $\gamma_i$ 와 $\beta_i$는 trainable parameter입니다.&lt;/p&gt;

&lt;p&gt;이제 방법을 생각하고 나면, 또다른 문제가 있습니다. Stochastic optimization을 하는 우리의 특성상, batch 한번을 이용해서 gradient update를 하고나면 평균과 표준편차가 달라집니다. 전체 데이터가 1만개고 batch가 100개씩이라고 하면, 100개를 이용해서 레이어의 각 파라미터를 한번 바꾸고 나면 1만개의 입력을 넣고 돌려서 새로운 평균을 구해야 한다는 것입니다. 이럴거면 애초에 batch를 잡아서 stochastic하게 뭔가를 하는 의미가 없어져 버립니다.&lt;/p&gt;

&lt;p&gt;그래서 저자들은 실제 평균과 표준편차를 구해서 쓰는게 아니라, batch별로 평균과 표준편차를 구해서 그 값들만 활용하는 방법을 제시합니다. 즉… Tensor $X$를 $B \times N$ 로 볼 때,
\(\begin{align*}
    \mu[:] &amp;amp;= \frac{1}{B} \sum_{b = 1}^{B} X[b, :] \\
    \sigma^2[:] &amp;amp;= \frac{1}{B} \sum_{b = 1}^{B} (X[b, :] - \mu[:])^2 + \epsilon \\
    \text{BN}_{\beta, \gamma}(X)[b, :] &amp;amp;= \gamma[:] \odot \frac{X[b, :] - \mu[:]}{\sigma[:]} + \beta[:]
\end{align*}\)
이렇게 됩니다. $\epsilon$은 floating point error를 피하기 위해 집어넣은 적당히 작은 수이므로 수학적으로는 고려하지 않아도 됩니다.&lt;/p&gt;

&lt;p&gt;Convolution 연산에서도 거의 같습니다. 다만, Convolution의 경우 어떤 spatial locality를 유지한다는 성질을 유지하고 싶기 때문에 (이 정보가 어디서 왔는지를 어느정도는 보존하면서 가고 싶기 때문에) H, W 방향으로는 정규화하지 않고, C방향만 이용해서 수행합니다. 즉, tensor $X$를 $B \times C \times H \times W$ 로 볼 때, 
\(\begin{align*}
    \mu[:] &amp;amp;= \frac{1}{BHW} \sum_{b = 1}^{B} \sum_{i = 1}^{H} \sum_{j = 1}^{W} X[b, :, i, j] \\
    \sigma^2[:] &amp;amp;= \frac{1}{BHW} \sum_{b = 1}^{B} \sum_{i = 1}^{H} \sum_{j = 1}^{W} (X[b, :, i, j] - \mu[:])^2 + \epsilon^2 \\
    \text{BN}_{\beta, \gamma}(X)[b, :, i, j] &amp;amp;= \gamma[:] \odot \frac{X[b, :, i, j] - \mu[:]}{\sigma[:]} + \beta[:]
\end{align*}\)&lt;/p&gt;

&lt;h2 id=&quot;experimental-result&quot;&gt;Experimental Result&lt;/h2&gt;
&lt;p&gt;저자들은 다음과 같은 사항들을 &lt;strong&gt;실험적으로&lt;/strong&gt; 확인했습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dropout과 Batch norm은 둘 다 쓸 필요는 없습니다. BN 자체가 어느정도의 regularizaiton효과가 있기 때문이라고 합니다.&lt;/li&gt;
  &lt;li&gt;BN이 있기 때문에, learning rate를 좀더 크게 잡을 수 있습니다. 같은 원리로, momentum을 늘리거나 lr decay를 줄일 수도 있겠습니다.&lt;/li&gt;
  &lt;li&gt;ReLU가 아닌 tanh, sigmoid 등의 함수도 activation으로 쓸 수 있습니다.&lt;/li&gt;
  &lt;li&gt;Local Response Normalization 이라는 방법을 &lt;a href=&quot;/deep-learning-study/AlexNet&quot;&gt;AlexNet 포스팅&lt;/a&gt;에서 언급했지만 더이상 쓰이지 않는다고 말했었는데, 그 이유가 여기에 있습니다. BN을 사용하면 Local Response Normalization은 굳이 필요하지 않다고 합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;또한, &lt;a href=&quot;/deep-learning-study/VGGNet&quot;&gt;VGGNet 포스팅&lt;/a&gt;에서도 “VGGNet은 깊어서 training이 어렵기 때문에 11레이어 훈련하고 거기에 2개 얹고…하는 식으로 training한다” 는 말을 언급한 적이 있는데, BN을 사용하면 이것도 굳이 그렇게 하지 않아도 그냥 바로 16레이어를 훈련할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;후속연구--real-effect-of-bn&quot;&gt;후속연구 : Real effect of BN?&lt;/h2&gt;
&lt;p&gt;MIT의 연구팀 (Santurkar et al) 은 2018년 &lt;a href=&quot;https://arxiv.org/abs/1805.11604&quot;&gt;NeurlPS에 발표된 연구&lt;/a&gt;에서, BN의 저자들이 주장한 Internal Covariate Shift (이하 ICS)에 대한 부분을 반박했습니다. 보다 정확히는, 이 논문의 요점을 정리하자면…&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ICS를 교정한다고 실제로 performance가 높아진다는 근거는 별로 없다. BN의 저자들이 (BN이 ICS를 교정하며) (그래서 performance가 높아진다) 라고 주장했지만, concrete evidence가 있는것은 아니다.&lt;/li&gt;
  &lt;li&gt;In fact, BN이 ICS를 정말 교정하는 것도 사실 아니다. 실험 결과는 BN이 ICS 교정과 별로 상관이 없는것 같다.&lt;/li&gt;
  &lt;li&gt;그럼에도 불구하고 BN이 performance를 개선하는건 사실이다.&lt;/li&gt;
  &lt;li&gt;사실 BN의 진짜 효과는, loss function의 surface를 smooth하게 만드는 효과가 있다. 또한, 다른 regularization 방법들도 수학적으로 분석해보면 그런 효과가 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이정도로 요약할 수 있습니다. 즉, Loss function이 좀더 좋은 형태로 잡히기 때문에 BN을 써야 하긴 하지만, 그게 ICS 때문은 아니라는 것입니다. 이 논문은 다양한 실험결과와 함께 이론적으로도 분석하고 있는데, 이론적 기반을 먼저 갖춘상태로 정확히 필요한 것이 무엇인지를 잡아내서 실험을 설계하면 보다 원하는 결과를 쉽고 정확하게 얻을 수 있다는 점과, 우리가 잘 알려진 기법들에 대해서도 수학적/통계학적으로 명확하게 이해하고 있는 것은 아니라는 것을 보여주는 예시라고 할 수 있겠습니다.&lt;/p&gt;

&lt;p&gt;그럼에도, 이후의 많은 딥러닝 모델들 - 특히 CNN들에 대해서, Batch Normalization은 거의 필수적인 것으로 받아들여지고 있을 만큼 성능 개선 효과가 뚜렷하기에 원 저자들의 연구가 빛이 바래는 것은 아닙니다. 2014년 이후의 Deep CNN 모델들은 BN의 효과 덕분에 훈련이 가능해졌다고 해도 과언이 아니니까요. 앞으로는 이런 방법들을 적용한, 더 깊은 network들에 대해 좀더 살펴보겠습니다.&lt;/p&gt;</content><author><name></name></author><category term="deep-learning-study" /><summary type="html">Contents</summary></entry><entry><title type="html">2021 2학기 종강후 쓰는 일기(?)</title><link href="http://localhost:4000/retrospects-and-plans/finishing-2021-fall/" rel="alternate" type="text/html" title="2021 2학기 종강후 쓰는 일기(?)" /><published>2021-12-20T00:00:00+09:00</published><updated>2021-12-20T00:00:00+09:00</updated><id>http://localhost:4000/retrospects-and-plans/finishing-2021-fall</id><content type="html" xml:base="http://localhost:4000/retrospects-and-plans/finishing-2021-fall/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#컴공-전공과목&quot; id=&quot;markdown-toc-컴공-전공과목&quot;&gt;컴공 전공과목&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#수학-전공과목&quot; id=&quot;markdown-toc-수학-전공과목&quot;&gt;수학 전공과목&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#기타-공부&quot; id=&quot;markdown-toc-기타-공부&quot;&gt;기타 공부&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#프로그래밍-대회&quot; id=&quot;markdown-toc-프로그래밍-대회&quot;&gt;프로그래밍 대회&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#수리과학부-졸업논문&quot; id=&quot;markdown-toc-수리과학부-졸업논문&quot;&gt;수리과학부 졸업논문&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gsim-project&quot; id=&quot;markdown-toc-gsim-project&quot;&gt;GSIM Project&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#posting&quot; id=&quot;markdown-toc-posting&quot;&gt;Posting&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;이번학기는 뭐랄까, 너무 많은걸 시도한 탓인지 중간에 조금은 흔들렸던것 같습니다. 학부 졸업이 가까운 지금도 load balancing이 어렵네요.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/retrospects-and-plans/starting-2021-fall/&quot;&gt;2학기 시작할때 쓴 글&lt;/a&gt; 이랑 지금 느끼는점을 비교해보려고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;컴공-전공과목&quot;&gt;컴공 전공과목&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;계산 이론 : 재밌을것 같았고, 실제로도 재밌었습니다.
    &lt;ul&gt;
      &lt;li&gt;String algorithm에 상상 이상으로 많은 시간이 투자되어, 재밌어 보이는 토픽인 randomized algorithm이나 online algorithm은 많이 배우지 못했습니다. 그래도 suffix tree, aho-corasick같은 이름만 들어본 것들부터, 전반적으로 새로운 것들을 재밌게 배우는 시간이었습니다.&lt;/li&gt;
      &lt;li&gt;관련 분야의 진학에 좀더 확고한 결론을 내리게 되었습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;창의적 통합 설계 : 산학연계 프로젝트 과목입니다. 이번학기에 어떻게보면 가장 아쉽고 가장 성공했습니다.
    &lt;ul&gt;
      &lt;li&gt;주제는 놀랍게도 &lt;strong&gt;Subgraph matching을 이용한 약물의 간독성 판별&lt;/strong&gt; 로, 제가 평소 관심을 갖던 주제와 매우 유사한데다가&lt;/li&gt;
      &lt;li&gt;제가 랩인턴 했던 내용과 매우 관련이 깊은 알고리즘을 활용해보는 기회가 되었습니다.&lt;/li&gt;
      &lt;li&gt;겨울에도 이쪽 관련된 공부를 지속할 예정입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;수학-전공과목&quot;&gt;수학 전공과목&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;현대대수학 II : 생각보다 더 어려웠고, 대수 자체가 너무 뭐랄까, 취향이 아니었습니다.
    &lt;ul&gt;
      &lt;li&gt;Too abstract..? 글쎄, 제가 암호같은걸 공부하지 않아서인지 별로 “이걸 그래서 어따쓰지” 라는 생각이 들어서인지, 공부하는데 너무 힘들었습니다.&lt;/li&gt;
      &lt;li&gt;막 열심히 더 공부할 의욕도 딱히 없고… 그렇다고 뭐 딱히 잘하는 과목도 아니고…&lt;/li&gt;
      &lt;li&gt;수리과학부 남은 3과목은 전부 응용수학과 해석계열로 채워넣을 생각입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;심층신경망의 수학적 기초 : 새로 생긴 수학과 ML 과목.
    &lt;ul&gt;
      &lt;li&gt;최적화이론에 대해 굉장히 좋은 기억이 있고, 정말 많이 배울수 있었던것 같아서 ML도 그런 느낌으로 배워볼 생각으로 신청했습니다.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;소개원실-최적화와 창통설-심수개를 보며 데자뷰가 좀 있는데, 그런 일은 없었으면 합니다.&lt;/em&gt; 네. 결국 창통설 프로젝트 발표에 쫓겨 공부는 제대로는 못한것 같습니다. 블로그에 노트정리한거 올리면서 다시 보고 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;해석학특강 (심층학습의 수치해석) : 2번이랑 같이 배우면서 약간 내용이 많이 겹쳤지만, Uniform approximation theorem 같은것들을 배우는 차이가 있었습니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;기타-공부&quot;&gt;기타 공부&lt;/h2&gt;
&lt;h3 id=&quot;프로그래밍-대회&quot;&gt;프로그래밍 대회&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;SNUPC Div2는 무난히 3등상 받았습니다. 딱히 잘한것도 (힘들게 문제를 하나 해결한것도) 못한것도 (풀었어야할걸 못푼것도) 아닙니다.&lt;/li&gt;
  &lt;li&gt;ICPC 2021은 결국 Little Piplup으로 18등했습니다. 본선 진출권을 왜 던졌는지 후회…해야겠지만, 솔직히 팀연습과 대회가 진심으로 너무 재밌었기 때문에 전혀 후회하지 않습니다.&lt;/li&gt;
  &lt;li&gt;CF 2200도, AtC 2000도 시간이 있었어야 찍을텐데 안되더군요. ㅋㅋ…&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;수리과학부-졸업논문&quot;&gt;수리과학부 졸업논문&lt;/h3&gt;
&lt;p&gt;Ongoing. Image segmenation 관련 주제의 생각외로 심각한 맹점을 두개 알게 되었는데,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;코드가 &lt;strong&gt;미친듯이&lt;/strong&gt; 오래 걸립니다. 딥러닝 경험이 처음이라 하나 돌려놓고 24시간씩 기다려본적이 처음인데, 컴퓨터는 한대인데 24시간동안 기다려야하고 자고일어나서 켜보니 오타 하나 때문에 내가 새벽 소중한 computation time과 전기를 낭비했다는 사실을 깨닫고 혈압이 상승한적이 한 15번은 있는 것 같습니다. 덕분에 1070Ti의 수명이 비트코인 채굴하는거마냥 감소했을텐데 쩝…&lt;/li&gt;
  &lt;li&gt;왜인지 모르겠지만 Pytorch로 짜면 재현이 잘 안 됩니다. 이건 제 문제인지, 뭔가가 명시되지 않은 것인지…&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;gsim-project&quot;&gt;GSIM Project&lt;/h3&gt;
&lt;p&gt;다들 너무 바빠서 중간에 흐지부지했습니다. 재밌었는데 아쉽네요&lt;/p&gt;

&lt;h3 id=&quot;posting&quot;&gt;Posting&lt;/h3&gt;
&lt;p&gt;생각보다 딥러닝 정리를 많이 포스팅했고 (할 예정이고) 많은 것들을 배웠습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/deep-learning-study/optimizers-for-deep-learning&quot;&gt;Optimizer 정리하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/deep-learning-study/backpropagation/&quot;&gt;Backpropagation&lt;/a&gt;
뭐 이런 것들.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;논문 공부는 많이 못 읽었습니다. 학교 공부하다보니 읽어야했던것들 정도&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/cs-adventure/VEQ/&quot;&gt;VEQ 부분그래프 매칭&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/cs-adventure/APASP/&quot;&gt;‘거의’ 최단 경로&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="retrospects-and-plans" /><summary type="html">Contents</summary></entry><entry><title type="html">Unsupervised Learning, Autoencoders</title><link href="http://localhost:4000/deep-learning-study/autoencoders/" rel="alternate" type="text/html" title="Unsupervised Learning, Autoencoders" /><published>2021-12-14T00:00:00+09:00</published><updated>2021-12-14T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning-study/autoencoders</id><content type="html" xml:base="http://localhost:4000/deep-learning-study/autoencoders/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#unsupervised-learning&quot; id=&quot;markdown-toc-unsupervised-learning&quot;&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#autoencoder&quot; id=&quot;markdown-toc-autoencoder&quot;&gt;Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;unsupervised-learning&quot;&gt;Unsupervised Learning&lt;/h2&gt;
&lt;p&gt;Unsupervised (또는, Self-supervised) 의 의미는, &lt;strong&gt;정답이 제공되지 않는&lt;/strong&gt; 머신 러닝 상황을 말합니다. 이게 무슨 말인지 알아보기 위해, 이전에 알아본 supervised learning의 기본 프레임으로 돌아가보겠습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;미지의 함수 $f$에 대해 알고자 하는데,&lt;/li&gt;
  &lt;li&gt;모든 지점이 아닌 어떤 지점 $x_i$ 들에서만 그 값 $f(x^i) = y^i$ 를 알고 있고,&lt;/li&gt;
  &lt;li&gt;그래서 어떤 페널티 $\ell$ 을 정의해서, $\sum_i \ell(f(x^i), g(x^i))$가 작은 $g$를 $f$의 근사-함수로 생각하고 싶습니다.&lt;/li&gt;
  &lt;li&gt;그런데 이 $g$를 모든 함수의 공간에서 최적화하는 것은 일반적으로 가능하지 않으므로,&lt;/li&gt;
  &lt;li&gt;어떤 parameter $\theta$ 에 의해 표현되는 함수공간의 부분집합 $g_\theta$만을 생각하며,&lt;/li&gt;
  &lt;li&gt;$\minimize \sum_i \ell(f(x^i), g_\theta(x^i))$ by moving $\theta$로 생각합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기서, ‘supervision’ 은 정답 $f(x^i) = y^i$ 를 말합니다. 이를 이미 알고 있는 경우 (예를 들어, Imagenet 데이터는 사람을 잔뜩 고용해서 이미지를 실제로 구분했습니다) 도 있지만, 많은 경우에 이 데이터를 모으는 과정 자체가 매우 비싸거나 불가능합니다. 이런 경우에는, 이 프레임을 과감히 포기하고, supervision 없이 데이터 자체의 내부적인 구조를 학습하도록 하고자 합니다.&lt;/p&gt;

&lt;p&gt;즉, unsupervised learning이란:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 $X_1, \dots X_n$ 이 주어질 때,&lt;/li&gt;
  &lt;li&gt;그 내부의 어떤 &lt;strong&gt;구조&lt;/strong&gt;화가 얼만큼 진행되었는지를 실용적으로 유의미한 어떤 loss function $\mathcal{L}(\theta)$ 의 학습가능한 형태로 만들어서&lt;/li&gt;
  &lt;li&gt;이를 최적화하는 방향으로 생각합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Supervised learning과는 달리, 이 프레임만 읽어서는 무엇을 하고 싶은지가 별로 명확하지 않습니다. 그래서 대표적인 예시인 Autoencoder를 직접 살펴보고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;autoencoder&quot;&gt;Autoencoder&lt;/h2&gt;
&lt;p&gt;Autoencoder란 간단히 압축처럼 생각하는 것이 가장 편하게 이해할 수 있는 방법입니다. 우리는 어떤 함수 $E_\theta$ 와 $D_\phi$를 만들어서, $E_\theta$ 가 모델의 어떤 특징을 &lt;strong&gt;인코딩&lt;/strong&gt; 하고, $D_\phi$ 가 반대로 이를 &lt;strong&gt;디코딩&lt;/strong&gt; 하도록 하고자 합니다. 즉… $\R^n$ 에서, $\R^m$ 으로 가는 함수 $E_\theta$와, (수학적으로 존재하지 않는) 그 역함수 비슷한 $D_\phi$를 만들고자 합니다.&lt;/p&gt;

&lt;p&gt;이때, 다음과 같은 값을 생각합니다.
\(\mathcal{L}(\theta, \phi) = \expect{\norm{X - D_\phi(E_\theta(X))}^2}\)
이 값이 작다는 것은, $D(E(X))$ 가 $X$를 거의 그대로 가져간다는 의미입니다. 다시 말해, $E(X)$는 일종의 &lt;strong&gt;압축&lt;/strong&gt; 을 수행하고, $D$는 이 압축을 해제하는 함수가 됩니다.&lt;/p&gt;

&lt;p&gt;여전히, 우리는 expectation을 직접 최적화할 수 없고, 데이터만을 이용해서 뭔가를 해야 하므로, 
\(\mathcal{L}(\theta, \phi) = \frac{1}{N} \sum_{i = 1}^{N} \norm{X_i - D_\phi(E_\theta(X_i))}^2\)
이 값을 대신 최적화한 다음, $\theta, \phi$가 잘 최적화되어 실제로는 expectation이 작기를 기대합니다. 이는 사실 위에서 설명한 supervised learning의 원리에서도 적용했던 방법 (충분히 많은 데이터를 통해 최적화하면 &lt;strong&gt;아마도&lt;/strong&gt; 작동할 것이다) 이므로, 이러한 논증은 (비교적) 합리적이라고 할 수 있겠습니다.&lt;/p&gt;

&lt;p&gt;이걸로 뭘 할 수 있을지는 굉장히 다양합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Encoder는 일종의 압축 알고리즘이므로, 충분히 잘 훈련한 autoencoder를 그냥 이미지 압축같은거에 써도 됩니다.&lt;/li&gt;
  &lt;li&gt;데이터의 noise를 제거할 수 있습니다. Encoder-Decoder는 아마도 데이터의 inherent한 구조를 반영하고 있을 것이므로, noise는 encoding과정에 별로 유의미하게 반영되지 않을 것이며, decoding할 때 그 noise가 줄어들 것이라고 생각할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;데이터의 anomally를 잡아낼 수 있습니다. 이미 충분히 잘 훈련한 Encoder-Decoder에 대해, $\norm{X - D_\phi(E_\theta(X))}^2$가 다른 데이터에 비해 특이하게 큰 뭔가가 있다면, 이 데이터에는 &lt;strong&gt;다른 값들과 다른 어떤 요소&lt;/strong&gt; 가 있다고 생각할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;Label은 없지만, 비슷한 것들끼리 묶는것은 가능합니다. K-means같은 clustering.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;참고로, classical machine learning의 대표적인 알고리즘 중 하나인 PCA(Principal Component Analysis)는 $E$로 $y = Ax$를, $D$로 $y = A^T x$를 쓰는 autoencoder라고 생각할 수 있습니다. 이는 나중에 PCA정도는 따로 포스팅할 예정이므로 그때 추가하겠습니다.&lt;/p&gt;</content><author><name></name></author><category term="deep-learning-study" /><summary type="html">Contents</summary></entry><entry><title type="html">CNN Architecture - VGGNet (2014)</title><link href="http://localhost:4000/deep-learning-study/VGGNet/" rel="alternate" type="text/html" title="CNN Architecture - VGGNet (2014)" /><published>2021-12-13T00:00:00+09:00</published><updated>2021-12-13T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning-study/VGGNet</id><content type="html" xml:base="http://localhost:4000/deep-learning-study/VGGNet/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#architecture&quot; id=&quot;markdown-toc-architecture&quot;&gt;Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#very-deep-cnn&quot; id=&quot;markdown-toc-very-deep-cnn&quot;&gt;Very Deep CNN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#training&quot; id=&quot;markdown-toc-training&quot;&gt;Training&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#regularization&quot; id=&quot;markdown-toc-regularization&quot;&gt;Regularization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#optimization&quot; id=&quot;markdown-toc-optimization&quot;&gt;Optimization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#initialization&quot; id=&quot;markdown-toc-initialization&quot;&gt;Initialization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#결과와-의의&quot; id=&quot;markdown-toc-결과와-의의&quot;&gt;결과와 의의&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;VGGNet은 2014 ImageNet Challenge에서 2위의 성적을 거둔 모델로, &lt;strong&gt;깊은 네트워크&lt;/strong&gt; 를 쌓았을때의 효용을 보여준 모델입니다. 역시 &lt;a href=&quot;/deep-learning-study/AlexNet&quot;&gt;AlexNet 포스팅&lt;/a&gt; 때처럼 논문을 따라가면서 메인 아이디어를 살펴보겠습니다.&lt;/p&gt;
&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;VGGNet은 레이어 개수에 따라 11, 13, 16, 19 등으로 나누어집니다. 이는 실제로 &lt;strong&gt;Convolution Layer&lt;/strong&gt;가 11개에서 19개까지 있는 모델로, 크게는 (Conv-ReLU)-(Conv-ReLU)-(maxpool) 을 반복하는 식으로 구성됩니다.
&lt;img src=&quot;../../images/e8c6095a3de9c6520b217e1cdbbba24a55ff01ee9da5a58ef2c073a314aa3202.png&quot; alt=&quot;drawing&quot; width=&quot;85%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 그림은 VGG-13의 그림인데, 정확한 11, 13, 16, 19의 구조는 다음과 같습니다. 중간에 ReLU가 매 conv 뒤에 들어가있지만 표시는 안되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/b61f8a33132897fbda7e600dc158dea5be4104709e9bfc6462e0900cec2fddff.png&quot; alt=&quot;drawing&quot; width=&quot;60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다양한 VGGNet의 버전들은 큰 틀에서는 비슷하기 때문에, 다음에 Pytorch 구현 코드를 볼 때 다시 살펴볼 예정입니다.&lt;/p&gt;

&lt;p&gt;VGGNet 논문의 Contribution 두 가지를 정리하자면,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;CNN을 깊이 쌓을 수 있다면 유의미한 성능 향상이 있다&lt;/strong&gt; 는 것을 보였고&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;잘 훈련하면 이렇게 깊은 CNN도 훈련할 수 있다&lt;/strong&gt; 라는 것을 보였습니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;각 Contribution을 살펴보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;very-deep-cnn&quot;&gt;Very Deep CNN&lt;/h2&gt;
&lt;p&gt;VGGNet은 메인으로 3 by 3 convolution만 사용합니다. 그 이유를 저자들은 다음과 같이 제시합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;7 by 7 convolution 하나를 쓰는 것과, 3 by 3 convolution을 세번 쓰는 것은 같은 receptive field를 가집니다. (즉, 입력의 같은 영역이 반영됩니다) 이는 직접 receptive field를 확인해보면 알 수 있습니다.&lt;/li&gt;
  &lt;li&gt;그런데, 파라미터의 개수는 49 : 27로 대략 절반선까지 줄어듭니다.&lt;/li&gt;
  &lt;li&gt;그러면서, 3번의 레이어 사이사이에는 ReLU가 들어가기 때문에 전체 함수가 더 non-linear해집니다.&lt;/li&gt;
  &lt;li&gt;경우에 따라 1 by 1 convolution도 쓰긴 하는데 (VGG16), 이것도 적은 수의 파라미터만으로 nonlinearity를 추가하기 위해서입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그외에, Convolution을 깊게 들어가서 마지막에 Fully Connected Layer로 실제 classification을 추가하는 것은 AlexNet과 동일합니다.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Regularization으로는 적절한 data augmentation과 함께, $5 \times 10^{-4}$ 의 weight decay를 사용합니다.&lt;/li&gt;
  &lt;li&gt;Fully connected layer에서는 $p = 0.5$로 dropout도 적용합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SGD with Momentum. Initial LR 0.01, Momentum 0.9&lt;/li&gt;
  &lt;li&gt;Validation accuracy가 정체되면 LR을 1/10으로 깎는 식으로 진행합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;initialization&quot;&gt;Initialization&lt;/h3&gt;
&lt;p&gt;VGGNet의 재밌는 점 중 하나는, training 과정에서 너무 깊은 19레이어 CNN을 한번에 training할 수 없기 때문에, 11-Layer 버전을 훈련한 후 이 위에서 레이어를 하나씩 추가해가면서 training했다는 점입니다. 이는 따져보면, pre-train 된 모델을 이용하여 initialize를 잘 하는 느낌으로 생각할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;결과와-의의&quot;&gt;결과와 의의&lt;/h2&gt;
&lt;p&gt;VGGNet은 2014 Imagenet Challenge에서 top-5 error 6.8% 정도로, AlexNet의 16%에 비해 큰 성능 향상을 이루었습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2014년 1위였던 GoogLeNet이 6.7%로 거의 똑같은 정확도를 갖는데, 이는 나중에 포스팅하겠지만 VGG보다 훨씬 복잡한 내부 구조를 가지고 있습니다.&lt;/li&gt;
  &lt;li&gt;그래서, 이후 딥러닝 연구에서 실험이나 다른 용도로 쓰기에 VGGNet이 좀더 용이합니다. 대표적으로, Semantic Segmentation 모델중 하나인 FCN은 VGGNet의 구조를 활용하여 이미지의 특징을 추출합니다.&lt;/li&gt;
  &lt;li&gt;Initialization을 통해 훈련을 더 잘 하는 방법, 큰 convolution 하나보다 작은 convolution 여러개를 쓰는 것의 이점 등을 보여주었습니다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="deep-learning-study" /><summary type="html">Contents</summary></entry><entry><title type="html">Overfitting and Regularization</title><link href="http://localhost:4000/deep-learning-study/overfitting-and-regularization/" rel="alternate" type="text/html" title="Overfitting and Regularization" /><published>2021-12-11T00:00:00+09:00</published><updated>2021-12-11T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning-study/overfitting-and-regularization</id><content type="html" xml:base="http://localhost:4000/deep-learning-study/overfitting-and-regularization/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#overfitting--underfitting&quot; id=&quot;markdown-toc-overfitting--underfitting&quot;&gt;Overfitting / Underfitting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#weight-decay--l2-regularization&quot; id=&quot;markdown-toc-weight-decay--l2-regularization&quot;&gt;Weight Decay / L2 Regularization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dropout&quot; id=&quot;markdown-toc-dropout&quot;&gt;Dropout&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-augmentation&quot; id=&quot;markdown-toc-data-augmentation&quot;&gt;Data Augmentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;overfitting--underfitting&quot;&gt;Overfitting / Underfitting&lt;/h2&gt;
&lt;p&gt;이 블로그에서도 여러 차례 기본 세팅으로 언급한, &lt;strong&gt;Deep learning의 기본 프레임워크&lt;/strong&gt; (사실은, 좀더 general하게 machine learning 내지는 regression 전체에 적용되는 프레임입니다) 를 돌아보는 것으로 시작하겠습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;미지의 함수 $f$에 대해 알고자 하는데,&lt;/li&gt;
  &lt;li&gt;모든 지점이 아닌 어떤 지점 $x_i$ 들에서만 그 값 $f(x^i) = y^i$ 를 알고 있고,&lt;/li&gt;
  &lt;li&gt;그래서 어떤 페널티 $\ell$ 을 정의해서, $\sum_i \ell(f(x^i), g(x^i))$가 작은 $g$를 $f$의 근사-함수로 생각하고 싶습니다.&lt;/li&gt;
  &lt;li&gt;그런데 이 $g$를 모든 함수의 공간에서 최적화하는 것은 일반적으로 가능하지 않으므로,&lt;/li&gt;
  &lt;li&gt;어떤 parameter $\theta$ 에 의해 표현되는 함수공간의 부분집합 $g_\theta$만을 생각하며,&lt;/li&gt;
  &lt;li&gt;$\minimize \sum_i \ell(f(x^i), g_\theta(x^i))$ by moving $\theta$로 생각합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;/deep-learning-study/optimizers-for-deep-learning&quot;&gt;Optimizer에 관한 포스팅&lt;/a&gt; 에서는 이 중, &lt;strong&gt;그래서 어떻게 최적화할지&lt;/strong&gt; 를 생각했습니다. 여기서는, 조금 다른 문제를 생각해 보려고 합니다.&lt;/p&gt;

&lt;p&gt;우리가 딥러닝이든, 일반적인 머신러닝이든 이용해서 $f$를 알아내려는 이유는 원래 이미 주어진 $x^i$ 들 외의, 새로운 점 $z$가 들어왔을 때 $f(z)$를 알고자 하는 것입니다. 예를들어 사진 1만 장을 이용해서 개와 고양이를 구분하는 함수 $g_\theta$를 훈련하고 나면, 훈련과정에서 한번도 본 적 없는 새로운 $z$가 개인지 고양이인지를 알아낼 수 있어야 합니다. 위 프레임은 그래서 다음 두 가지 의문이 생길 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\ell$은 페널티이므로, $z$를 올바르게 판정하는 것은 새로운 데이터에 대해 $\ell(f(z), g_\theta(z))$ 가 작았으면 좋겠다고 생각할 수 있습니다. 그런데, 우리는 $x^i$들에 대해서 $g_\theta$를 훈련했습니다. $f$가 매우 이상하게 생긴 함수라면, $g_\theta$를 아무리 잘 최적화해 왔더라도 완전히 다른 이슈가 발생할 수도 있습니다.&lt;/li&gt;
  &lt;li&gt;애초에, $g$가 모든 함수의 공간이 아닌 $g_\theta$로 표현되는 함수공간의 부분집합만을 생각하는데 $f$랑 충분히 가까운 함수가 $g_\theta$들의 집합에 있기는 할지도 모를 일입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 두가지 이슈를 딥러닝에서는 (보다 일반적으로, 통계학에서는) 각각 overfitting / underfitting이라고 부릅니다. 즉,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt; 이란, 우리가 가진 모델 $g_\theta$가 훈련은 잘 되지만 미지의 데이터에 대한 성능이 그에 미치지 못하는 경우입니다. 우리가 잘 알고 있는 인간의 학습과 비교해보면, 같은 책을 계속 보다 보니 그 책은 잘 풀지만 새로운 문제를 주면 못 푸는(…) 상황이라고 할 수 있겠습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Underfitting&lt;/strong&gt; 이란, $g_\theta$를 충분히 잘 최적화하지 못한 상황입니다. 역시 인간의 학습과 비교해보면 그냥 공부가 덜 된 상황입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/81b7294441f2b9c96cce938661b95a1d20d22366e5c0f72e48d2c69c9c7ad7b4.png&quot; alt=&quot;picture 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 굳이 인간의 학습에 비유한 이유가 있습니다. 해결책도 약간 motivation이 비슷합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;먼저, &lt;strong&gt;Underfitting&lt;/strong&gt; 을 해결하는 방법은 훈련을 더 하거나 (공부를 더 시키는 느낌) 아니면 더 좋은 모델을 개발하는 것입니다 (이건…인간의 학습으로 치면 스탯의 문제임을 인정하는거라 좀 애매합니다 ㅋㅋ;;)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt; 은 훈련이 잘못되고 있는 것입니다. 즉 모델이 필요 이상으로 훈련데이터의 특징을 잡아내고 있다는 점이고, 이것도 마찬가지로 더 많은 데이터를 쓴다거나 (아예 공부할 자료를 더 주는 느낌입니다), 아니면 regularization이라는 방법을 이용, 학습이 훈련데이터의 미세한 특징보다는 좀더 큰그림에 집중하도록 유도합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기서는 Overfitting을 줄이는 regularization에 주목합니다. Underfitting은 더 강한 모델을 쓰거나, 데이터를 늘리거나, 훈련을 늘리는 등 좀더 직관적인 방법으로 극복이 가능하기 때문에 다루지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;weight-decay--l2-regularization&quot;&gt;Weight Decay / L2 Regularization&lt;/h2&gt;
&lt;p&gt;우리는 SGD를 이용하여, 다음과 같이 weight $\theta$ 값을 조정하는 식으로 training을 수행합니다. 
\(\theta^{k+1} = \theta^k - \alpha g^k\)
Weight decay란, 다음과 같이 SGD update를 수정하는 방법입니다. 
\(\theta^{k+1} = (1 - \alpha \lambda) \theta^k - \alpha g^k\)
즉, 매번 $\theta$의 값이 조금씩 decay합니다.&lt;/p&gt;

&lt;p&gt;이 값은 때로는 L2 Regularization이라고 불립니다. 그 이유는, 위 update를 잘 보면 $\alpha$가 $\lambda \theta^k$에 곱해져 있고, 이는 즉 $\lambda \theta^k$ 가 뭔가 $g^k$처럼 gradient스러운 값임을 의미하는 데서 찾을 수 있습니다. 즉, Loss function을 잘 조정한 다음 일반 SGD를 쓰면, 일반 Loss function에 대해서 weight decay 한 것과 같은 결과를 얻는다는 점입니다.&lt;/p&gt;

&lt;p&gt;정확히, 다음과 같은 loss function을 쓰면 weight decay와 동치가 됩니다. 
\(\frac{1}{N} \sum_{i = 1}^{N} \ell(f_\theta(x^i), y^i) + \frac{\lambda}{2} \norm{\theta}^2\) 
이 계산은 간단하게 verify할 수 있는 미분 계산이므로 생략합니다. 핵심 아이디어는, “Weight값이 커지는 것을 기분나쁘게 받아들이자” 라는 것입니다. 모델이 한정된 데이터에 대해 overfitting하기 위해서는 각 파라미터값이 크게 fluctuate한다는 것을 역이용하는 아이디어입니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 6개의 점 $(0, 0), (2, 4), (4, 16), (6, 35), (8, 65), (10, 99)$ 를 polynomial regression 한다고 생각해 봅시다. 이 데이터는 $y = x^2$ 를 “거의” 따라가므로, 2차식을 이용하여 approximate하면 $y = x^2$ 정도가 올바른 훈련 결과가 될 것입니다. 그런데, 여기에 5차식을 이용하면 training error가 0인 모델을 만들 수 있습니다 (Lagrange Interpolation). 당연히 훈련 데이터를 100% 맞추는대신 unseen data와 크게 어긋나는 좋지 못한 (overfitting이 심한) 모델인데, 이 모델을 아무튼 피팅해보면 그 결과는 $-\frac{x^5}{240}+\frac{37 x^4}{384}-\frac{73 x^3}{96}+\frac{323 x^2}{96}-\frac{287 x}{120}$ 입니다. 계수가 훨씬 크다는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게, 오버피팅이 일어나면 대체로 계수가 크게 널뛰는 일들이 벌어집니다. 따라서, 반대로 계수가 커지는걸 supress하면, 오버피팅을 일부 줄일 수 있습니다.&lt;/p&gt;

&lt;p&gt;Weight decay는 2-norm을 loss에 더해서 L2 regularization이지만, 같은 논리로 L1, L3 등 다른 norm을 이용하는 regularization도 가능합니다.&lt;/p&gt;

&lt;h2 id=&quot;dropout&quot;&gt;Dropout&lt;/h2&gt;
&lt;p&gt;Dropout은 훈련중에 일부 뉴런을 죽이고, 그 값만큼을 나중에 보정해주는 방법입니다. 
&lt;img src=&quot;../../images/4a8a1792f00876d0aa088fdab7687ada01f4561ccfd8b51e8578c1bc053a131a.png&quot; alt=&quot;picture 1&quot; /&gt;&lt;br /&gt;
(이미지 출처 : Dive To Deep Learning)&lt;/p&gt;

&lt;p&gt;즉, 각 레이어에서 $p$확률로 각 뉴런을 죽인다음, 살아남은 뉴런에 대해서는 가중치에 $\frac{1}{1-p}$ 만큼을 곱해서 전체의 가중치 합을 유지합니다. 이 방법은 수학적으로는 함수의 smoothness를 좀더 강제하는 방향으로 그럴듯한 argument가 있는데, 별로 rigorous하지는 않습니다. 다만 in practice 잘 작동하기 때문에 자주 사용됩니다.&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h2&gt;
&lt;p&gt;이 방법은 간단히 설명하자면 데이터에 변형을 가해서 데이터 자체를 좀더 강화하는 것입니다. 인간의 학습으로 치자면, 같은 문제에서 숫자를 바꾼 유제를 더 풀도록 시키는 느낌인데요.&lt;/p&gt;

&lt;p&gt;대표적으로 Convolutional Neural Network같은걸 이용해서 classification을 하는 상황을 생각해 보겠습니다. 어떤 사진 $X_1$을 고양이라고 판단할 수 있는 네트워크는, $X_1$을 가로로 180도 뒤집은 (좌우대칭) 사진도 고양이라고 판단할 수 있어야 합니다. 또한, 이미지 전체에 약간의 노이즈가 있거나, 이미지의 크기가 바뀌거나, 가로세로에서 작은 픽셀만큼을 뺀다거나, 5도정도 돌려놓은 사진도 다 고양이라고 판단할 수 있어야 합니다.&lt;/p&gt;

&lt;p&gt;따라서, 이런 방법들을 이용한다면, 고양이 사진 한 장을 마치 여러 데이터인것처럼 사용할 수 있기 때문에&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;단순히 데이터를 늘린것처럼 생각하더라도, 왜 훈련이 잘 되는지 직관적으로 납득가능하며&lt;/li&gt;
  &lt;li&gt;결정적으로, “enforce하고싶은 성질” - 예를들어, “좌우대칭해도 라벨이 바뀌지 않는다” 를 직접 주입할 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그래서, 어떤 augmentation을 할지는 데이터를 보고 직접 택해야 합니다. 예를들어 손글씨를 판단하는 태스크라면 좌우대칭 같은것은 쓸 수 없습니다 (대칭하면 라벨이 바뀌기 때문에) 그러나 대신 색깔을 바꾸는 augmentation이 가능한 식입니다.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overfitting은 모델이 복잡하고 깊어질수록 더 많이 발생한다는 점에서 현대에 와서 더 강한(?) 모델이 등장함에 따라 더 큰 문제로 다가옵니다. 이러한 overfitting을 막기 위한 여러 regularization 방법들은 대부분의 모델에서 다양하게 사용되며, 새로운 모델을 설계할 때 필수적인 요소라고 할 수 있겠습니다.&lt;/p&gt;</content><author><name></name></author><category term="deep-learning-study" /><summary type="html">Contents</summary></entry><entry><title type="html">AlexNet으로 CIFAR10 풀어보기</title><link href="http://localhost:4000/deep-learning-study/alexnet-cifar10/" rel="alternate" type="text/html" title="AlexNet으로 CIFAR10 풀어보기" /><published>2021-11-26T00:00:00+09:00</published><updated>2021-11-26T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning-study/alexnet-cifar10</id><content type="html" xml:base="http://localhost:4000/deep-learning-study/alexnet-cifar10/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#구현--alexnet&quot; id=&quot;markdown-toc-구현--alexnet&quot;&gt;구현 : AlexNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#결과--alexnet&quot; id=&quot;markdown-toc-결과--alexnet&quot;&gt;결과 : AlexNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#구현--smallalexnet&quot; id=&quot;markdown-toc-구현--smallalexnet&quot;&gt;구현 : SmallAlexNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#결과--smallalexnet&quot; id=&quot;markdown-toc-결과--smallalexnet&quot;&gt;결과 : SmallAlexNet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;코드는 &lt;a href=&quot;https://github.com/gratus907/Pytorch-Cifar10&quot;&gt;Github repository&lt;/a&gt; 에 업로드됩니다.&lt;/p&gt;

&lt;h2 id=&quot;구현--alexnet&quot;&gt;구현 : AlexNet&lt;/h2&gt;
&lt;p&gt;상세한 코드는 위 Repository에서 볼 수 있으므로, 여기서는 모델만 구현하겠습니다.
&lt;img src=&quot;../../images/8190f2b2db0f7eb370af157be64544adbe4c13e88488ef86e8d2bf9a60d90be2.png&quot; alt=&quot;picture 1&quot; /&gt;&lt;br /&gt;
정확히 그림을 따라가지는 않을건데, 그림은 ImageNet task를 위한 모델이므로 우리의 CIFAR-10 데이터셋과 다르기 때문입니다. ImageNet은 224 * 224 이미지인데 비해 우리는 32 * 32 이미지를 사용합니다.&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AlexNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AlexNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;AlexNet&quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;96&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;96&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;384&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9216&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9216&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;이 모델은 최대한 원본 그림을 그대로 따라간 버전입니다. 채널수 96-&amp;gt;256-&amp;gt;384-&amp;gt;384도 똑같이 맞췄고, 5 by 5 convolution과 overlapping pooling을 적용했습니다.&lt;/p&gt;

&lt;p&gt;Convolution 파트의 결과값이 256채널 * 6 * 6 = 9216개의 뉴런이기 때문에, 9216 -&amp;gt; 4096 -&amp;gt; 4096 -&amp;gt; 10 으로 진행되는 MLP를 붙여야 합니다. 원본과 똑같이 dropout을 적용했습니다.&lt;/p&gt;

&lt;p&gt;이 모델의 summary는 다음과 같습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 96, 29, 29]           4,704
              ReLU-2           [-1, 96, 29, 29]               0
            Conv2d-3          [-1, 256, 29, 29]         614,656
              ReLU-4          [-1, 256, 29, 29]               0
         MaxPool2d-5          [-1, 256, 14, 14]               0
            Conv2d-6          [-1, 384, 14, 14]         885,120
              ReLU-7          [-1, 384, 14, 14]               0
            Conv2d-8          [-1, 384, 14, 14]       1,327,488
              ReLU-9          [-1, 384, 14, 14]               0
           Conv2d-10          [-1, 256, 14, 14]         884,992
             ReLU-11          [-1, 256, 14, 14]               0
        MaxPool2d-12            [-1, 256, 6, 6]               0
          Dropout-13                 [-1, 9216]               0
           Linear-14                 [-1, 4096]      37,752,832
             ReLU-15                 [-1, 4096]               0
          Dropout-16                 [-1, 4096]               0
           Linear-17                 [-1, 4096]      16,781,312
             ReLU-18                 [-1, 4096]               0
           Linear-19                   [-1, 10]          40,970
          AlexNet-20                   [-1, 10]               0
================================================================
Total params: 58,292,074
Trainable params: 58,292,074
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 8.26
Params size (MB): 222.37
Estimated Total Size (MB): 230.64
----------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;주목할 점은…&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deeper convolution network 라는 컨셉이긴 하지만, 사실 잘 보면 대부분의 parameter는 마지막 linear layer에 몰려 있습니다. 이건 이후에 공부할 VGG를 비롯한 모델들도 마찬가지인데, 결국 앞부분의 Convolution으로 feature를 &lt;strong&gt;적당히&lt;/strong&gt; 추출한 다음, 추출한 feature에 대한 MLP라고 생각할 수 있겠습니다.&lt;/li&gt;
  &lt;li&gt;LeNet이 6만개의 파라미터를 갖는것에 비해, 대략 1000배인 6천만 개의 파라미터를 갖습니다. 오버피팅을 막기 위한 regularization이 매우 중요할 것 같습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;결과--alexnet&quot;&gt;결과 : AlexNet&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;../../images/0dd5b633fa8a9698e37e4a14f57b3927358ceec1149742d6aa31884a1224d2b8.png&quot; alt=&quot;picture 2&quot; /&gt;&lt;br /&gt;
파란색이 training, 주황색이 testing (validation) 이고, 왼쪽이 Loss, 오른쪽이 정확도 (Accuracy)입니다. 대략 80%정도까지는 잘 올라가지만, 이후에는 Overfitting의 문제가 살짝 발생하는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;최종적으로, 50 Epoch (대략 76분 정도)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 의 training을 통해, 85.03%의 정확도를 얻을 수 있었습니다.&lt;/p&gt;

&lt;h2 id=&quot;구현--smallalexnet&quot;&gt;구현 : SmallAlexNet&lt;/h2&gt;
&lt;p&gt;정말 이렇게 큰 모델이 CIFAR10에 필요할까요? 좀 빨리 훈련할수 있는, 경량화 AlexNet을 적당히 짜맞춰 보겠습니다.&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SmallAlexNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SmallAlexNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;SmallAlexNet&quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;192&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;192&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_layer3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_layer1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;우선, linear layer의 크기를 확 줄였습니다. 이를 위해 적당히 맞춰서 앞부분의 크기도 줄였습니다. 뭔가 합당한 이유와 원리를 가지고 줄인건 아니고, 그냥 적당히 비율을 맞춰 줄여봤습니다.&lt;/p&gt;

&lt;p&gt;대신에, 좀더 넓은 범위를 보면서 feature를 추출할 수 있도록 첫 convolution 필터사이즈를 $7 \times 7$ 로 늘렸습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 26, 26]           9,472
              ReLU-2           [-1, 64, 26, 26]               0
            Conv2d-3          [-1, 192, 26, 26]         307,392
              ReLU-4          [-1, 192, 26, 26]               0
         MaxPool2d-5          [-1, 192, 12, 12]               0
            Conv2d-6          [-1, 256, 12, 12]         442,624
              ReLU-7          [-1, 256, 12, 12]               0
            Conv2d-8          [-1, 256, 12, 12]         590,080
              ReLU-9          [-1, 256, 12, 12]               0
           Conv2d-10          [-1, 128, 12, 12]         295,040
             ReLU-11          [-1, 128, 12, 12]               0
        MaxPool2d-12            [-1, 128, 5, 5]               0
          Dropout-13                 [-1, 3200]               0
           Linear-14                 [-1, 2048]       6,555,648
             ReLU-15                 [-1, 2048]               0
          Dropout-16                 [-1, 2048]               0
           Linear-17                 [-1, 2048]       4,196,352
             ReLU-18                 [-1, 2048]               0
           Linear-19                   [-1, 10]          20,490
     SmallAlexNet-20                   [-1, 10]               0
================================================================
Total params: 12,417,098
Trainable params: 12,417,098
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 4.38
Params size (MB): 47.37
Estimated Total Size (MB): 51.76
----------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Linear layer를 확 줄였기 때문에, 전체 파라미터의 수는 1/5로 줄어들었고 이제는 전체 파라미터의 대략 15% 정도가 Convolution에 들어가 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;결과--smallalexnet&quot;&gt;결과 : SmallAlexNet&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;../../images/10f7666c0f4b99884bcab280055f43b11089b3e15a0f3af57f2073de8af0840a.png&quot; alt=&quot;picture 3&quot; /&gt;&lt;br /&gt;
이 모델도 비슷한 behavior를 보입니다.&lt;br /&gt;
최종적으로, 50 Epoch (대략 31분 정도)&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 의 training을 통해, 83.16%의 정확도를 얻을 수 있었습니다. 원본 AlexNet과 비교할 때, 대략 1/3 가까운 훈련시간으로 거의 비슷한 수준의 정확도를 얻을 수 있었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;GTX 1070Ti 하나를 사용했습니다. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="deep-learning-study" /><summary type="html">Contents</summary></entry><entry><title type="html">Pytorch-Cifar10</title><link href="http://localhost:4000/deep-learning-study/pytorch-cifar10/" rel="alternate" type="text/html" title="Pytorch-Cifar10" /><published>2021-11-25T00:00:00+09:00</published><updated>2021-11-25T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning-study/pytorch-cifar10</id><content type="html" xml:base="http://localhost:4000/deep-learning-study/pytorch-cifar10/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#data&quot; id=&quot;markdown-toc-data&quot;&gt;Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#models&quot; id=&quot;markdown-toc-models&quot;&gt;Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;ImageNet Challenge를 따라가며, CNN의 가장 큰 태스크중 하나였던 &lt;strong&gt;이미지 분류&lt;/strong&gt;에 사용되는 모델들의 구현을 공부합니다.&lt;/p&gt;

&lt;p&gt;코드는 &lt;a href=&quot;https://github.com/gratus907/Pytorch-Cifar10&quot;&gt;Github repository&lt;/a&gt; 에 업로드됩니다.&lt;/p&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;
&lt;p&gt;CIFAR10은 32 x 32의 매우 작은 이미지 6만개로 구성된 데이터셋으로, 이미지 분류에서 MNIST보다는 어렵고 Imagenet보다는 쉬운, 적당한 연습용 데이터셋으로 생각할 수 있습니다. 각 이미지는 10개 중 하나의 클래스로 라벨링이 되어있습니다.&lt;/p&gt;

&lt;p&gt;여기서는 5만개를 training에, 1만개를 test에 사용할 것입니다.&lt;/p&gt;

&lt;p&gt;Data augmentation은 다음과 같이 수행합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;가로세로 4만큼의 패딩&lt;/li&gt;
  &lt;li&gt;Random crop (32 by 32). Padding된 다음 자르는거라 이미지 위치가 정가운데가 아니게 만드는 효과가 있습니다.&lt;/li&gt;
  &lt;li&gt;Random flip&lt;/li&gt;
  &lt;li&gt;Normalization (Imagenet weight)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;style&gt;
table th:first-of-type {
    width: 15%;
}
table th:nth-of-type(2) {
    width: 40%;
}
table th:nth-of-type(3) {
    width: 25%;
}
table {
    width : 80%;
}
table td, table th {
    font-weight : 500;
}
&lt;/style&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Model Name&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Post Link&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;LeNet&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;66.77% (50 epoch)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;AlexNet&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/deep-learning-study/AlexNet/&quot;&gt;AlexNet : Explained&lt;/a&gt; &lt;br /&gt; &lt;a href=&quot;/deep-leanrning-study/alexnet-cifar10&quot;&gt;AlexNet on Cifar10&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;85.03% (50 epoch)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="deep-learning-study" /><summary type="html">Contents</summary></entry><entry><title type="html">CNN Architecture - AlexNet (2012)</title><link href="http://localhost:4000/deep-learning-study/AlexNet/" rel="alternate" type="text/html" title="CNN Architecture - AlexNet (2012)" /><published>2021-11-25T00:00:00+09:00</published><updated>2021-11-25T00:00:00+09:00</updated><id>http://localhost:4000/deep-learning-study/AlexNet</id><content type="html" xml:base="http://localhost:4000/deep-learning-study/AlexNet/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#architecture&quot; id=&quot;markdown-toc-architecture&quot;&gt;Architecture&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#2-gpu-training&quot; id=&quot;markdown-toc-2-gpu-training&quot;&gt;2-GPU Training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#usage-of-relu&quot; id=&quot;markdown-toc-usage-of-relu&quot;&gt;Usage of ReLU&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#local-response-normalization&quot; id=&quot;markdown-toc-local-response-normalization&quot;&gt;Local Response Normalization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#overlapping-pooling&quot; id=&quot;markdown-toc-overlapping-pooling&quot;&gt;Overlapping Pooling&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#training&quot; id=&quot;markdown-toc-training&quot;&gt;Training&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#regularization-techniques&quot; id=&quot;markdown-toc-regularization-techniques&quot;&gt;Regularization Techniques&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#optimization&quot; id=&quot;markdown-toc-optimization&quot;&gt;Optimization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#code&quot; id=&quot;markdown-toc-code&quot;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;AlexNet은 2012년 Imagenet challenge를 큰 격차로 우승하면서, image classification task를 통해 Deep neural network &amp;amp; GPU-computing의 시대를 연 모델이라는 평가를 받는 그런 아키텍쳐입니다. 이번 포스팅에서는 그런 AlexNet의 원본 논문을 따라가면서, 메인 아이디어들에 대해 살펴봅니다.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;../../images/8190f2b2db0f7eb370af157be64544adbe4c13e88488ef86e8d2bf9a60d90be2.png&quot; alt=&quot;picture 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기본적으로 AlexNet의 구조는 LeNet과 많이 다르지 않습니다. 보다 큰 이미지를 처리하기 위해 좀더 깊어진 구조라고 생각할 수 있는데요. Convolution layer 5개와 Fully connected layer 2개로 구성되어 있습니다. LeNet에서 본것처럼 Convolution layer들이 주변을 보면서 feature를 추출하고, 그 추출한 특징들을 마지막 linear layer에서 어떻게 합칠지를 고민해서 하나의 결과를 낸다고 생각할 수 있겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;2-gpu-training&quot;&gt;2-GPU Training&lt;/h3&gt;
&lt;p&gt;AlexNet은 몇가지 특이한 점이 있는데, 눈에 보이는 가장 큰 특징 중 하나는 위 그림에서 보듯 네트워크 전체를 두개로 나눠서 구현해놨다는 점입니다. 이는 당시 (2012) GPU 메모리가 3GB정도로 현재에 비해 부족했기 때문에 GPU 2개에 네트워크를 반씩 나눠서 돌리면서, 필요한 때만 서로간에 communicate하도록 한 것인데요. 지금에 와서는 GPU의 성능이 비약적으로 향상됨에 따라 굳이 이렇게 두개로 나누지 않아도 충분히 구현할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;usage-of-relu&quot;&gt;Usage of ReLU&lt;/h3&gt;
&lt;p&gt;LeNet에서도 그렇고, 이전까지의 많은 Neural network들은 activation function으로 $\tanh$ 나 sigmoid (어차피 거의 비슷하므로 sigmoid로 통칭하겠습니다) 같은 smooth한 함수를 사용했습니다. 그러나 AlexNet에서는 ReLU를 사용하면 훨씬 빠르게 training이 가능함을 주장하고 있으며, 이를 실험을 통해 확인하였습니다. 이부분에 대해서는 다양한 이야기와 생각해볼 이슈들이 있는데, ReLU는 계산 자체가 빠르게 가능한데다 양쪽에서 vanishing gradient 문제가 발생하는 sigmoid에 비해 이런 문제들이 덜하다는 장점을 직관적으로 생각해 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;AlexNet 논문에서는 이를 non-saturating이라고 부르고 있으며, 이후 많은 논문에서도 ReLU를 사용하여 이러한 이점을 얻고자 하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;local-response-normalization&quot;&gt;Local Response Normalization&lt;/h3&gt;
&lt;p&gt;Sigmoid의 경우 각 뉴런의 입력이 0 주위로 모아져야 learning의 효율이 발휘되기 때문에 (끝쪽으로 갈수록 미분계수가 0에 가까워서 아무 일도 일어나지 않음), 많은 네트워크들이 input normalization을 통해 이를 맞춰주려고 했습니다. ReLU는 0 이하만 아니라면 입력값에 따라 미분계수가 줄어들거나 하지는 않으므로 이게 꼭 필요하지는 않지만, AlexNet 논문에서는 Local normalization이라는 방법을 적용할 때 효율이 좋았다고 합니다. 다만, 이 방법은 이후 Batch normalization 등 다양한 방법들이 제시되고 이러한 방법들의 효율이 더욱 우수함이 밝혀짐에 따라 이후의 연구에서 더이상 계승되지 않았기 때문에 자세히 다루지는 않겠습니다.&lt;/p&gt;

&lt;p&gt;간단하게만 말하자면, convolution layer 한번이 필터를 여러개 쓰는 상황에서 적용하는 normalization입니다. 예를 들어 보자면 총 96개의 필터 (커널) 을 쓰는 상황에서 뭐 17번 필터의 결과값이 있을텐데, 이 값을 13번, 14번, …, 21번까지의 필터의 결과값을 이용하여 normalize하는 것입니다 (좌우로 4개씩 쓰는건 그냥 임의로 정한 값입니다) 이 방법은 실제 뇌에서의 신경생리학에 있어서 측면 억제 (lateral inhibition) 로부터 motivation을 얻은 방법이라고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;overlapping-pooling&quot;&gt;Overlapping Pooling&lt;/h3&gt;
&lt;p&gt;AlexNet에서는 일부 pooling을 서로 살짝 겹치게 수행하는데, 이 방법을 통해 overfitting을 크게 줄일 수 있다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;h3 id=&quot;regularization-techniques&quot;&gt;Regularization Techniques&lt;/h3&gt;
&lt;p&gt;AlexNet은 이전 LeNet에 비해 훨씬 더 큰 모델로, 파라미터의 개수가 훨씬 더 많기 때문에 overfitting의 우려가 큽니다. 이에 대응하기 위해, LeNet과 비교해 보면 훨씬 Regularization에 공을 들이는 것을 알 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2012년 당시에는 최신 테크닉이었던 (그러나, 이후에는 Batch normalization등의 활용으로 인해 그 효용이 많이 줄어든) dropout을 적용합니다. 구체적으로 fully-connected layer 중 앞 두 칸에 $p = 0.5$입니다.&lt;/li&gt;
  &lt;li&gt;SGD에 Weight decay 0.0005를 적용합니다. 논문에서는 그 이유를 명확하게 밝히고 있지는 않으나, 단순히 regularization일 뿐 아니라 실제로 training에 반드시 필요하다고 주장하고 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization&quot;&gt;Optimization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SGD with Momentum 0.9, weight decay 0.0005.&lt;/li&gt;
  &lt;li&gt;Learning rate는 0.01로 시작해서, loss가 줄어들지 않는 것 같아 보일때마다 1/10으로 줄이는 방법을 사용했습니다.&lt;/li&gt;
  &lt;li&gt;“Adjusted Manually”…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;/deep-learning-study/alexnet-cifar10&quot;&gt;AlexNet으로 CIFAR10 풀어보기&lt;/a&gt; 로 이어집니다&lt;/p&gt;</content><author><name></name></author><category term="deep-learning-study" /><summary type="html">Contents</summary></entry><entry><title type="html">논문읽기 : All Pairs Almost Shortest Path</title><link href="http://localhost:4000/cs-adventure/APASP/" rel="alternate" type="text/html" title="논문읽기 : All Pairs Almost Shortest Path" /><published>2021-11-22T00:00:00+09:00</published><updated>2021-11-22T00:00:00+09:00</updated><id>http://localhost:4000/cs-adventure/APASP</id><content type="html" xml:base="http://localhost:4000/cs-adventure/APASP/">&lt;div id=&quot;toc&quot;&gt;
Contents
&lt;/div&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#key-ideas&quot; id=&quot;markdown-toc-key-ideas&quot;&gt;Key ideas&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#at-a-glance&quot; id=&quot;markdown-toc-at-a-glance&quot;&gt;At a glance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dominating-set&quot; id=&quot;markdown-toc-dominating-set&quot;&gt;Dominating Set&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#on32m12-log-n-complexity-2-error&quot; id=&quot;markdown-toc-on32m12-log-n-complexity-2-error&quot;&gt;$O(n^{3/2}m^{1/2} \log n)$ complexity, 2-error&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#on73-log2-n-complexity-2-error&quot; id=&quot;markdown-toc-on73-log2-n-complexity-2-error&quot;&gt;$O(n^{7/3} \log^2 n)$ complexity, 2-error&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#further--time-accuracy-tradeoff&quot; id=&quot;markdown-toc-further--time-accuracy-tradeoff&quot;&gt;Further : Time-Accuracy Tradeoff&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#thoughts&quot; id=&quot;markdown-toc-thoughts&quot;&gt;Thoughts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;p&gt;이번에 공부해볼 논문은 &lt;strong&gt;All Pairs Almost Shortest Path&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;D. Dor, S. Halperin and U. Zwick, “All pairs almost shortest paths,” Proceedings of 37th Conference on Foundations of Computer Science, 1996, pp. 452-461, doi: 10.1109/SFCS.1996.548504.&lt;/p&gt;

&lt;p&gt;1996년도 논문으로, 이후에도 많은 논문들이 이 논문에서 제시한 bound를 더 내리는 등 더 contribution이 있지만, 아이디어가 흥미롭고 이후 논문들의 기반이 되는 연구입니다.&lt;/p&gt;

&lt;p&gt;연도에 대해 언급하는 이후는 이후 25년간 알고리즘 분야의 발전으로 현재에는 다른 문제들의 복잡도가 약간씩 달라졌기 때문입니다.&lt;/p&gt;

&lt;p&gt;그래프 문제에서 흔히 그렇듯, $G = (V, E)$에 대해 $\abs{V} = n$, $\abs{E} = m$으로 쓰되, $O(V)$ 와 같이 notation을 abuse하겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;All Pairs Shortest Path (이하 APSP) 란, 단순하게 어떤 그래프 $G = (V, E)$ 가 주어졌을 때, 각 vertex에서 다른 vertex로 가는 최단 경로의 길이를 모두 알아내는 문제입니다. 당연하게도, 실용적으로 많은 쓰임이 있습니다.&lt;/p&gt;

&lt;p&gt;우리는 우선 논문을 따라, Undirected의 경우만 고려합니다. 이때 경로의 길이는 지나가는 edge의 개수가 될 것입니다.&lt;/p&gt;

&lt;p&gt;다음 세 가지 알고리즘이 가장 먼저 고려할 만 합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Floyd-Warshal Algorithm : $O(V^3)$ 시간에 APSP를 해결하는 알고리즘입니다. 구현이 매우 쉽고, 간단한 Dynamic programming의 원리를 따르기 때문에 이해하기 쉽습니다.&lt;/li&gt;
  &lt;li&gt;Dijkstra (or any SSSP) : 다익스트라 알고리즘은 한 정점으로부터 다른 모든 정점까지의 거리를 찾는 Single Source Shortest Path (이하 SSSP) 문제를 $O(E + V \log V)$ 또는 $O(E \log V)$ 시간에 해결합니다. (Fibonacci heap의 사용 여부에 따라 다름) 자명하게, 모든 정점에서 출발하는 각 SSSP를 해결하면 되기 때문에 우리는 $O(VE + V^2 \log V)$ 정도 시간에는 APSP를 해결할 수 있습니다. 이 방법은 sparse graph에 대해서는 매우 빠르지만, dense graph에 대해서는 $E = O(V^2)$ 까지 가능하기 때문에 $O(V^3)$ 입니다.&lt;/li&gt;
  &lt;li&gt;Matrix Multiplication : Floyd-Warshall의 inner loop이 행렬곱셈이랑 비슷하게 생겼음을 활용합니다. 행렬곱셈과 거의 비슷한 알고리즘을 이용하면, Adjacency matrix $A$를 repeated squaring하여 이 문제를 $O(V^3 \log V)$ 에 풀 수 있습니다. 더 느려 보이는 이 방법을 언급하는 이유는, 행렬곱셈에 쓰는 빠른 알고리즘들 중 일부가&lt;sup id=&quot;fnref:matmul&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:matmul&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 이 ‘유사-행렬곱셈’에도 똑같이 적용가능하기 때문입니다. 스트라센을 쓰면 $O(V^{2.807} \log V)$ 이 되고 이는 위 두 방법보다 worst-case에 더 빠릅니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 논문에서는, “약간의 오차를 허용하면” 이보다 빠른 알고리즘이 존재함을 보입니다. 정확히는, 모든 정점쌍 $(v_i, v_j)$에 대해, $d(v_i, v_j) + 2$ 를 넘지 않는 값을 반환하는 알고리즘을 논의하고, 이를 확장하여 $k$만큼의 오차를 허용하되 그만큼 빨라지는 알고리즘을 논의합니다.&lt;/p&gt;

&lt;h2 id=&quot;key-ideas&quot;&gt;Key ideas&lt;/h2&gt;
&lt;h3 id=&quot;at-a-glance&quot;&gt;At a glance&lt;/h3&gt;
&lt;p&gt;이 알고리즘의 큰 아이디어는 다음과 같습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Dijkstra 알고리즘은 sparse graph에 대해 &lt;strong&gt;꽤&lt;/strong&gt; 빠릅니다.&lt;/li&gt;
  &lt;li&gt;그래프의 성질을 &lt;strong&gt;거의&lt;/strong&gt; 보존하면서 원래의 그래프보다 sparse한 auxillary graph 위에서 Dijkstra를 돌리고 싶습니다.&lt;/li&gt;
  &lt;li&gt;Unweighted graph에 대해서는 특히, dijkstra를 일반적인 BFS로 대체할 수 있습니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dominating-set&quot;&gt;Dominating Set&lt;/h3&gt;
&lt;p&gt;그래프 $(V, E)$에서, 어떤 정점의 부분집합 $S$가 다른 부분집합 $T$를 dominate 한다는 것은, $T$의 모든 vertex가 $S$의 vertex중 하나 이상과 edge로 연결되어 있는 것으로 정의합니다. 즉, 아래 그림에서 빨간색으로 색칠된 vertex는 그래프 전체를 dominate합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/82987572ed763cc4b5c4051dafdd2f09f351a476aeca2d83fdbf6ca5632322ea.png&quot; alt=&quot;image&quot; width=&quot;40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반적인 그래프에 대해 minimal dominating set을 찾는 것은 매우 잘 알려진 NP-Hard 문제입니다. 그러나, 우리는 좀 특수한 경우만 논의합니다. 구체적으로, 어떤 parameter $s$에 대해, 다음을 해결합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dominate(G, s)&lt;/strong&gt; : $G$에서, degree가 $s$이상인 정점들을 모은 부분집합을 dominate하는 적당히 작은 dominating set을 찾는다.&lt;/p&gt;

&lt;p&gt;여기서 &lt;strong&gt;적당히 작은&lt;/strong&gt; 이란, $O\left(\frac{n \log n}{s}\right)$를 목표로 합니다. 이를 위해 다음의 그리디가 잘 알려져 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;각 vertex에 대해 neighbor set을 생각하면, dominating set을 찾는 것은 set cover를 찾는 것과 같은 문제입니다.&lt;/li&gt;
  &lt;li&gt;각 집합 $S_i$의 원소는 $\Set{1, \cdots, n}$ 범위 내에 있고, 전체 원소의 합은 $2m$개 입니다.&lt;/li&gt;
  &lt;li&gt;Greedy 알고리즘을 생각합니다. 각 스텝마다, ‘현재까지 cover되지 않은 가장 많은 element를 새로 cover하는’ 집합을 택합니다.&lt;/li&gt;
  &lt;li&gt;이 알고리즘은 잘 생각해보면 $m$에 대해 선형으로 구현가능합니다.
    &lt;ul&gt;
      &lt;li&gt;각 $x \in \Set{1, \cdots, n}$에 대해, $x$를 포함하는 모든 집합의 리스트 $L$ 를 관리하고, 동시에 현재 남은 covering power (현재 uncovered인 vertex 개수) 가 $i$인 집합의 리스트 $A[i]$를 관리합니다. 이때 각 원소의 remaining covering power를 $P[i]$라고 하겠습니다.&lt;/li&gt;
      &lt;li&gt;식으로 쓰면 복잡해지므로, pseudocode를 이용해서 표현해 보겠습니다.
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cur = maximum of |S| among sets
while cur &amp;gt; 0:
    if A[cur] empty : cur -= 1, continue
    X = choose one with maximum P
    for t in X:
        if t is uncovered:
            mark t as covered
            for each Y containing t:
                remove Y from A[P[Y]]
                add Y to A[P[Y]-1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;시간 복잡도 증명 (위 알고리즘이 $O(\sum \abs{S})$) 임을 증명)
    &lt;ul&gt;
      &lt;li&gt;A를 관리하는 - 즉, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Y&lt;/code&gt;를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A[P[Y]]&lt;/code&gt;에서 꺼내서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A[P[Y]-1]&lt;/code&gt; 로 옮기는 decrease 연산이 $O(1)$에 시행가능하다면, 각 집합이 모두 그 크기만큼 decrease되어야 하므로 전체 복잡도는 각 집합의 원소의 개수의 합입니다. Bucket Queue 같은 자료구조를 이용하면 이게 가능함이 알려져 있습니다. (CLRS, 35-2-3).&lt;/li&gt;
      &lt;li&gt;그런데, 각 vertex의 neighbor 개수의 합은 결국 edge를 두번씩 센 것이므로 $2m$ 입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\log$ approximation
    &lt;ul&gt;
      &lt;li&gt;먼저 이 알고리즘이 얼마나 좋은 approximation을 제공하는지 생각해 봅니다. Optimal solution이 $k$개의 부분집합을 사용한다고 가정합니다. 전체 cover할 원소가 $n = n_0$개이고, 집합을 하나씩 택한후 각 step에서 “남은 cover해야할 원소의 수” 를 $n_i$라 합시다.&lt;/li&gt;
      &lt;li&gt;비둘기집의 원리에 따라, 어떤 집합은 $n / k$개보다 많은 원소를 cover해야만 합니다. WLOG, 이를 $S_1$로 택하면 $n_1 \leq (n - n/k) = n\left(1-\frac{1}{k}\right)$ 입니다.&lt;/li&gt;
      &lt;li&gt;다시 비둘기집의 원리에 따라, $n_1/(k-1)$ 개보다 많은 원소를 cover하는 집합이 존재해야 하고, 
\(n_2 \leq n_1\left(1-\frac{1}{k}\right)\left(1-\frac{1}{k-1}\right)\leq n\left(1-\frac{1}{k}\right)^2\)&lt;/li&gt;
      &lt;li&gt;Greedy set cover의 결과가 $u$개의 집합을 쓴다면, $u$번을 반복하여, $n_u \leq n\left(1-\frac{1}{k}\right)^u &amp;lt; 1$
이를 다시 정리하여 다음을 얻습니다. 
\(\left(1-\frac{1}{k}\right)^{k \cdot u/k} &amp;lt; \frac{1}{n}\)
$(1-x)^{1/x} \leq 1/e$ 이므로, 이를 잘 정리하면 $u &amp;lt; k \log n$을 얻습니다.&lt;/li&gt;
      &lt;li&gt;즉, Greedy set cover의 결과는 optimal결과보다 $\log n$배 이상 나쁘지 않습니다!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다만 위 알고리즘은 전체 graph의 dominating set을 제공합니다. 따라서, dummy vertex $s$개를 생성한 다음 degree가 $s$ 미만인 모든 vertex들에 대해 이들을 dummy vertex에 연결해 버리면 전체 edge는 최대 $ns$개 증가하고, 모든 vertex가 $s$이상의 degree를 가집니다. dummy vertex들은 어차피 high-degree vertex와는 연결되지 않으므로, dominating set에서 이를 마지막에 제거해도 됩니다. 이렇게 하면…&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dominate(G, s)&lt;/strong&gt; 의 결과값은 $O\left(\frac{n \log n}{s}\right)$개 이하의 dominating set을 찾아주고, 그 수행시간은 $O(m + ns)$ 입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;on32m12-log-n-complexity-2-error&quot;&gt;$O(n^{3/2}m^{1/2} \log n)$ complexity, 2-error&lt;/h3&gt;
&lt;p&gt;이 알고리즘은 Aingworth et al. &lt;sup id=&quot;fnref:aingworth&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:aingworth&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 에서 제시된, $\tilde{O}(n^{5/2})$ 알고리즘을 약간 개선한 알고리즘입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$s = (m / n)^{1/2}$ 라 하고, degree가 $s$ 이상인 vertex 집합 $V_1$ 과 미만인 집합 $V_2$로 $V$를 나눕니다.&lt;/li&gt;
  &lt;li&gt;Low-degree vertex를 touch하는 edge의 집합을 $E_2$라 합시다. 이때, 이들의 개수는 $ns$개 이하입니다. (Low degree인 정점 최대 $V$개, 각각의 degree 최대 $s$)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dominate(G, s)&lt;/strong&gt; 를 돌립니다. 이 집합은 $O((n \log n) / s)$ 개 이하이므로, $O((n^{3/2} \log n) / m^{1/2})$ 개 이하의 정점으로 구성된 집합입니다. 또한, dominating set $D$를 찾는 과정에서, $V_1$개의 edge를 이용하여 $V_1$의 각 정점을 $D$의 정점에 매달 수 있습니다. 이 $V_1$개의 edge집합 (편의상, dominating edge라 하겠습니다) $E^*$을 찾습니다.&lt;/li&gt;
  &lt;li&gt;이제, $u \in D$에 대해, BFS를 이용해 최단경로를 그냥 찾습니다. BFS는 한번에 $O(m)$ 시간이 걸리므로 (connected graph이므로 $m \geq n$), 이부분의 수행시간은 $O(\abs{D}m) = O(m^{1/2}n^{3/2}\log n)$ 입니다.&lt;/li&gt;
  &lt;li&gt;$u \in V-D$ 에 대해, 다음과 같이 weighted graph를 만들어 다익스트라를 돌립니다.
    &lt;ul&gt;
      &lt;li&gt;각 $u$에 대해, $G_2(u)$를 만듭니다. 이때, $G_2(u)$의 정점은 $V$이고, edge는 $E^*$ 와 $E_2$에 있는 edge들을 모두 취한 다음, 각 $v \in D$에 대해 $(u, v)$ edge가 $\delta(u, v)$ weight을 갖도록 만듭니다.&lt;/li&gt;
      &lt;li&gt;$\delta(u, v)$는 앞선 BFS, 구체적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BFS(v)&lt;/code&gt;에서 구한 값이므로 &lt;strong&gt;정확한 최단거리값&lt;/strong&gt; 입니다.&lt;/li&gt;
      &lt;li&gt;이때, $\abs{E^*} = V_1 = O(n)$, $\abs{E_2} = O(ns) = O(n^{1/2}m^{1/2})$, $\abs{\Set{u} \times D} = \abs{D}  = O((n^{3/2} \log n) / m^{1/2})$ 입니다.&lt;/li&gt;
      &lt;li&gt;이 그래프 위에서, 다익스트라 알고리즘의 수행시간은 $O(n + n^{1/2}m^{1/2} + (n^{3/2} \log n) / m^{1/2})$ 개의 간선을 가진 그래프에서 돌리는 다익스트라입니다. 이 Edge set의 크기는 $O(m^{1/2}n^{1/2} \log n)$ 이하입니다. 다시, 다익스트라는 최대 $\abs{V-D} \leq n$ 번 시행되므로 모두 합하면 이쪽도 $O(m^{1/2}n^{3/2} \log n)$ 이 될 것입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;따라서 전체 알고리즘은 $O(m^{1/2}n^{3/2} \log n)$ 입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이제, 이 알고리즘이 최대 2 이하의 오차를 가짐을 보입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;정점쌍 $(u, v)$에 대해 $u \to v$의 최단경로가 적어도 하나 이상의 High-degree 정점을 지난다면:
    &lt;ul&gt;
      &lt;li&gt;마지막으로 지나는 high-점을 $w$라 합시다. 이제, $w \to v$의 최단경로는 모두 low-degree 정점만으로 구성되어 있고, 이는 다시 $G_2(u)$ 에 통째로 들어갑니다. 이는 $G_2(u)$를 만들때, low-degree 정점을 한번이라도 터치하는 모든 간선들을 때려넣었기 때문입니다.&lt;/li&gt;
      &lt;li&gt;$w’$ 를 $w$를 dominate하는 $D$의 정점이라고 하겠습니다. $(w, w’)$ 간선 또한 $G_2(u)$ 에 들어갑니다.&lt;/li&gt;
      &lt;li&gt;$w’ \in D$ 이므로, $(u, w’) \in G_2(u)$이고, 이 간선의 가중치 $\delta(u, w’)$는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BFS(w&apos;)&lt;/code&gt;에서 구한 값이므로 &lt;strong&gt;정확&lt;/strong&gt;합니다.&lt;/li&gt;
      &lt;li&gt;Dominating은 바로 연결되어 있다는 뜻이므로, $\delta(u, w’) \leq \delta(u, w) + 1$ 입니다.&lt;/li&gt;
      &lt;li&gt;다익스트라 알고리즘에 의해 구해지는 거리를 $\hat{\delta}$ 이라 하면, $\hat{\delta}(u, v)$ 는 다음을 만족합니다. \(\hat{\delta}(u, w&apos;) + \hat{\delta}(w&apos;, w) + \hat{\delta}(w, v)\)&lt;/li&gt;
      &lt;li&gt;그런데…$\hat{\delta}(u, w’)$ 은 BFS에서 구한 weight가 넘어오므로 정확하고, 이는 다시 $\delta(u, w) + 1$ 이하입니다.&lt;/li&gt;
      &lt;li&gt;$\hat{\delta}(w’, w)$는 어차피 1입니다.&lt;/li&gt;
      &lt;li&gt;$\hat{\delta}(w, v)$ 는 다시 실제 최단경로가 $G_2(u)$에 들어있으므로 정확합니다.&lt;/li&gt;
      &lt;li&gt;그러면, 다음 부등식이 성립합니다. 
\(\hat{\delta}(u, v) \leq \delta(u, w) + 1 + 1 + \delta(w, v) \leq \delta(u, v)\)&lt;/li&gt;
      &lt;li&gt;따라서, 이렇게 구한 최단경로는 최대 2만큼의 오차를 가집니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;정점쌍 $(u, v)$에 대해 $u \to v$의 최단경로가 High-degree 정점을 지나지 않는다면, 최단경로가 통째로 $E_2$에 들어있고, 다익스트라 알고리즘이 정확한 경로를 반환합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;on73-log2-n-complexity-2-error&quot;&gt;$O(n^{7/3} \log^2 n)$ complexity, 2-error&lt;/h3&gt;
&lt;p&gt;알고리즘 자체는 거의 똑같지만, 이번에는 세조각으로 나눕니다. degree가 $n^{1/3}$ 이상, $n^{2/3}$ 이상인 점을 각각 MID, HIGH라 하고, MID와 HIGH의 dominating set을 찾습니다. (이렇게 표현하기는 했지만, MID는 HIGH를 &lt;strong&gt;포함&lt;/strong&gt; 합니다. MID이상, HIGH이상이라고 생각해주세요!)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MID의 dominating set $D_M$ 은 $O(n^{2/3} \log n)$ 크기이고, 구하는데는 $O(m + n^{4/3})$ 시간이 걸립니다.&lt;/li&gt;
  &lt;li&gt;HIGH의 dominating set $D_H$ 는 $O(n^{1/3} \log n)$ 크기이고, 구하는데는 $O(m + n^{5/3})$ 시간이 걸립니다.&lt;/li&gt;
  &lt;li&gt;위에서와 같은 방법으로, MID에 포함되지 않는 정점 (즉 degree가 “작은” 정점) 을 하나라도 터치하는 edge의 집합을 $E_M$, HIGH에 포함되지 않는 정점 (degree가 “크지 않은” 정점) 을 하나라도 터치하는 edge의 집합을 $E_H$라 합니다. $E_M$의 크기는 $O(n^{4/3})$, $E_H$의 크기는 $O(n^{5/3})$ 입니다.&lt;/li&gt;
  &lt;li&gt;$D_H$ 에서 BFS를 돌립니다. 이는 $O(mn^{1/3}\log n)$ 시간이 걸립니다.&lt;/li&gt;
  &lt;li&gt;$D_M$ 에서 BFS를 돌리되, 이번에는 모든 간선을 쓰는 대신 $E_H$에 속하는 간선만 씁니다. 이는 $O(n^{2/3}n^{5/3}\log n) = O(n^{7/3} \log n)$ 시간이 걸립니다.&lt;/li&gt;
  &lt;li&gt;마지막으로, 나머지 점들에서는 Dijkstra를 돌립니다. 이때, 간선은 $E_M$에 속하는 간선들, dominate에 쓰인 간선들, $(D_H \times V)$ 에 속하는 간선들, $(D_M \times D_M)$ 에 속하는 간선들, $(\Set{u} \times D_M)$ 이렇게만 고릅니다.
    &lt;ul&gt;
      &lt;li&gt;$E_M$이 $O(n^{4/3})$, dominate에는 $n$개가 쓰이고,&lt;/li&gt;
      &lt;li&gt;$(D_H \times V)$ 가 또 $O(n^{4/3} \log n)$개, $D_M \times D_M$ 이 $O(n^{4/3} \log^2 n)$ 개, $(\Set{u} \times D_M)$이 $O(n^{2/3} \log n)$ 개입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;총 edge가 $O(n^{4/3} \log^2 n)$ 개이고, 돌리는 횟수가 $n$번 이하이므로 $O(n^{7/3} \log^2 n)$ 시간에 수행됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;같은 방법으로 2-error를 보입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;만약 최단경로가 HIGH의 정점을 하나라도 지난다면, 그중 마지막을 $w$라 하고 $D_H$에서 $w$를 dominate하는 $w’$를 고릅니다.
    &lt;ul&gt;
      &lt;li&gt;$\delta(u, w’)$ 는 $w’ \in D_H$ 이므로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BFS(w&apos;)&lt;/code&gt;에 의한 정확한 거리를 압니다. 이는 $\delta(u, w) + 1$ 보다 작거나 같습니다.&lt;/li&gt;
      &lt;li&gt;$\delta(w’, v)$ 는 역시 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BFS(w&apos;)&lt;/code&gt;에 의해 정확한 거리를 알고, $\delta(w, v) + 1$ 보다 작거나 같습니다.&lt;/li&gt;
      &lt;li&gt;따라서, 이를 값은 $\delta(u, w) + 2 + \delta(w, v)$ 보다 작거나 같습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;만약 최단경로가 HIGH는 아예 안 지나지만, MID를 지난다면, 그중 마지막을 $w$라 하고 $D_M$에서 $w$를 dominate하는 $w’$를 고릅니다.
    &lt;ul&gt;
      &lt;li&gt;$\delta(u, w’)$ 에 대한 근사값은 $D_M$에서의 BFS(두번째 BFS) 도중에 비슷하게 찾을 수 있습니다. 그러나, 이번에는 $D_M$이 전체 그래프에 대고 BFS를 한 결과가 아니기 때문에 이게 정확하다는 보장이 없어서, $\delta_2(u, w’)$라고 쓰겠습니다. HIGH를 아예 안 지나는 경로가 최단임을 가정에서 보장했으므로 $\delta_2(u, w) = \delta(u, w)$ 이고, 결과적으로 $\delta_2(u, w’)$ 값은 $1 + \delta(w, u)$ 이하입니다.&lt;/li&gt;
      &lt;li&gt;$\delta(w’, w)$ 는 1이고&lt;/li&gt;
      &lt;li&gt;$\delta(w, v)$ 는 모두 low-touching 임이 보장되므로 다익스트라의 결과로 정확한 값을 구할 수 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서, 이 알고리즘은 $O(n^{7/3} \log^2 n)$ 시간에 2-error approximation을 보장합니다.&lt;/p&gt;

&lt;h3 id=&quot;further--time-accuracy-tradeoff&quot;&gt;Further : Time-Accuracy Tradeoff&lt;/h3&gt;
&lt;p&gt;이를 이용하여, 2조각이나 3조각이 아닌 더 많은 조각으로 자르면 시간 복잡도가 더 낮아지는 대신 에러를 더 허용하게 됩니다. 구체적으로,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;첫번째 알고리즘을 확장, $s_i = (m / n)^{1-i/k}$을 이용하여 계산하는 알고리즘은 $\tilde{O}(n^{2-1/k}m^{1/k})$ 시간에 $2(k-1)$ 에러를 허용하고,&lt;/li&gt;
  &lt;li&gt;두번째 알고리즘을 확장, $s_i = n^{1-i/k}$를 이용하여 $D_{j} \times D_{j’}$ 같은 간선들을 추가해서 다익스트라를 쓰는 알고리즘은 $\tilde{O}(n^{2+1/k})$ 시간에 작동하는 대신 $2(\floor{k/3} + 1)$ 에러를 허용하게 됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이를 합쳐서, $u$만큼의 에러 바운드를 고정하고 싶으면, 첫번째 경우의 $k = u/2 + 1$ 또는 두번째 경우에서 $k = 3u - 2$를 사용, 
\(\min(\tilde{O}(n^{2-\frac{2}{u+2}}m^{\frac{2}{u+2}}), \tilde{O}(n^{2+\frac{2}{3u-2}}))\)
이정도 시간복잡도에 최대 $u$만큼의 에러를 허용하는 APSP를 풀 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;지나치게 난해하지 않은 아이디어 (그래프의 edge를 HIGH-LOW로 나눈 다음, 이 위에서 경우에 따라 알고리즘을 바꿔끼움으로써 복잡도 개선하는 아이디어는 의외로 많이 보입니다) 를 이용하여 복잡도를 내리는 부분이 인상적이었습니다.&lt;/li&gt;
  &lt;li&gt;그래프가 dense하다면 최단경로가 생각보다 짧고, 그래프가 sparse하다면 $n$번의 다익스트라가 $O(nm)$으로 빠르기 때문에 이 알고리즘이 실용적으로 쓰이려면 $m$이 특정한 범위에 있고, 그래프가 꽤 커야 할 것 같습니다. 이론적인 복잡도 분석 측면에서 보다 큰 의미가 있는 듯 합니다.&lt;/li&gt;
  &lt;li&gt;이 논문의 Journal full version에는 weighted graph에서 최단경로의 3배 이하를 estimate하는 spanner에 대한 알고리즘이 추가로 논의되는것 같습니다. weighted graph는 이상한 케이스를 만들기가 쉽기 떄문에 이부분도 상당히 재밌을것 같습니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:matmul&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;이론적으로 가장 빠른 알고리즘 (Coppersmith-Winograd), 실용적으로 빠른 알고리즘 (Strassen)이 이 min-plus-mm에도 적용가능함이 알려져 있습니다. &lt;a href=&quot;#fnref:matmul&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:aingworth&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;D. Aingworth, C. Chekuri, and R. Motwani. Fast estimation of diameter and shortest paths (without matrix multiplication). SODA 1996 &lt;a href=&quot;#fnref:aingworth&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="cs-adventure" /><summary type="html">Contents</summary></entry></feed>