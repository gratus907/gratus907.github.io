<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://gratus907.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://gratus907.github.io/" rel="alternate" type="text/html" /><updated>2024-10-12T00:15:32+09:00</updated><id>https://gratus907.github.io/feed.xml</id><title type="html">Wonseok Shin</title><subtitle>Wonseok Shin, SNU CSE
</subtitle><entry><title type="html">[Randomized Algorithms] Median-of-Mean Trick</title><link href="https://gratus907.github.io/theory/Median-of-Mean-Trick/" rel="alternate" type="text/html" title="[Randomized Algorithms] Median-of-Mean Trick" /><published>2024-06-30T00:00:00+09:00</published><updated>2024-06-30T00:00:00+09:00</updated><id>https://gratus907.github.io/theory/Median-of-Mean-Trick</id><content type="html" xml:base="https://gratus907.github.io/theory/Median-of-Mean-Trick/"><![CDATA[<p>오늘 포스팅에서는 randomized algorithm 을 비롯하여, estimation 에서 매우 흔히 활용되는 중간값을 이용한 기법을 소개합니다.</p>

<p>이하, 확률변수 $X$의 기댓값, 분산, 표준편차를 기댓값 $\E[X]$, 분산 $\V[X]$, 표준편차 $\sigma[X] = \sqrt{\V[X]}$ 로 쓰겠습니다.</p>

<h3 id="problem">Problem</h3>
<p>어떤 미지의 값에 대한 unbiased estimator random variable $X$가 주어진다고 가정합니다. 
(여기서 unbiased라 함은, $\E[X] = \mu$ 가 우리가 추정하고자 하는 미지의 값과 같음을 의미합니다.)
Randomized algorithm의 맥락에서는, 알고리즘의 출력이 확률변수로 표현되므로 그 기댓값이 참값과 같은 상황이라고 생각할 수 있습니다.</p>

<p>여기서 추가로, $\V[X]$ 가 최대 $\E[X]^2$ 에 비례하는 상황을 가정합니다. (즉, $\sigma[X] = c\E[X]$. 사실 이하의 논의는 부등호 $\leq$ 로도 잘 성립합니다) 
얼핏 보면 약간 특수해 보이지만, 사실 이 상황은 알고리즘 분석을 비롯해서 매우 흔히 발생합니다.</p>

<p>우리의 목표는 어떤 상수 $\delta$와 $\epsilon$에 대해, $1 - \delta$ 확률로 $\epsilon$ 만큼의 상대 오차를 보장하고 싶습니다. 
즉, 확률변수 $Y$가 다음을 만족하도록 하고자 합니다. 
<span style="display:block" class="math_item">
    <b class="math_item_title">확률변수에 의한 $(\epsilon, \delta)$-근사</b><br />
    Unbiased estimator $Y$와 참값 $\mu = \E[Y]$에 대해, 다음이 성립할 때 $Y$를 $\mu$에 대한 $(\epsilon, \delta)$-근사라고 정의한다.
    \(\P[\abs{Y - \mu} \geq \epsilon\mu] \leq \delta\)
</span> 
(이 용어는 표준으로 쓰는 용어는 아니고, 편의상 도입한 것입니다)</p>

<hr />

<h3 id="naive-approach">Naive Approach</h3>
<p>어떤 확률변수 $X$에 대해, 다음이 잘 알려져 있습니다.</p>

<p><span style="display:block" class="math_item">
    <b class="math_item_title">Chebyshev’s Inequality</b><br />
    확률변수 $X$의 기댓값 $\mu = \E[X]$, 분산 $\V[X]$, 표준편차 $\sigma[X] = \sqrt{\V[X]}$에 대하여, 다음이 성립한다. 
    \(\P[\abs{X - \mu} \geq k\sigma[X]] \leq \frac{1}{k^2}\)
</span> 
우리는 $\sigma[X] = c\mu$를 가정하였으므로, 이는 다시 말해 $X$에 대해 다음이 성립한다는 의미입니다.
\(\P[\abs{X - \mu} \geq kc\mu] \leq \frac{1}{k^2}\)</p>

<p>무언가를 더 잘 추정하기 위해서 생각할 수 있는 가장 나이브한 방법은, 여러 개의 estimator를 independent하게 실행한 다음 그들의 평균을 취하는 것입니다. 
$T$개의 평균을 취하면, 기댓값과 분산에는 아래 관계가 성립합니다. 
\(Y = \frac{1}{T}\sum_{i = 1}^{T} X_i \qquad \Rightarrow \qquad \E[Y] = \E[X],\quad \V[Y] = \V[X] / T\)</p>

<p>따라서, $k = 1 / \sqrt{\delta}$로 두고, $\sigma[Y] = \epsilon\sqrt{\delta}\mu$ 가 되도록
$T = \frac{c}{\epsilon^2 \delta}$ 개를 잡으면 목표하는 $(\epsilon, \delta)$ 근사를 달성할 수 있습니다.</p>

<p><span style="display:block" class="math_item">
    <b class="math_item_title">Theorem.</b><br />
    Unbiased estimator $X$가 $\V[X] = O(\E[X]^2)$를 만족하면, $O(1 / (\epsilon^2 \delta))$ 개로 $(\epsilon, \delta)$ 근사를 달성할 수 있다. 
</span></p>

<hr />

<h3 id="median-of-mean-trick">Median-of-Mean Trick</h3>
<p>오늘 포스팅의 메인 주제인, 이 방법보다 더 적은 개수로 목표를 달성할 수 있는 방법입니다.</p>
<ul>
  <li>Estimator $X$를 $4c / \epsilon^2$ 번만큼 실행하여 평균을 취한다. 이를 Estimator $Z$ 라 한다.</li>
  <li>Estimator $Z$를 $12 \log (1 / \delta)$ 번만큼 실행하여 <strong>중간값</strong> 을 취한다. 이를 Estimator $Y$ 라 한다.</li>
</ul>

<p>이렇게 얻어진 $Y$가 $\mu$의 $(\epsilon, \delta)$ 근사가 됩니다!</p>

<p>수식을 정확히 전개하기 위해 상수를 잘 맞춰놓았지만, 실제로는 $O(1 / \epsilon^2)$, $O(\log (1 / \delta))$ 만 기억하면 충분합니다.</p>

<h4 id="intuitive-explanation">Intuitive Explanation</h4>
<p>중간값은 평균에 비해 양쪽 끝의 outlier에 덜 민감한 값입니다. 
여러 개의 estimator들이 <strong>대체로</strong> 좋은 값을 제공한다면, 낮은 확률로 값이 크게 튀더라도 중간값은 그 영향을 많이 받지 않고 robust하게 좋은 값을 얻을 수 있습니다.</p>

<p>아래의 증명은 사실 이 아이디어를 formal하게 작성한 것에 지나지 않습니다.</p>

<hr />

<h3 id="proof-of-epsilon-delta-approximation">Proof of $(\epsilon, \delta)$ Approximation</h3>
<p>위 설명에서, <em><strong>대체로</strong> 좋은 값을 제공한다</em> 
를 먼저 생각하고 들어갑니다. Estimator $Z$에 대해, 다음이 Chebyshev 부등식에 의해 성립합니다. 
\(\P[\abs{Z - \mu} \geq \epsilon\mu] \leq \frac{\V[Z]}{\epsilon^2\mu^2} = \frac{1}{4}\)
즉, $Z$ 는 상수 (3/4) 확률로 우리가 원하는 좋은 estimation을 제공합니다. 목표는 이 확률을 $1 - \delta$ 까지 높이는 것입니다.</p>

<p>$r$개의 $Z$ estimator들의 중간값이 우리가 원하는 범위를 벗어나기 위해서는, 적어도 $r / 2$ 개만큼이 우리가 원하는 범위 밖에 있어야 합니다. 
($r / 2$개의 값이 구간 $[a, b]$ 사이에 있다면, 중간값이 그 범위에 들어간다는 것은 쉽게 생각할 수 있습니다) 
이를 <strong>나쁜</strong> estimator라 하면, $r / 2$개의 값이 <strong>나쁠</strong> 확률은, 성공 확률이 $1/4$ 이하인 베르누이 시행을 $r$번 해서 성공 횟수가 $r/2$번 이상이기를 기대한다는 의미이므로
\(\P[\text{at least } r / 2 \text{ estimators are bad}] = \P[B(r, 1/4) \geq r/2]\)
로 쓸 수 있습니다. 이 확률을 계산하기 위해, Chernoff Bound를 적용합니다.</p>

<p><span style="display:block" class="math_item">
    <b class="math_item_title">Chernoff Bound (for sum of indep. random variables).</b><br />
    확률변수 $X_1, X_2, \dots X_n$이 각각 $\set{0, 1}$ 에서 값을 갖는 independent random variable 이고, $X = \sum X_i, \mu = \E[X]$ 일 때, $t \in (0, 1)$에 대해 다음이 성립한다. 
    \(\P[X \geq (1 + t) \mu] \leq \exp\left(-t^{2}\mu/3\right)\)
</span> 
이를 적용하면, $\P[B(r, 1/4) \geq r/2] \leq e^{-r / 12}$ 를 얻습니다. 
따라서, $r = 12 \log (1 / \delta)$ 를 택하면, 
\(\P[\text{median is bad}] \leq \P[\text{at least } r / 2 \text{ estimators are bad}] \leq \delta\)
가 성립하므로, 중간값 $Y$는 $(\epsilon, \delta)$ 근사입니다.</p>

<p>그러므로, 다음의 정리가 성립합니다. 
<span style="display:block" class="math_item">
    <b class="math_item_title">Theorem.</b><br />
    Unbiased estimator $X$가 $\V[X] = O(\E[X]^2)$를 만족하면, $O(\frac{1}{\epsilon^2}\log(1 / \delta))$ 개로 $(\epsilon, \delta)$ 근사를 달성할 수 있다. 
</span></p>

<p>앞선 정리와 비교하면, $1 / \delta$ term을 $\log(1 / \delta)$로 개선한 것이 됩니다.</p>

<p>중간값에 대해서는 재밌는 성질이 알려져 있습니다.</p>

<p>Popultation median <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> 이 $\tilde{\mu}$인 $X$에서 추출한 sample의 median에 대해,
<span style="display:block" class="math_item">
    <b class="math_item_title">Theorem. <a class="citation" href="#medianthm">[1]</a></b><br />
    (약간의 가정 하에서) $X$의 density function이 $f$, $X$의 median이 $\tilde{\mu}$일 때, $X$에서 $2m+1$ 개를 sample하여 얻은 median의 분포는 
    평균이 $\tilde{\mu}$, 분산이 $\frac{1}{8f(\tilde{\mu})^2 m}$인 정규분포로 수렴한다. 
</span> 
위 정리는 absolute continuity, infinite population 등 몇가지 technical하고 mild한 가정이 들어가지만, 
일단은 이부분은 우리의 목표인 randomized algorithm의 경우에는 크게 중요하지는 않습니다.</p>

<p>따라서, 평균의 중간값이 어떤 분포를 보일지는 대략 알 수 있습니다. 
저 정리에서 우리가 필요한 $f$ 는 (알기 어려운) 원래 estimator의 probability density가 아닌, 그 표본평균의 pdf입니다 ($Z$에 대해 중간값을 취하므로)
<strong>Central Limit Theorem</strong>에 의해, <strong>표본 평균의 분포</strong> 는 정규분포로 수렴하므로, 충분히 큰 sample size에서 $Z$ estimator는 거의 정규분포이고…
그러면 $f$ 값도 정확하게 구할 수 있기 때문입니다.</p>

<p>CLT의 정확한 statement를 이용하면, 분포의 거리 상에서 많은 것을 더 논의할 수 있지만, 이 포스팅의 범위를 벗어나므로 생략하겠습니다.</p>

<hr />

<h3 id="평균의-중간값-vs-중간값의-평균">평균의 중간값 vs 중간값의 평균</h3>
<blockquote>
  <p>이 장에서, 분포가 <strong>정규분포이다</strong> 라고 말하는 많은 경우에 사실은 <strong>CLT에 의해</strong> 정규분포로 수렴한 경우를 말합니다. 
이때의 분포는 정확히 정규분포가 되지 않으므로, 모든 수식전개는 약간의 :hand-waving: 이 포함되어 있습니다. 
CLT에 의해 얻어지는 분포에 대해 무언가를 논증하려면 convergence in distribution 등 복잡한 개념을 적용해야 하며, 
과정에서 수렴을 한번씩 쓸때마다 약간씩 문제가 있습니다.</p>
</blockquote>

<p><strong>평균의 중간값</strong>이어야 하는 이유가 있을까요? 구체적으로는, <strong>중간값의 평균</strong> 으로는 안 될까요?</p>

<p>중간값의 평균에는 크게 두 가지 문제가 있습니다.</p>

<ol>
  <li>
    <p>첫째로, 중간값의 평균은 원래 $X$의 기댓값이 유지된다는 (unbiasedness) 보장이 없습니다. 
만약 분포가 한쪽으로 기울어져 있어, 중간값이 평균과 다르다면, 각 sample의 median에 bias가 발생합니다. 이후 평균을 취하는 과정에서 이 bias가 사라지지 않습니다.</p>
  </li>
  <li>
    <p>그렇다면, <strong>대칭인</strong> 분포에서는 똑같을까요? 그렇지 않습니다. 대칭인 분포에서는 unbiasedness는 보장할 수 있습니다 (median = mean이므로)
그러나 이 경우 어느 쪽이 더 나은 estimator인지는 약간 어려운 문제이고, 일반적으로는 평균의 중간값이 더 좋은 결과를 제공합니다.</p>
  </li>
</ol>

<p>1번의 경우 조금만 생각해 보면 바로 알 수 있습니다. 2번의 경우는 그렇지 않으므로, 이부분을 더 생각해 보고자 합니다.</p>

<p>수식에 대해서 생각하기 전에, 실험을 약간 해보겠습니다. $X$가 $[-1, 1]$ 에서 uniform한 분포라고 생각해 봅시다. 
이 분포의 평균은 0이고, 분산은 $1 / 3$ 이 됩니다. (앞서 알고리즘의 프레임워크에 맞추면, 우리가 예측하고자 하는 값이 0이고 분산이 $1/3$이라고 생각하면 됩니다)</p>

<p>이 분포에서, 10,000개의 sample을 뽑아 이를 100개씩 100개의 그룹으로 나누어, 
1) 각 group의 mean을 구하고 median을 취하는 경우
2) 각 group의 median을 구하고 mean을 취하는 경우
를 5,000번 반복하면, 아래와 같은 결과를 얻습니다.</p>

<p><img src="../../images/median-of-mean.png" alt="experiment" /></p>

<p>Median-of-mean의 분포가 더 좋은 (분산이 작고, 0 주변으로 몰린) 것을 확인할 수 있습니다. 구체적으로는 표준편차가 약 0.0097 vs 0.007로, 40%정도의 개선이 있습니다. 
Kolmogorov–Smirnov test를 수행했을 때도 $p &lt; 2.5 \times 10^{-13}$으로, <strong>명확히 통계적으로 유의하게</strong> 개선이 있습니다.</p>

<p>다시 수식으로 돌아와서 생각해 보겠습니다. 앞서의 theorem에 대입해 보면…</p>

<p>$X$가 기댓값 $\mu$와 분산 $\sigma^2$을 가진다고 할 떄, $(2m+1)$개를 묶어 median을 취하고, 이것을 $(2k+1)$개 묶어서 mean을 취한다면, 
정규분포로부터 얻은 sample mean은 분산을 sample size로 나눈 정규분포를 따름이 잘 알려져 있으므로, 
\(\hat{\mu} = \mu, \qquad \hat{\sigma}^2 = \frac{1}{(8f(\mu)^2 m) (2k + 1)}\)
의 parameter를 갖는 정규분포로 수렴하게 되며, (대칭인 분포를 생각하므로, $\tilde{\mu} = \mu$ 입니다)
여기서 $f$는 원래 $X$ estimator의 pdf입니다.</p>

<p>반대로, $(2k+1)$ 개를 묶어 mean을 취하고, 이것을 $(2m+1)$개 묶어서 median을 취하는 경우를 생각해 보겠습니다. 
Central Limit Theorem에 의해, $(2k+1)$개의 sample mean의 분포는 (원래 분포가 정규분포가 아니어도) 평균 $\mu$, 분산 $\sigma^2 / (2k + 1)$ 의 정규분포입니다. 
정규분포에 대해, 위 median theorem을 적용하면, 평균의 중간값은
\(\hat{\mu} = \mu, \qquad \hat{\sigma}^2 = \frac{1}{(8g(\mu)^2 m)}\)
의 parameter를 갖는 정규분포로 수렴하게 되며, 여기서 $g$는 $X$로부터 얻은 $(2k+1)$개의 sample mean의 분포가 갖는 pdf입니다.</p>

<p>그러므로 결국 중요한 것은 $f(\mu)$ 와 $g(\mu)$가 얼마나 차이가 나는가의 문제가 됩니다. 구체적으로는, 평균의 중간값이 더 나은 estimator라는 주장이 참이기 위해서는, 
$g(\mu)$ 가 $f(\mu)$ 의 $\sqrt{2k + 1}$ 배 이상이 되어야 합니다.</p>

<p>$X$가 정규분포인 경우, $\mu$에서의 값이 $1 / \sigma$에 비례하므로 $g(\mu)$가 정확히 $\sqrt{2k+1}$ 배가 되며, 이때 두 분포 (med-of-mean / mean-of-med) 가 정확히 같아지게 됩니다.</p>

<p>$X$가 다른 분포인 경우, $g(\mu)$는 여전히 정규분포에 수렴하므로 $g(\mu)$ 는 $\frac{\sqrt{2k+1}}{\sigma \sqrt{2\pi}}$ 가 됩니다. 그러므로 $f(\mu)$ 가 $\frac{1}{\sigma \sqrt{2\pi}}$ 보다 큰지 작은지에 따라 갈리게 되며, 이 값은 <strong>정규분포에서 $\mu$ 위치의 pdf값</strong> 입니다. 
따라서, <strong>정규분포보다 concentrated된</strong><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> 분포를 갖는 $X$에 대해서는 중간값의 평균이, 그렇지 않은 $X$에 대해서는 평균의 중간값을 택하는 것이 유리합니다.</p>

<p>그 조건을 만족하는 대표적인 분포로는 Laplace 분포가 있습니다 (PDF의 최댓값이 $\sqrt{\pi}$배만큼 더 큽니다)</p>

<p>같은 실험을 Laplace분포에 대해 진행하면, 아래 결과를 얻습니다. $\mu = 0, b = 1$ 인 Laplace 분포로부터 위와 똑같이 5,000 * 100 * 100 실험을 하면,</p>

<p><img src="../../images/median-of-mean-2.png" alt="experiment" /></p>

<p>와 같이, 중간값의 평균이 더 나은 분포를 가짐을 볼 수 있습니다.</p>

<p>다만, 중간값의 평균의 경우 위와 같이 Chebyshev-Chernoff를 이용한 논증이 똑같이 적용되지는 않고, CLT에 의존해서 증명해야 하며, rigor가 약간 애매해지는 문제가 있습니다.</p>

<hr />

<h3 id="applications">Applications</h3>
<p>Count-Min, Count-Sketch 등 sublinear sketching에 많이 사용됩니다. 언젠가 포스팅하면 링크를 추가할 예정입니다.</p>

<hr />
<p><strong>References</strong></p>

<ol class="bibliography"><li><script>
window.onload = function() {

    var venue_dict = {
        "\\NeurIPS": "Advances in Neural Information Processing Systems",
        "\\ICLR": "International Conference on Learning Representations",
        "\\CVPR": "Conference on Computer Vision and Pattern Recognition",
        "\\ICML": "International Conference on Machine Learning",
        "\\WWW": "Proceedings of ACM Web Conference",
        "\\VLDB": "Proceedings of the VLDB Endowment",
        "\\ICDE": "Proceedings of IEEE ICDE",
        "\\ICDM": "Proceedings of IEEE ICDM",
        "\\ICDT": "Proceedings of ICDT",
        "\\TKDE": "IEEE Transactions on Knowledge and Data Engineering",
        "\\SIGMOD": "Proceedings of ACM SIGMOD",
        "\\SIGKDD": "Proceedings of ACM SIGKDD",
        "\\SODA": "Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms",
    }
    function foo(bookTitle) {
        if (bookTitle in venue_dict) {
            return venue_dict[bookTitle];
        }
        else 
            return bookTitle;
    }
    
    var bookTitleElements = document.getElementsByClassName('entryBookTitle');
    console.log(bookTitleElements, bookTitleElements.length);
    for (let i = 0; i < bookTitleElements.length; i++) {
        var elem = bookTitleElements.item(i);
        var originalBookTitle = elem.getAttribute('data-booktitle');
        var transformedBookTitle = foo(originalBookTitle);
        elem.textContent = transformedBookTitle;
    }
    
}
</script>

<p> 
    
              
            
            
                A. Merberg,
            
              
            
            
                and S. J. Miller,
            
        
    
<b>Course Notes for Math 162: Mathematical Statistics, The Sample Distribution of the Median</b>, 


(2008)

</p></li></ol>

<hr />
<p><strong>Footnotes</strong></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>확률분포 $X$의 median이란, cumulative distribution function $F_X$에 대해 $F_X(t) = 1/2$ 가 되는 지점을 말합니다. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>여기서의 의미는, 평균에서의 함수값 $f(\mu)$ 이 정규분포보다 크다는 의미입니다. 일반적으로 대칭이고 $\abs{x - \mu}$에 대해 단조감소하는 bell curve형태의 pdf를 생각하면, curve가 뾰족함을 의미합니다. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="theory" /><category term="study" /><category term="algorithms" /><category term="randomized-algorithms" /><category term="probability-theory" /><summary type="html"><![CDATA[오늘 포스팅에서는 randomized algorithm 을 비롯하여, estimation 에서 매우 흔히 활용되는 중간값을 이용한 기법을 소개합니다.]]></summary></entry><entry><title type="html">2024년 6월 알고리즘 문제풀이</title><link href="https://gratus907.github.io/competitive-programming/Jun24-ProblemSolving/" rel="alternate" type="text/html" title="2024년 6월 알고리즘 문제풀이" /><published>2024-06-29T00:00:00+09:00</published><updated>2024-06-29T00:00:00+09:00</updated><id>https://gratus907.github.io/competitive-programming/Jun24-ProblemSolving</id><content type="html" xml:base="https://gratus907.github.io/competitive-programming/Jun24-ProblemSolving/"><![CDATA[<p>마지막 UCPC를 위해 팀연습을 돌았습니다. 
이제와서 딱히 수상에 미련은 없고, 마지막으로 참가하는 PS 팀 대회를 재밌게 즐길 요량으로 항상 함께했던 <code class="language-plaintext highlighter-rouge">dlwocks31</code>, <code class="language-plaintext highlighter-rouge">dhdroid</code> 와 같이 나가기로 했습니다 :)</p>

<p>두문제는 여기에, 나머지 팀연습은 7월에 올라갈 예정입니다.</p>

<h3 id="2022-icpc-southwest-european-regional-contest-c-another-wine-tasting-event">2022 ICPC SouthWest European Regional Contest C. Another Wine Tasting Event</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 27595 / SWERC 2022C</code><br />
난이도: <span style="color: rgb(255, 176, 40);">Gold I</span></p>

<p><strong>문제 요약:</strong> <code class="language-plaintext highlighter-rouge">W</code> 와 <code class="language-plaintext highlighter-rouge">R</code>로 이루어진 $2n-1$ 길이의 string에 대해, 다음 조건을 만족하는 어떤 $k$ 값을 하나 찾으시오</p>
<ul>
  <li>서로 다른 적절한 $n$개의 부분구간 $[a_i, b_i]$ 들을 골라서 (겹쳐도 좋지만, 정확히 같아서는 안 됩니다)</li>
  <li>각각의 길이가 $n$보다 길거나 같고,</li>
  <li>각각이 모두 정확히 $k$개의 <code class="language-plaintext highlighter-rouge">W</code> 를 가지고 있게 할 수 있다.</li>
</ul>

<p>예를 들어, <code class="language-plaintext highlighter-rouge">RWWRRRWWW</code> 는 구간 $[2, 6], [1, 5], [1, 6], [4, 8], [3, 7]$을 고르면 각각이 2개씩의 <code class="language-plaintext highlighter-rouge">W</code>를 가지므로 $k = 2$ 는 답이 됩니다.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>길이가 정확히 $n$인 부분구간들을 모두 나열했을 때, 그중 가장 <code class="language-plaintext highlighter-rouge">W</code> 를 많이 가진 것이 $x$개를 가지고 있다고 하면, 그 $x$가 답이 됩니다.</p>

  <p>이 부분 구간을 $[a, b]$ 라고 하겠습니다. 이때, 이 구간을 오른쪽 또는 왼쪽으로 밀면서 다음 구간을 찾아나갈 것입니다. 
먼저 오른쪽으로 구간을 민다고 생각할 때, $b+1$ 위치에 <code class="language-plaintext highlighter-rouge">R</code>이 있다면 $[a, b+1]$ 은 정당한 구간입니다. 이와 같이 연속한 <code class="language-plaintext highlighter-rouge">R</code>에 대해서는 계속 정당한 구간을 찾아나갈 수 있습니다. 
만약 오른쪽 끝이 <code class="language-plaintext highlighter-rouge">W</code>라면, 왼쪽에서 처음으로 <code class="language-plaintext highlighter-rouge">W</code>를 하나 빼게 될 때까지 왼쪽 끝을 오른쪽으로 민다고 생각합니다.</p>

  <p>그렇게 하더라도 언제든지 구간의 길이가 $n$보다 작아지지 않음을 보여야 합니다. 
만약 그렇지 않아서, 이때 구간의 길이가 $n$보다 strictly 작다면, 
이렇게 구성된 $[a’, b’]$의 왼쪽 끝 바로 옆, 즉 $a’-1$ 위치에는 <code class="language-plaintext highlighter-rouge">W</code>가 있습니다. 
(방금 전에 구간의 왼쪽끝을 밀면서 <code class="language-plaintext highlighter-rouge">W</code>를 빼는 일이 발생할 때까지 밀었기 때문입니다)</p>

  <p>따라서, $[a’-1, b’]$에는 <code class="language-plaintext highlighter-rouge">W</code>가 $[a, b]$에서보다 하나 더 많고, 구간의 길이가 $n$보다 작거나 같습니다. 
이는 길이가 정확히 $n$인 부분구간들 중 최대한 <code class="language-plaintext highlighter-rouge">W</code>를 가진 것이 $[a, b]$ 라는 처음의 구성에 모순되므로, 이러한 구간을 찾을 수는 없습니다.</p>
</details>

<p><strong>Thoughts:</strong> 세명이서 논의하면서도 이 문제를 풀기 꽤 어려웠고, 결국 그럴싸한 아이디어를 내고 구현해서 맞았지만 증명을 확실히 생각하진 못했습니다. 
이 문제가 골드 1이라는것이 정말 충격적인데, 그걸 못푸는 저희 문제인지 난이도 assessment가 많이 달라진것인지 아직도 잘 모르겠습니다.</p>

<hr />

<h3 id="2016-icpc-southwest-european-regional-contest-f-performance-review">2016 ICPC SouthWest European Regional Contest F. Performance Review</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 13962 / SWERC 2016F</code><br />
난이도: <span style="color: rgb(0, 199, 139);">Platinum III</span></p>

<p><strong>문제 요약:</strong> 트리 형태의 관계와 각 사람 (노드) 들마다 $a_i, b_i$ 두 개의 값이 주어진다. 
각 노드는 트리상에서 자신의 ancestor들 중, 자기보다 $a$값이 높은 노드들에게 자신의 $b$값만큼을 더해 준다.<br />
이때, 각 노드가 자신의 descendent들로부터 받는 $b$값의 합을 구하시오.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>Euler Tour Tree를 아는지 묻는 문제입니다. 
ETT는 트리를 DFS 순서로 순회하면서, 각 노드에 들어간 시간 $s_i$ 와 그 노드로부터 나온 시간 $e_i$ 를 보관하여, 노드 $r$을 루트로 하는 서브트리를 $[s_i, e_i]$ 구간으로 표현하는 자료구조입니다. 
<a href="https://usaco.guide/gold/tree-euler?lang=cpp">설명 링크</a> 이렇게 만들어진 구간들을 이용, 세그먼트 트리가 subtree에 대한 쿼리를 처리하도록 할 수 있습니다.</p>

  <p>따라서, 이 문제에서는 주어진 트리에 대해 ETT를 빌드하면, 세그먼트 트리에서 흔히 나타나는 <em>나보다 값이 낮으면서 오른쪽에 있는 점들에 대하여…</em> 같은 류의 문제가 됩니다 (<a href="https://www.acmicpc.net/problem/1849">예시</a>, <a href="https://www.acmicpc.net/problem/17131">예시 2</a>)</p>

  <p>이런 문제를 푸는 일반적인 방법처럼, <strong>$a$값이 낮은 노드부터 순서대로</strong> 다음을 처리합니다.</p>
  <ul>
    <li>$[s_i, e_i]$ 구간의 합을 구하여 자신의 답으로 기억한다</li>
    <li>해당 노드의 $s_i$를 찾아, 그 지점에 $b_i$만큼 더하는 업데이트를 수행한다</li>
  </ul>

  <p>이런 문제에서 주의할 점은 순서의 처리입니다.</p>
  <ul>
    <li>두 연산의 순서를 정확히 해야 하며 (자기 값을 업데이트하고 합을 구하면 자기 자신의 값까지 더해집니다)</li>
    <li>$a$값이 같은 노드들의 순서를 잘 정해야 합니다. (이 경우, $a$값이 같은 노드끼리는 $b$값을 주고받지 않으므로, 깊이가 얕은 노드를 먼저 처리하여 자손에서 업데이트한값이 영향을 미치지 않게 해야 합니다)</li>
  </ul>
</details>

<hr />]]></content><author><name></name></author><category term="competitive-programming" /><category term="competitive-programming" /><summary type="html"><![CDATA[마지막 UCPC를 위해 팀연습을 돌았습니다. 이제와서 딱히 수상에 미련은 없고, 마지막으로 참가하는 PS 팀 대회를 재밌게 즐길 요량으로 항상 함께했던 dlwocks31, dhdroid 와 같이 나가기로 했습니다 :)]]></summary></entry><entry><title type="html">2024년 3-5월 알고리즘 문제풀이</title><link href="https://gratus907.github.io/competitive-programming/May24-ProblemSolving/" rel="alternate" type="text/html" title="2024년 3-5월 알고리즘 문제풀이" /><published>2024-05-05T00:00:00+09:00</published><updated>2024-05-05T00:00:00+09:00</updated><id>https://gratus907.github.io/competitive-programming/May24-ProblemSolving</id><content type="html" xml:base="https://gratus907.github.io/competitive-programming/May24-ProblemSolving/"><![CDATA[<p>해외 출장과 여러 일정으로 계속 바쁘네요 :(</p>

<p>5월까지 포함하여 한번에 작성할 예정입니다</p>

<h3 id="atcoder-beginner-contest-352g-socks-3">Atcoder Beginner Contest 352G. Socks 3</h3>
<p><code class="language-plaintext highlighter-rouge">ABC352G</code><br />
난이도 (Atcoder): <span style="color: rgb(255, 128, 0);">2413</span></p>

<p><strong>문제 요약:</strong> $1$ 부터 $n$까지의 숫자 카드가 각각 $x_i$ 장만큼씩 있다. 한장씩 뽑는 행동을 반복할 때, 처음으로 한 종류의 카드가 두 장이 되기 위한 뽑기 횟수의 기댓값을 구하라.</p>

<details>

  <summary><b>풀이 보기:</b></summary>
  <p>일종의 Coupon Collector’s Problem 변형 문제입니다.</p>

  <p>1부터 3까지의 카드가 $a, b, c$ 장만큼씩 있고, $a + b + c = M$이라고 할 때, 두 장을 뽑는 경우는 다음의 경우들이 있습니다.</p>
  <ul>
    <li>(1, 1), (2, 2), (3, 3) 을 뽑는 경우: $a(a - 1) + b(b - 1) + c(c - 1)$ 가지 경우의 수가 있습니다.</li>
    <li>(1, 2), (2, 3), (3, 1) 을 뽑는 경우 (또는 그 반대 순서): $2(ab + bc + ca)$ 가지가 경우의 수가 있습니다.</li>
  </ul>

  <p>종합하여 $a^2 + b^2 + c^2 + 2(ab + bc + ca) - M = (a + b + c)^2 - M = M(M - 1)$ 가지 경우를 이루게 됩니다.</p>

  <p>이때, 전자는 <strong>성공</strong> 했지만, 후자는 <strong>더 뽑아야</strong> 합니다. $k$장을 뽑았을 때 실패할 확률을 $p_k$라고 한다면, 구하고자 하는 답은 <strong>성공할 때까지 필요한 반복의 기댓값</strong> 이므로,
  $1 + (\sum_{i = 1}^{\infty} p_i)$ 가 우리가 원하는 답이 됩니다. (실패한다는건 다음 실행을 해야 한다는 의미가 되므로)
  여기서, $n + 1$ 장을 뽑으면 반드시 성공하므로, 우리는 $\infty$가 아닌 $n$까지만 더하면 됩니다.</p>

  <p>각각의 $p_i$를 구하기 위해, 위 예시로 돌아가 보겠습니다. 일반화해서, 2장을 뽑아서 실패할 확률을
  \(p_2 = \frac{1}{M(M - 1)}\sum_{i \neq j} x_i x_j = \frac{2}{M(M - 1)}\sum_{i &lt; j} x_i x_j\)
  로 쓸 수 있습니다. 비슷한 방법으로 $k$ 장을 뽑았을 때 실패하는 확률을 생각해 보면, 전체 경우의 수가 $M(M-1)\dots(M-K+1)$ 가지고, <strong>서로 겹치지 않게</strong> 무엇을 뽑을지 정하고 나면 경우의 수는 그냥 곱해지기 때문에,
  \(p_k = \frac{k!}{M(M-1)\dots(M-k+1)}\sum_{1 \leq j_1 &lt; j_2 &lt; \dots &lt; j_k \leq n} x_{j_1} x_{j_2} \dots x_{j_k}\)
  이렇게 구해다는 것을 알 수 있습니다.</p>

  <p>위 $p_k$ 항에서, 앞부분은 미리 팩토리얼값을 전처리해두면 되기 때문에 구하기 어렵지 않습니다. 뒷부분은 하나의 $k$를 구하기 위해 $\binom{n}{k}$ 시간이 들기 때문에 약간의 생각을 더 해야 합니다. 뒷부분의 합이 <strong>근과 계수의 관계</strong> 에서 얻어지는 항과 비슷함을 관찰할 수 있습니다. 구체적으로,
  \(S_k = \sum_{1 \leq j_1 &lt; j_2 &lt; \dots &lt; j_k \leq n} x_{j_1} x_{j_2} \dots x_{j_k}\)
  에 대해, 다음이 성립함을 알고 있습니다. ($S_0 = 1$로 생각합니다)
  \((t + x_1)(t + x_2) \dots (t + x_n) = \sum_{i = 0}^{n}t^n S_{n - i}\)
  따라서, 좌변의 다항식을 전개하여 그 계수를 알아낼 수 있다면 $S_1$부터 $S_n$까지의 모든 $S$를 구할 수 있습니다.</p>

  <p>그러나 $d$차 다항식을 곱셈하는 것은 그냥 곱셈하면 $d^2$ 시간이 걸리고, FFT를 이용하더라도 $d \log d$ 시간이 걸립니다. 구체적으로, $a$차와 $b$차 다항식을 곱하면 FFT를 사용하더라도 $(a + b) \log (a + b)$ 시간이 걸리므로, 앞에서부터 순서대로 곱한다면 전체 시간 복잡도는 $n^2 \log n$ 시간이 걸리게 됩니다.</p>

  <p>이러한 형태의 다항식을 빠르게 전개하기 위해서는 FFT를 이용하되, 이진 트리 형태로 곱셈해 나가는 방법을 쓸 수 있습니다. 다항식들을 이진 트리의 리프노드에 배치했다고 생각하고, 분할정복식으로 첫번째부터 $n / 2$ 번째까지의 다항식들의 곱과 $n / 2 + 1$ 부터 $n$번째까지의 다항식들의 곱을 따로 구하여 FFT로 합치는 방법입니다.
  이렇게 하면, $n$개의 $t + x$ 형태 다항식을 곱하기 위해
  \(T(n) = 2T(n / 2) + O(n \log n)\)
  시간이 들기 때문에, 이를 마스터 정리로 전개하면 $O(n \log^2 n)$ 시간이 됩니다.</p>

  <p>따라서, $O(n \log^2 n)$ 시간에 모든 $S_i$ 들을 구하고, 이걸로 $p_i$들을 구하면 전체 문제를 해결할 수 있습니다.</p>

</details>

<hr />

<h3 id="1997-koi-고등부-2번-교차하지-않는-원의-현들의-최대집합">1997 KOI 고등부 2번, 교차하지 않는 원의 현들의 최대집합</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 2673 / KOI 1997 H2</code>
난이도: <span style="color: rgb(0, 199, 139);">Platinum IV</span></p>

<p><strong>문제 요약:</strong> 문제 서술이 충분히 간단하게 작성되어 있어 그대로 가져옵니다.</p>

<p>평면상에 있는 원의 둘레에 100개의 점이 일정한 간격으로 시계방향으로 번호가 $1, 2, … 100$ 으로 붙여져 있다. 이 점들을 끝점으로 갖는 $N$개의 선분(원의 현)이 입력으로 주어질 때, 이들중에서 서로 교차하지 않는 것들을 최대한 많이 찾아서 그 개수를 출력하는 프로그램을 작성하라.</p>

<p>단, $1 \leq N \leq 50$ 이고, 주어진 각 점은 많아야 한 현의 끝점이 될 수 있다.</p>

<details>
  <summary><b>풀이 보기:</b></summary>

  <p>현의 개수가 최대 50개임을 생각하고, 주어진 각 점은 많아야 한 현의 끝점이 될 수 있다는 것을 생각하면, <strong>반드시</strong> 선분 $(i, i+1)$이 주어진 현이 아닌 $i$를 생각할 수 있습니다. 그 점에서 원을 끊어서, 선형으로 문제를 변형할 수 있습니다.</p>

  <p>일반성을 잃지 않고, 끊은 지점이 $(N, 1)$ 사이라고 하겠습니다. 현의 끝점이 아닌 점들은 필요 없으므로 잊어버리고, 좌표압축을 수행합니다.</p>
  <ul>
    <li>$D(i, j)$ 를 $(i, j)$ 번 점까지만 보면서 고를 수 있는 최대 현의 개수라고 하고, 이를 DP를 이용하여 찾습니다.</li>
    <li>$(i, j)$가 주어진 현이면 $c = 1$, 아니면 $0$이라고 생각합니다.</li>
    <li>임의의 $i \leq k \leq j$ 인 $k$에 대해, $D(i, j) \geq D(i, k) + D(k, j) + c$ 가 성립합니다. 어떤 $k$에서 끊어서 그 왼쪽과 오른쪽 구간을 따로 계산한 후, $(i, j)$ 현을 끼워넣을 수 있기 때문입니다.</li>
    <li>선택한 현들이 겹칠 수 없기 때문에, $D(i, j)$ 들은 반드시 저러한 $k$에 의해 정해져야 하므로, 실제로는 $D(i, j) \geq D(i, k) + D(k, j) + c$ 로 계산할 수 있습니다.</li>
    <li>잘 알려진 <a href="https://www.acmicpc.net/problem/11049">행렬 곱셈 순서</a> 와 같은 느낌의 DP로, $O(N^3)$ 시간에 해결할 수 있습니다.</li>
  </ul>
</details>]]></content><author><name></name></author><category term="competitive-programming" /><category term="competitive-programming" /><summary type="html"><![CDATA[해외 출장과 여러 일정으로 계속 바쁘네요 :(]]></summary></entry><entry><title type="html">2024년 1-2월 알고리즘 문제풀이</title><link href="https://gratus907.github.io/competitive-programming/Feb24-ProblemSolving/" rel="alternate" type="text/html" title="2024년 1-2월 알고리즘 문제풀이" /><published>2024-03-03T00:00:00+09:00</published><updated>2024-03-03T00:00:00+09:00</updated><id>https://gratus907.github.io/competitive-programming/Feb24-ProblemSolving</id><content type="html" xml:base="https://gratus907.github.io/competitive-programming/Feb24-ProblemSolving/"><![CDATA[<p>1월-2월에는 논문 일정으로 바빠서 PS를 못 한지라, 3/2일에 푼 ICPC Asia Pacific Final 문제를 위주로 작성합니다. :(</p>

<hr />
<h3 id="2024-icpc-asia-pacific-championship-j-there-and-back-again">2024 ICPC Asia-Pacific Championship J. There and Back Again</h3>
<p><code class="language-plaintext highlighter-rouge">ICPC Asia-Pacific Championship J</code><br />
난이도 (체감): <span style="color: rgb(236, 154, 0);">Gold 2?</span></p>

<p><strong>문제 요약:</strong> 정점과 간선이 $10^5$, $3 \times 10^5$ 개 이하인 그래프 $G$에 대하여, 1번 정점에서 $n$번 정점에 갔다가 다시 돌아오려고 한다. 
이때, <strong>가는 경로</strong> 와 <strong>오는 경로</strong> 는 사용하는 간선의 집합으로 볼 때 달라야 한다. 최단 경로를 찾아라.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>이런 류 문제는 대부분 <strong>그래프를 적절히 변형하여 모델링한 다익스트라</strong> 가 됩니다. 비슷하게 풀 수 있습니다.</p>

  <p>먼저, 1번에서 $n$번으로 가는 최단 경로 $P$가 있다면, 가는 경로를 $P$로 고정해도 상관없음을 관찰합니다. 
  만약 가는 경로와 오는 경로가 모두 $P$와 다르다면, 그중 하나를 $P$로 바꾸어도 손해가 발생하지 않습니다. 가는 경로와 오는 경로를 맞바꾸어도 상관없으므로, $P$로 $n$에 도달했다고 생각해도 상관 없습니다.</p>

  <p>이제, $P$와 다르면서 최단인 경로를 찾아야 합니다. 정점 $v$들마다 새로운 정점 $v’$를 생각하고, 간선 $(u, v, t)$에 대해 (실제로는 양방향입니다)</p>
  <ul>
    <li>항상 $(u, v, t)$와 $(u’, v’, t)$를 추가하고</li>
    <li>만약 $(u, v)$ 간선이 $P$에서 사용되지 않았다면, $(u, v’, t)$와 $(u’, v, t)$를 추가한</li>
  </ul>

  <p>그래프를 생각합니다. 이는 그래프를 위와 아래 두 층으로 나누어서, 위-그래프와 아래-그래프 각각이 $G$와 똑같이 생겼으면서, $P$에 포함되지 않은 간선들만 이용하여 아래-그래프로부터 위-그래프로 이동할 수 있도록 한 셈이 됩니다.</p>

  <p>이제, 아래-그래프의 $n$번에서 출발하여 위-그래프의 $1$번인 $1’$ 으로 이동하는 최단 경로를 찾으면 됩니다.</p>

  <p>비슷한 문제는 정말 흔한것 같습니다. 경로의 홀짝성, 특정 점을 밟았는지 여부 등 다양한 최단 경로 문제에 이용됩니다.</p>
</details>

<hr />
<h3 id="2024-icpc-asia-pacific-championship-e-duplicates">2024 ICPC Asia-Pacific Championship E. Duplicates</h3>
<p><code class="language-plaintext highlighter-rouge">ICPC Asia-Pacific Championship E</code><br />
난이도 (체감): <span style="color: rgb(0, 199, 139);">Platinum IV?</span></p>

<p><strong>문제 요약:</strong> $n \times n$ 행렬에 1부터 $n$까지의 수가 쓰여 있다. 이때, 최소 개수의 수를 바꾸어서, <strong>모든 행과 모든 열에</strong> 중복된 수가 있도록 하라.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>중복된 수가 없는 행 또는 열을 <strong>슬프다</strong> 고 하겠습니다. (Sad? ㅋㅋㅋㅋㅋㅋ 문제풀때 실제로 이런식으로 말했던거 같은데, 공식 해설은 Colorful 같은 행복한 용어를 사용합니다.)</p>

  <p>$r$번째 행과 $c$ 번째 열이 모두 슬픈 경우, $(r, c)$ 칸의 원소를 아무것으로나 바꾸어줌으로써 양쪽의 슬픔을 동시에 해소할 수 있습니다. 
(가능한 수가 $1$부터 $n$까지밖에 없기 때문에, 어떤 행/열이 슬프다면 항상 1부터 $n$까지의 수가 정확히 한 번씩 사용되고 있기 때문입니다)</p>

  <p>$r$번째 행이 슬프지만 $c$번째 열이 슬프지 않은 경우, $(r, c)$ 칸의 원소를 <strong>적절히</strong> 바꾸어야 합니다.</p>

  <p>적절히 바꾸는 것만 조심한다면, 행렬 전체의 슬픔은 매번 2 또는 1만큼씩 줄어듭니다. 
Priority Queue 등의 적절한 자료구조를 이용하여 2만큼 줄일 수 있을때 항상 그리한다면 최소 횟수만에 해결할 수 있습니다.</p>

  <p>공식 해설에는 최적성에 대한 더 깔끔한 증명이 있습니다. 
일반성을 잃지 않고 슬픈 열의 개수 $m_c$가 슬픈 행의 개수 $m_r$보다 적다고 하면, <strong>항상 열의 슬픔을 교정하는</strong> 식으로 접근합니다. 
그러나 이때 첫 $m_r$번은 행과 열을 동시에 교정할 수 있습니다. 이렇게 하면 $min(m_r, m_c)$ 번이 됩니다.</p>
</details>

<hr />
<h3 id="2024-icpc-asia-pacific-championship-f-forming-groups">2024 ICPC Asia-Pacific Championship F. Forming Groups</h3>
<p><code class="language-plaintext highlighter-rouge">ICPC Asia-Pacific Championship F</code><br />
난이도 (체감): <span style="color: rgb(0, 199, 139);">Platinum III?</span></p>

<p><strong>문제 요약:</strong> 2번부터 $n$번까지 $n-1$명의 사람이 한줄로 늘어서 있고, 각각은 $a_i$ 의 <strong>실력</strong> 을 가지고 있다.</p>

<p>이때, <strong>팀장</strong>을 줄의 원하는 위치로 움직일 수 있고, 팀의 개수 $k$를 선택할 수 있다.</p>

<p>선택한 후에는 (팀장이 들어간 위치를 고려하여, 새로 번호를 매긴) $(1, k+1, 2k+1 \dots)$가 1번 팀이 되고, $(2, k+2, \dots)$ 가 2번 팀이 되는 식으로 팀이 이루어진다.</p>

<p>팀의 실력은 팀원의 실력의 합이라고 할 때, 최대한 공정하게 (팀 실력의 최대/최소의 비율로) 팀장을 배치하고, $k$를 고르는 방법을 제시하라.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>팀장을 배치하는 것과 팀의 수를 선택하는 두가지 작업을 할 수 있다는 점이 어렵습니다.</p>

  <p>먼저, $k$가 고정되어 있다고 치고 팀장의 위치만 움직여 가며 문제를 푼다고 생각해 봅니다. 배치가 결정되면 $O(n)$ 시간에 각 팀의 실력을 계산할 수 있지만, 계산해야 할 자리가 $n$개이기 때문에 이 방법으로는 복잡도를 맞출 수 없습니다. 그러나, 다음 관찰로 복잡도를 줄일 수 있습니다.</p>

  <p><strong>관찰</strong>: 팀장을 오른쪽으로 한 칸 옮길 때, 소속된 팀이 바뀌는 사람은 팀장과 방금 자리를 바꾼 사람 두명밖에 없다.</p>

  <p>팀 $k$개의 실력을 적절한 자료구조를 이용하여 유지한다면, 최대 두 팀만 실제로 실력값이 바뀌게 되고, 그 과정에서 최대와 최소를 유지해야 하므로 $O(\log k)$ 시간에 팀장을 한 칸 옮겨볼 수 있습니다. $k \leq n$ 이므로, 이는 즉 팀장을 맨 왼쪽부터 맨 오른쪽까지 옮기면서 최적배치를 계산하는 데 $O(n \log n)$ 시간을 쓴다는 것입니다.</p>

  <p>이제, $n$의 각 약수 $k$에 대해 따로 문제를 푼다고 생각하고, 전체 복잡도를 계산해 봅시다. $n$의 약수의 개수를 $d(n)$ 이라고 쓸 때, 
\(\sum_{k \di n} n \log k = O(d(n) \times n \log n)\)
입니다. ($\log k$를 타이트하게 잡아도, $\sum_{k \di n} \log k$ 는 $(d(n) \log n) / 2$ 이므로 변하지 않습니다)
$d(n)$은 꽤 작은 값이지만, $n = 10^6$ 이하에서 $d(n)$의 최댓값은 무려 240임을 (720,720이 240개의 약수를 가집니다) 생각하면 시간상 무리해 보입니다.</p>

  <p>따라서, 다음의 관찰을 통해 복잡도를 더 줄여야 합니다.</p>

  <p><strong>관찰</strong>: 팀의 개수는 반드시 $n$의 소인수일 때 최적이다.</p>

  <p>이는 임의의 정수 $m &gt; 1$에 대해, $k$팀을 만드는 것보다 $mk$팀을 만드는건 항상 손해임을 보이면 됩니다. 
증명은 비교적 간단한데, formal하게 작성하기보다는 2팀을 만들지 4팀을 만들지의 예시를 들어 생각해보겠습니다.
4팀 상태에서 팀 4개를 강한 순서대로 나열하면, 2팀으로 팀을 줄일때 1번과 3번, 2번과 4번 팀이 묶이게 됩니다. 이때 강한쪽 팀은 1번팀의 2배보다 약하고, 약한쪽 팀은 4번팀의 2배보다 강해지기 때문에 강한 팀과 약한 팀의 실력 비율은 더 공정해집니다.</p>

  <p>따라서, $n$의 모든 약수가 아닌 소인수들에 대해서만 풀면 되고, 100만 이하의 수는 최대 소인수가 7개뿐이므로 $n \log n$의 상수배 (7) 만에 해결할 수 있습니다.</p>
</details>

<hr />

<h3 id="2006-icpc-northen-eurasia-finals-h-hard-life">2006 ICPC Northen Eurasia Finals H. Hard Life</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 3611 팀의 난이도 / 2006 ICPC Northen Eurasia Finals H</code><br />
난이도: <span style="color: rgb(0, 158, 229);">Diamond II</span></p>

<p><strong>문제 요약:</strong> 최대 정점 100개, 간선 1000개의 그래프에서, Densest Subgraph 찾기.</p>

<p>(정점의 부분집합을 골라서, induced subgraph의 density를 최대화하라)</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>사실 PS에 이런 문제를 낸 저의까지는 잘 모르겠습니다만, TCS / DB / DM 모두에서 유명하고 실제 research도 많이 된 문제입니다. 
  <a href="https://en.wikipedia.org/wiki/Dense_subgraph">Wikipedia에 이 문제에 대한 글</a>도 있습니다.</p>

  <p>아마도 출제 의도는 Goldberg나 Tarjan의 flow 기반한 알고리즘이었던 것으로 보입니다. 이 문제는 적절하게 min-cut 문제로 모델링할 수 있고, 이를 flow로 해결할 수 있으므로…</p>

  <p>다만 저는 이 문제를 PS하다가 본게 아니라, 연구실에서 관련 논문을 통해 접했습니다. 최근 제시된 구현하기 비교적 어렵지 않은 approximation 알고리즘 <a class="citation" href="#FlowLess">[1]</a>이 있어서 구현해보고 제출했습니다. 기본적인 아이디어는 Core Decomposition (나중에 언젠가 소개할 기회가 있을 것 같습니다) 을 수행하면 dense한 영역을 대강 찾아준다는 것에 착안하여 이를 반복하는 것인데, 나중에 이론적으로도 정확한 approximation ratio에 대한 증명이 이루어졌습니다 <a class="citation" href="#SODA22DensestSubgraph">[2]</a>.</p>

  <p>이외에도 이 문제를 비롯하여 관련한 여러 문제에 대한 연구가 활발히 이루어지고 있습니다. 여전히 PS 대회에서 flow 알고리즘이 생각해낼만 한지는 잘 모르겠습니다.</p>

  <ol class="bibliography"><li><script>
window.onload = function() {

    var venue_dict = {
        "\\NeurIPS": "Advances in Neural Information Processing Systems",
        "\\ICLR": "International Conference on Learning Representations",
        "\\CVPR": "Conference on Computer Vision and Pattern Recognition",
        "\\ICML": "International Conference on Machine Learning",
        "\\WWW": "Proceedings of ACM Web Conference",
        "\\VLDB": "Proceedings of the VLDB Endowment",
        "\\ICDE": "Proceedings of IEEE ICDE",
        "\\ICDM": "Proceedings of IEEE ICDM",
        "\\ICDT": "Proceedings of ICDT",
        "\\TKDE": "IEEE Transactions on Knowledge and Data Engineering",
        "\\SIGMOD": "Proceedings of ACM SIGMOD",
        "\\SIGKDD": "Proceedings of ACM SIGKDD",
        "\\SODA": "Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms",
    }
    function foo(bookTitle) {
        if (bookTitle in venue_dict) {
            return venue_dict[bookTitle];
        }
        else 
            return bookTitle;
    }
    
    var bookTitleElements = document.getElementsByClassName('entryBookTitle');
    console.log(bookTitleElements, bookTitleElements.length);
    for (let i = 0; i < bookTitleElements.length; i++) {
        var elem = bookTitleElements.item(i);
        var originalBookTitle = elem.getAttribute('data-booktitle');
        var transformedBookTitle = foo(originalBookTitle);
        elem.textContent = transformedBookTitle;
    }
    
}
</script>

<p> 
    
             
             
            
                D. Boob et al.
            
             
             
            
             
             
            
             
             
            
             
             
            
             
             
            
             
             
            
        
    
<b>Flowless: Extracting Densest Subgraphs Without Flow Computations</b>, 

<span class="entryBookTitle" data-booktitle="\WWW"></span>


(2020)

</p></li>
<li><script>
window.onload = function() {

    var venue_dict = {
        "\\NeurIPS": "Advances in Neural Information Processing Systems",
        "\\ICLR": "International Conference on Learning Representations",
        "\\CVPR": "Conference on Computer Vision and Pattern Recognition",
        "\\ICML": "International Conference on Machine Learning",
        "\\WWW": "Proceedings of ACM Web Conference",
        "\\VLDB": "Proceedings of the VLDB Endowment",
        "\\ICDE": "Proceedings of IEEE ICDE",
        "\\ICDM": "Proceedings of IEEE ICDM",
        "\\ICDT": "Proceedings of ICDT",
        "\\TKDE": "IEEE Transactions on Knowledge and Data Engineering",
        "\\SIGMOD": "Proceedings of ACM SIGMOD",
        "\\SIGKDD": "Proceedings of ACM SIGKDD",
        "\\SODA": "Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms",
    }
    function foo(bookTitle) {
        if (bookTitle in venue_dict) {
            return venue_dict[bookTitle];
        }
        else 
            return bookTitle;
    }
    
    var bookTitleElements = document.getElementsByClassName('entryBookTitle');
    console.log(bookTitleElements, bookTitleElements.length);
    for (let i = 0; i < bookTitleElements.length; i++) {
        var elem = bookTitleElements.item(i);
        var originalBookTitle = elem.getAttribute('data-booktitle');
        var transformedBookTitle = foo(originalBookTitle);
        elem.textContent = transformedBookTitle;
    }
    
}
</script>

<p> 
    
              
            
            
                K. Chandra Chekuri,
            
              
            
            
                and M. Torres,
            
        
    
<b>Densest Subgraph: Supermodularity, Iterative Peeling, and Flow</b>, 

<span class="entryBookTitle" data-booktitle="Proceedings of the Annual ACM-SIAM Symposium on Discrete Algorithms"></span>


(2022)

</p></li></ol>

</details>]]></content><author><name></name></author><category term="competitive-programming" /><category term="competitive-programming" /><summary type="html"><![CDATA[1월-2월에는 논문 일정으로 바빠서 PS를 못 한지라, 3/2일에 푼 ICPC Asia Pacific Final 문제를 위주로 작성합니다. :(]]></summary></entry><entry><title type="html">12월 알고리즘 문제풀이</title><link href="https://gratus907.github.io/competitive-programming/Dec23-ProblemSolving/" rel="alternate" type="text/html" title="12월 알고리즘 문제풀이" /><published>2023-12-30T00:00:00+09:00</published><updated>2023-12-30T00:00:00+09:00</updated><id>https://gratus907.github.io/competitive-programming/Dec23-ProblemSolving</id><content type="html" xml:base="https://gratus907.github.io/competitive-programming/Dec23-ProblemSolving/"><![CDATA[<p>앞으로 PS 포스팅은 월 1-2회정도는 해보려고 생각하고 있습니다.</p>

<hr />
<h3 id="atcoder-beginner-contest-331f-palindrome-query">Atcoder Beginner Contest 331F. Palindrome Query</h3>
<p><code class="language-plaintext highlighter-rouge">Atcoder Beginner 331F</code><br />
난이도: <span style="color: rgb(0, 0, 255);">1666 (Atcoder)</span></p>

<p><strong>문제 요약:</strong> 문자열 $S$에 대해, 다음 두 쿼리가 주어진다. (문자열 길이 $10^6$, 쿼리 개수 $10^5$)</p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">1 x c</code>: $S$의 $x$ 번째 문자를 $c$로 바꾼다</li>
  <li><code class="language-plaintext highlighter-rouge">2 l r</code>: $S$의 $[l, r]$ 부분 문자열이 Palindrome인지 판정하라.</li>
</ol>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>1번 쿼리 (문자열 업데이트) 가 없다면 당연히 Manacher’s algorithm으로 선형 시간에 전처리하고 쿼리당 $O(1)$에 답할 수 있습니다.</p>

  <p>그러나, Manacher’s algorithm은 문자열 업데이트에 대응할 수 없습니다. 대신에, 이 문제의 경우 <strong>Rabin-Karp</strong> 해싱을 이용하면 됩니다.</p>

  <p>Rabin-Karp 해싱에서, 문자열 $a_0\dots a_{L-1}$은 $\sum_{i = 0}^{L-1} a_i x^i \mod M$ 으로 해싱됩니다.
(이때 $M$ 으로는 일반적으로 소수를 사용하고, $x$는 $M$과 서로소이면 큰 문제가 없는 것으로 알고 있습니다. 저는 습관적으로 29 (대소문자가 있으면 61), 10억7 정도의 $x$와 $M$을 씁니다.)</p>

  <p>이러한 해싱을 이용하면, 문자열 $A$와 $B$가 주어졌을 때, $AB$ concatenation 의 해시값을 $h(A) \times p^{\text{len}(B)} + h(B)$ 로 $O(1)$에 빠르게 계산할 수 있습니다. 
($p^{\text{len}(B)}$ 를 계산하는 데 원래는 $\log$ 시간이 걸리지만, 이것은 전처리해 놓는다고 생각하고)</p>

  <p>따라서, Segment tree를 만들어서, 각 노드가 <strong>자신이 담당하는 부분문자열의 해시값</strong> 을 가지고 있게 한다면, 이 값을 빠르게 업데이트할 수 있습니다.</p>

  <p>이제, 실제로는 팰린드롬인지를 판정해야 하므로, $S$와 $\text{reverse}(S)$ 에 대한 segment tree를 각각 만듭니다.</p>
  <ol>
    <li>1번 쿼리에 대해서는 양쪽의 적절한 인덱스에 업데이트를 수행하면 $O(\log N)$ 시간에 처리 가능합니다.</li>
    <li>마찬가지로, $S$의 $[l, r]$ 과 $\text{reverse}(S)$ 의 $[N-r-1, N-l-1]$ 인덱스의 해시값을 만들어 비교하면 됩니다.</li>
  </ol>

  <p>해시 충돌이 약간 걱정되지만,  <strong>설마 그렇게 사악하겠느냐</strong> 는 믿음을 가지고 제출했습니다.</p>
</details>

<hr />

<h3 id="atcoder-beginner-contest-333f-bomb-game-2">Atcoder Beginner Contest 333F. Bomb Game 2</h3>
<p><code class="language-plaintext highlighter-rouge">Atcoder Beginner 331F</code><br />
난이도: <span style="color: rgb(0, 0, 255);">1770 (Atcoder)</span></p>

<p><strong>문제 요약:</strong> $1, 2, \dots N$ 번 사람이 줄에 서 있을 때, 맨 앞 사람에 대해 다음 ‘시행’ 을 반복한다.</p>

<blockquote>
  <p>맨 앞 사람을 $1/2$ 확률로 제거하고, 만약 제거되지 않았다면 맨 뒤로 보낸다.</p>
</blockquote>

<p>이 시행을 한 사람만이 남아 있을 때까지 계속 반복할 때, $i$번째 사람이 마지막으로 남아 있게 될 확률을 구하시오.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>대회중에는 약간의 실수로 해결하지 못했지만, 재밌는 확률 DP 문제라고 생각했습니다.</p>

  <p>$D(i, j)$ 를 $i$명이 줄에 남은 채로 $(1 \dots i)$ 이 작업을 시작할 때, 이때의 $j$번째 사람이 마지막으로 남는 사람일 확률이라고 정의합니다. 
이제, 우리가 원하는 값은 $D(N, j)$ 들을 구하면 됩니다.</p>

  <p>$D(i, 1)$ 을 구하는 과정을 생각해보면, $1/2$ 확률로 이 사람이 사라지고, $1/2$ 확률로 줄의 맨 뒤로 가게 됩니다. 이는 즉, 
\(D(i, 1) = \frac{1}{2} D(i, i)\)
식이 항상 성립합니다. 이제, 나머지 $D(i, j)$ 를 생각해보면, $j$번째 사람이 마지막 사람이기 위해서는</p>
  <ul>
    <li>맨 앞사람이 제거되고, 남은 $i-1$명중 $j-1$번째 사람이 마지막 사람이거나</li>
    <li>맨 앞사람이 제거되지 않고, 남은 $i$명중 $j-1$ 번째 사람이 마지막 사람인 
두 경우가 있습니다. 즉, 
\(D(i, j) = \frac{D(i-1, j-1) + D(i, j-1)}{2}\)
가 됩니다. $D(i, 1)$을 $x$로 놓고, $D(i, *)$ 들의 합이 1임을 이용하여 $x$의 값을 찾은 후, 나머지 값들을 모두 찾으면 $O(N^2)$ 시간에 풀 수 있습니다.</li>
  </ul>
</details>

<hr />

<h3 id="cerc-2019s-saba1000kg">CERC 2019S. Saba1000kg</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 18180 / ICPC Central European Regional Contest 2019 S</code><br />
난이도: <span style="color: rgb(0, 199, 139);">Platinum II</span></p>

<p><strong>문제 요약:</strong> 정점과 간선이 $10^5$ 개 이하인 그래프에 대하여, 다음의 쿼리가 최대 $10^5$개 주어진다.</p>
<ul>
  <li>정점 집합 $S$ 는 몇 개의 connected component로 구성되어 있는가?</li>
</ul>

<p>단, 주어지는 집합 $S$의 크기의 합은 $10^5$ 이하이다.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>굉장히 간단하지만, 복잡도를 맞추기 쉽지 않습니다. 정점 집합 하나에 대해서라면 union-find나 DFS를 이용하여 선형 시간에 판정할 수 있지만, 쿼리를 처리하기는 어려워 보입니다.</p>

  <p>관점을 달리하여, <strong>선형 시간에 충분히 큰 집합도 해결할 수 있다</strong> 라고 생각해 봅시다. 반대로, 작은 집합이라면 <strong>모든 정점의 쌍들을</strong> 확인해 볼 수 있습니다. 
즉, 어떤 $K$를 잡아서,</p>
  <ul>
    <li>집합의 크기가 $K$보다 작은 쿼리는, $K^2$ 시간에 모든 정점 쌍을 확인하면서 union-find를 하면 $O(K^2 \alpha(V))$ 시간에 풀 수 있고</li>
    <li>집합의 크기가 $K$보다 큰 쿼리는, 모든 간선을 한바퀴 돌면서 양쪽 끝점이 모두 $S$에 들어있을 때만 union-find를 하면 $O(E \alpha(V))$에 풀 수 있습니다.</li>
  </ul>

  <p>주어지는 쿼리의 크기를 $s_1 \dots s_Q$ 라 할 때, $\sum s_i \leq 10^{5}$ 이므로,<br />
첫번째 경우는 아무리 커도 $\sum_{i = 1}^{Q} s_i^2 \alpha(V) \leq K \times 10^5 \times \alpha(V)$ 를 넘지 않고, 
두번째 경우는 쿼리가 최대 $10^5 / K$ 개밖에 없으므로 연산량을 대략 $E \times 10^5 \times \alpha(V) / K$ 로 바운드할 수 있습니다.
따라서, $K = 10^{2.5}$ 정도로 잡으면 양쪽 모두를 $10^{7.5}$ ($\alpha(V)$ 항을 대충 무시하고) 정도로 맞춰볼 수 있습니다.</p>

  <p>일종의 sqrt decomposition 처럼 복잡도를 맞추는 이런 류의 트릭은 비교적 간단하면서도 굉장히 강력한것 같습니다. :)</p>
</details>]]></content><author><name></name></author><category term="competitive-programming" /><category term="competitive-programming" /><summary type="html"><![CDATA[앞으로 PS 포스팅은 월 1-2회정도는 해보려고 생각하고 있습니다.]]></summary></entry><entry><title type="html">11월 2주차 알고리즘 문제풀이</title><link href="https://gratus907.github.io/competitive-programming/Nov23W2-ProblemSolving/" rel="alternate" type="text/html" title="11월 2주차 알고리즘 문제풀이" /><published>2023-11-15T00:00:00+09:00</published><updated>2023-11-15T00:00:00+09:00</updated><id>https://gratus907.github.io/competitive-programming/Nov23W2-ProblemSolving</id><content type="html" xml:base="https://gratus907.github.io/competitive-programming/Nov23W2-ProblemSolving/"><![CDATA[<p>일정상 연습은 같이 참여하지 못하고, 혼자 문제만 풀어봤습니다. 이번에는 CERC 문제들만으로 세팅했다고 들었는데, 역시 퀄리티가 훌륭합니다.</p>

<p>For some reason, 유럽권은 동유럽이든 서유럽이든 알고리즘 하시는 분들이 geometry를 매우 사랑하는것 같습니다. 기하문제 두개가 있었지만 손도 대지 못했습니다 :( 
ICPC에서 기하문제의 비율이 다른 리저널보다 높은것만이 아니라, 묘하게 연구 쪽에서도 computational geometry가 강합니다.</p>

<p><img src="../../images/d4ee94d0e1007229cff0b9aad086998393efdd3d1b45789d2a1f9b7148cef7ff.png" alt="picture 1" /><br />
STOC (Symposium on Theory Of Computing) SODA (Symposium On Discrete Algorithms) SoCG (Symposium on Computational Geometry), 세개의 top conference에서 가장 많은 논문을 publish한 학교/연구소들인데, 왼쪽의 두개 (STOC/SODA) 는 알고리즘과 계산이론 전반을 다루고 오른쪽 SoCG는 계산기하에 집중하는 학회입니다. SoCG에 유독 유럽권 기관들이 눈에 띕니다 (Free U of Berlin, Inria Sophia, Utrechet, Max Planck, ETH Zurich…)</p>

<hr />
<h3 id="cerc-2019a-abb">CERC 2019A. ABB</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 18171 / ICPC Central European Regional Contest 2019 A</code><br />
난이도: <span style="color: rgb(0, 199, 139);">Platinum IV</span></p>

<p><strong>문제 요약:</strong> 문자열 $S$에 대해, 최소 개수의 글자를 뒤에 덧붙여서 문자열을 팰린드롬으로 만들려고 한다. 몇 글자가 필요한가?</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>문자열 $S$가 어떤 문자열 $A$ 와 팰린드롬 $P$ 에 대해 $S = A + P$ 라면, $\abs{A}$ 만큼을 뒤에 덧붙여 $A + P + rev(A)$ 를 만들면 됩니다.</p>

  <p>이러한 $A$가 최대한 짧아야 하므로, 반대로 $P$가 최대한 길어야 합니다. 따라서, Manacher’s Algorithm으로 $S$의 모든 극대 팰린드롬 부분문자열을 찾고, 그것들 중 $S$의 맨 뒤까지 닿는 최대 길이의 팰린드롬이 무엇인지를 판정하면 충분합니다. $O(\abs{S})$ 에 해결할 수 있습니다.</p>
</details>

<hr />
<h3 id="cerc-2018i-shooter-island">CERC 2018I. Shooter Island</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 16700 / ICPC Central European Regional Contest 2018 I</code><br />
난이도: <span style="color: rgb(0, 199, 139);">Platinum III</span></p>

<p><strong>문제 요약:</strong> 격자점 그리드상에, 두 가지 종류의 쿼리가 주어진다. 처음에는 모든 점이 ‘밟을 수 없는’ 점이다.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">1 x1 y1 x2 y2</code>: $[x_1, y_1] \times [x_2, y_2]$ 직사각형 상의 격자점들을 ‘밟을 수 있게’ 한다.</li>
  <li><code class="language-plaintext highlighter-rouge">2 x1 y1 x2 y2</code>: $(x_1, y_1)$ 에서 $(x_2, y_2)$ 까지, ‘밟을 수 있는’ 점들만 통해 이동할 수 있는지 판정하라.</li>
</ul>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>$R \times C$ 칸의 Union-Find를 유지할 수 있다면, 쉽게 풀 수 있습니다. 여기서 문제는 1번 쿼리 한번에 $O(RC)$ 시간을 써서는 복잡도를 맞출 수 없다는 점입니다.</p>

  <p>한개당 $C$칸을 관리하는, $R$개의 Union find 자료구조를 추가로 관리하면, 이미 합쳐진 segment를 건너뛰면서 합칠 수 있습니다.</p>
  <ul>
    <li>문제를 1차원으로 줄여서 생각해 보면, $[l, r]$ 사이 점을 모두 합치는 쿼리가 여러 개 주어지고 이걸 빨리 처리하는 문제가 되며,</li>
    <li>항상 맨 오른쪽의 값을 루트로 하도록 합니다. 이때, 본 노드를 다시 보지 않도록 잘 처리하면 됩니다.</li>
    <li>말로 쓰기는 어려운데… 예를 들어, $[3, 5]$ 를 먼저 합치고, $[1, 6]$ 을 합친다면,
      <ul>
        <li>$p(3) = 4, p(4) = 5$ 로 만들고</li>
        <li>$p(1) = 2$ 로 만든 다음, $p(2)$ 는 find(3) 을 수행해서 $p(2)$ 를 5로 만들고 넘어갑니다.</li>
        <li>$p(5) = 6$으로 만들고 끝냅니다.</li>
      </ul>
    </li>
    <li>이 과정에서 Path compression만 쓰면, 복잡도를 합리적으로 관리할 수 있습니다.</li>
    <li>맨 오른쪽의 값을 루트로 고정함에 따라, size나 rank에 따른 Union find를 할 수 없지만, 이 문제에서는 상관이 없습니다.</li>
  </ul>

  <p>이 테크닉은 <a href="https://www.acmicpc.net/problem/26087">2022년 서강대학교 프로그래밍 대회</a> 에서 저는 (검수하면서) 처음 알게 되었는데, 꽤 재밌는 테크닉인것 같습니다. 잘 알려져 있는지는…잘 모르겠습니다.</p>

  <p>여담으로, Path compression’만’ 사용하면, 쿼리당 amortized $O(\log n)$ 시간에 작동함이 알려져 있습니다. 
PS 세팅에서, $\log n$ 와 $\alpha(n)$을 구분해낼수있는 문제가 있는지는 잘 모르겠습니다. 어쩌면 모든 UF문제를 사실 path compression 하나로 뚫을수 있을지도 모르겠습니다.</p>
</details>

<hr />

<h3 id="cerc-2016h-hanger-hurdles">CERC 2016H. Hanger Hurdles</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 13952 / ICPC Central European Regional Contest 2016 H / 2016-2017 Grand Prix of Europe</code><br />
난이도: <span style="color: rgb(0, 158, 229);">Diamond V</span></p>

<p><strong>문제 요약:</strong> 일부 칸들이 막혀있는 2차원 그리드가 주어진다. 한 변의 길이가 홀수 $k$인 정사각형 박스를 중심의 위치가 $(r_1, c_1)$ 에서 $(r_2, c_2)$ 까지 옮길 수 있는지 판정하여라. 
단, 그리드의 크기는 $1000 \times 1000$ 이고, $(r_1, c_1), (r_2, c_2)$ 쿼리는 $300,000$개 주어진다.</p>

<p>쿼리 형태라는것이 난이도의 메인 요소라서 요약에서도 명시했습니다.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>변의 길이가 홀수인 박스가 있을 때, 이 박스가 $(r, c)$를 밑면의 중심으로 한다면 이를 “$(r, c)$ 위에 있다” 라고 부르겠습니다. 이하, $N$ 은 그리드의 변길이 (1000), $Q$는 쿼리의 개수 (30만) 입니다.</p>

  <p>쿼리가 한 개만 주어진다고 생각하면, 다음과 같이 문제를 풀 수 있습니다.</p>
  <ul>
    <li>각 칸에 대해, “가장 가까운 막혀 있는 칸” 이 어디인지를 알면, 이 칸 위에 올 수 있는 박스의 최대 크기를 알 수 있습니다.</li>
    <li>모든 칸에 대해, 이 칸 위에 올 수 있는 박스의 최대 크기를 안다는 것은, 자연수가 쓰여 있는 2차원 그리드가 주어지고, 값이 $k$ 이상인 점들만 밟으면서 $(r_1, c_1)$ 에서 $(r_2, c_2)$ 까지 이동할 수 있는지 판단하는 문제입니다.</li>
    <li>이 문제는 그래프로 만들고 직접 탐색을 하면 쉽게 풀 수 있습니다.</li>
    <li>각 칸에 대해 가장 가까운 막힌칸을 찾는 작업은, 반대로 막힌 칸들로부터 시작해서 BFS를 돌면 쉽게 할 수 있습니다.</li>
  </ul>

  <p>즉, 이 작업은 $O(N^2)$ 시간에 모두 수행할 수 있습니다. 그러나 $O(N^2 Q)$ 로는 시간제한을 통과할 수 없습니다.</p>

  <p>시간 복잡도를 낮추기 위해서는, 병렬 이분 탐색을 생각할 수 있습니다. 
1) 쿼리 문제이고, 각 쿼리를 오프라인으로 처리할 수 있으며
2) 각 쿼리를 이분탐색으로 처리할 수 있기 때문입니다.</p>

  <p>병렬 이분 탐색에 대해서는 <a href="https://blog.naver.com/kks227/221410398513">kks227님의 블로그 글</a> 이 매우 쉽게 소개하고 있습니다. 
요점은 각 쿼리에 대해 [lo, hi] 값을 관리하면서, 한번 도는 사이에 모든 쿼리에 대해 업데이트하는 것입니다.</p>

  <p>한 쿼리를 이분 탐색하는 방법을 생각해 봅시다. 큰 박스가 올라갈 수 있는 (값이 큰) 칸부터 추가하면서,</p>
  <ul>
    <li>$t$의 크기를 허용하는 칸을 모두 추가하고</li>
    <li>현재 밟을 수 있는 칸들 중, 인접한 칸들 간에 유니온-파인드로 서로 연결되어 있음을 표시하면</li>
    <li>각 쿼리마다 유니온-파인드를 이용, $s\to t$ 도달가능한지 판정할 수 있습니다.</li>
  </ul>

  <p>여기에 병렬 이분 탐색을 적용하여 전체를 $O(QN\log N)$ 시간에 풀 수 있고, 이 값이 조금 커 보이지만 제한시간이 8초이므로 통과할 수 있습니다.</p>
</details>
<hr />]]></content><author><name></name></author><category term="competitive-programming" /><category term="competitive-programming" /><summary type="html"><![CDATA[일정상 연습은 같이 참여하지 못하고, 혼자 문제만 풀어봤습니다. 이번에는 CERC 문제들만으로 세팅했다고 들었는데, 역시 퀄리티가 훌륭합니다.]]></summary></entry><entry><title type="html">11월 1주차 알고리즘 문제풀이</title><link href="https://gratus907.github.io/competitive-programming/Nov23W1-ProblemSolving/" rel="alternate" type="text/html" title="11월 1주차 알고리즘 문제풀이" /><published>2023-11-05T00:00:00+09:00</published><updated>2023-11-05T00:00:00+09:00</updated><id>https://gratus907.github.io/competitive-programming/Nov23W1-ProblemSolving</id><content type="html" xml:base="https://gratus907.github.io/competitive-programming/Nov23W1-ProblemSolving/"><![CDATA[<p>예전 ICPC 준비했던 팀원들이 (아직 졸업을 하지 않아서) 올해 ICPC에 출전한다고 하여, 바쁜 일상에서 잠시 벗어나는 겸 해서 연습에 끼어들어 같이 풀었습니다. 오랜만에 (해커컵같은 대회를 제외하고) 순수히 즐길수있는 PS에 참여하니 굉장히 refreshing 한 느낌이었습니다. ㅋㅋ…</p>

<hr />
<h3 id="amppz-2011f-laundry">AMPPZ 2011F. Laundry</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 7911 / Poland Collegiate Programming Contest (AMPPZ) 2011F.</code><br />
난이도: <span style="color: rgb(0, 199, 139);">Platinum IV</span></p>

<p>문제의 서술이 매우 혼란스럽습니다. 스토리가 문제의 해석을 방해하는 전형적인 경우가 아닌가 싶은데… 아래에는 문제의 수학적인 formulation을 유지하면서 스토리를 단순화하여 기술합니다.</p>

<p><strong>문제 요약:</strong> 수열 $d_1, \dots d_n$에 대해, $a_i = 2d_i, b_i = 3d_i$ 라 하자. 1번부터 $K$번 색까지의 페인트가 각각 $l_1 \dots l_K$ 만큼 있고, 이것을 이용하여 $a_i$ 들과 $b_i$ 들에게 색깔을 칠하고자 한다.
최소 종류의 페인트를 사용하면서 다음의 조건을 만족하도록 할 때, 필요한 페인트는 최소 몇 종류인가?</p>
<ol>
  <li>어떤 색도 서로 다른 $i$ 를 칠하는 데 사용할 수 없다 (즉, $a_2$ 와 $b_2$를 커버하는 것은 가능하나, $a_1$ 과 $b_2$ 를 커버할 수는 없음)</li>
  <li>$a_i$ 와 $b_i$ 하나는 두 개 이상의 색으로 나누어 색칠할 수 없다.</li>
</ol>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>다음과 같은 그리디 알고리즘으로 해결할 수 있습니다. 처음에는 $a_i$ 와 $b_i$ 를 합쳐서 $i$번 작업으로 생각합니다.</p>
  <ul>
    <li>만약 한번에 $a_i$ 와 $b_i$ 를 하나의 $j$로 처리할 수 있다면, 당연히 그렇게 처리하고 싶습니다.</li>
    <li>만약 그렇지 못하다면, $a_i$ 와 $b_i$ 를 쪼개어 각각의 작업으로 만들어야 합니다.</li>
    <li>이렇게 작업을 나누고 나면, 최대한 다른 작업의 수행을 방해하지 않기 위해서는 ‘이 작업을 수행할 수 있는 가장 작은 $l_j$’ 를 가지고 가야 합니다.</li>
    <li>“쪼개진 작업” 은 지금 처리하지 않으면 미래에는 처리하지 못하게 될 수도 있으므로 (다른 작업을 하다가 큰 색깔들을 써버려서), 우선 처리해야 합니다.
시간복잡도는 priority queue와 set/multiset 같은것을 잘 사용하면 $O(n \log n)$에 할 수 있습니다.</li>
  </ul>
</details>

<hr />

<h3 id="ctu-open-contest-2004e-electricity">CTU Open Contest 2004E. Electricity</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 6672 / CTU Open Contest 2004E.</code><br />
난이도: <span style="color: rgb(0, 199, 139);">Platinum III</span></p>

<p><strong>문제 요약:</strong> 그래프 $G$가 주어진다. 이때, 한 정점(과 그 정점에 이어진 간선들)을 삭제하여, 그래프의 Connected component의 개수를 최대화하라.</p>

<p>Note: :angry: 예제가 정말 아무런 도움이 되지 않습니다. :rage:</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>단절점 알고리즘은 대략 다음과 같습니다. DFS tree를 만들면서 방문시간을 기록하는데,</p>
  <ul>
    <li>루트에서는 child의 개수가 2개 이상이면 단절점이고</li>
    <li>루트가 아닌 점에서는, 내 DFS-서브트리들 중 ‘나보다 방문시간이 빠른 정점’ 을 방문할 수 없는 서브트리가 있다면 단절점입니다. 
이때, 위 알고리즘은 사실 ‘단절되는 component’ 가 무엇인지도 찾을 수 있습니다.</li>
    <li>루트를 지우면 서브트리들이 각각 단절되는 component고</li>
    <li>루트가 아닌 점에서는 ‘문제의 서브트리’ 들이 각각 단절되는 component입니다. 
따라서, 단절점 알고리즘을 약간 변형, $O(V + E)$ 에 해결할 수 있습니다.</li>
  </ul>

  <p>주의: 간선이 0개인 경우, 정점은 하나를 지워야 하므로 (정점 개수) 가 아니라 (정점 개수 - 1)이 답이 됩니다. 
예제에 이것마저 없었다면 아무도 맞추지 못했을 것 같습니다. :rage:</p>
</details>

<hr />

<h3 id="amppz-2012h-hydra--경기과학고-2016-나코더-송년대회-f-장비를-정지합니다">AMPPZ 2012H Hydra / 경기과학고 2016 나코더 송년대회 F 장비를 정지합니다</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 7981 / Poland Collegiate Programming Contest (AMPPZ) 2012H / 경기과학고 나는코더다 송년대회 2016F</code><br />
난이도: <span style="color: rgb(0, 199, 139);">Platinum II</span></p>

<p>이렇게 한문제에 전혀다른 출처가 두개 이상 달리는게 간혹 있던데, 문제를 가져다 쓴건지 뭔지 잘 모르겠네요.</p>

<p>문제 statement가 충분히 concise하게 잘 쓰여져 있어, 원문으로 충분한 것 같아 바로 풀이만 작성합니다.</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>Dhdroid가 알려주지 않았다면 아마 해결하기 힘들었을것 같습니다. solved.ac 난이도에 비해 훨씬 어렵다고 생각합니다.</p>

  <p>약한충격의 코스트 $u_i$ 와 강한충격의 코스트 $z_i$에 더하여, “진짜 최소 코스트” $x_i$ 가 있다고 생각하겠습니다. 다음을 관찰하는 것이 매우 중요합니다.</p>
  <ul>
    <li>$N$개의 장비들 중, <strong>가장 싼값에 강한 충격을 가할수 있는</strong> 장비 $k$에 대하여, $x_k = z_k$입니다.</li>
    <li><strong>증명:</strong> 약한 충격에 의해 열리는 장비의 리스트가 비어있지 않음이 주어졌으므로, 약한충격을 이용해서 장비 하나를 (깨끗하게) 닫기 위해서는 다른 어디선가는 강한충격을 써야만 합니다.</li>
    <li>또한, 만약 어떤 $x_k$ 가 정해졌다면, $k$를 ‘여는’ 약한 충격들에 대해, 이것들이 $k$를 연다고 하는 대신 약한 충격의 가격을 $x_k$만큼 올려버려도 답이 바뀌지 않습니다.</li>
    <li>따라서 다익스트라 알고리즘과 비슷한 방법으로 구현할 수 있습니다.</li>
  </ul>

  <p>시간복잡도는 장비의 개수 $N$과 간선개수 ($r_i$들의 합) $R$에 대해, $O(N \log N + R)$ 시간에 구현할 수 있습니다.</p>

  <p><a href="http://boj.kr/649f9cfb8f2f4610a209b8366d6d4447">코드 링크</a></p>
</details>

<hr />

<h3 id="cerc-2012-i-the-dragon-and-the-knights">CERC 2012 I The Dragon and the Knights</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 3413 / ICPC Central European Regional Contest 2012 I</code><br />
난이도: <span style="color: rgb(0, 199, 139);">Platinum I</span></p>

<p><strong>문제 요약:</strong> 평면상에 $N$개의 직선과 $M$ 개의 점이 주어질 때, <strong>직선으로 인해 나누어지는 평면의 영역</strong> 하나당 점이 적어도 한개씩 들어있는지 여부를 판정하라.</p>

<details>
  <summary><b>풀이 보기:</b></summary>

  <p>직선 $N$개가 주어졌을 때, 이로 인해 나누어지는 영역이 몇 개인지는 <strong>오일러 공식</strong> 으로 구할 수 있습니다. 직선과 직선들이 만나는 교점들을 정점으로 보면, 전체는 평면 그래프가 됩니다. 
따라서, $V - E + F = 2$ 에 따라, $V$ 와 $E$ 를 알고 있으므로 $F$를 구하면 됩니다.</p>

  <p>각 영역마다 점이 하나씩 들어있는지 여부를 판정하는 것은 어려운 일이므로, $M$개의 점이 서로 다른 영역을 몇개 커버하는지로 문제를 바꾸어 풀 것입니다. 
각 점 $x$에 대해, $f(x)$를 $N$ 차원의 boolean vector로, $f(x)_i$ 는 $x$가 $i$번째 직선의 오른쪽에 있는지 (CCW) 여부로 정의합니다. 
이제, 각 ‘영역’은 $f(x)$ 가 서로 다른 점들의 집합이므로, $N$개의 점에 대해 각각 $f(x)$ 벡터를 구하고, 이들중 서로 다른 것이 몇 개인지를 판정하면 됩니다.</p>

  <p>저는 $M$개의 점들을 집합으로 관리하고, 직선을 하나씩 추가하면서, 오른쪽과 왼쪽에 있는 점들이 갈린다면 새로운 집합으로 갈라주는 식으로 구현했습니다. 이렇게 하는 경우, 빈 집합을 만들지 않는다면 $M$개 이상의 파티션을 나누는 일은 없으므로 $O(N^2 + NM)$ 시간 알고리즘을 구현할 수 있습니다 ($N^2$는 교점을 모두 구해야 하므로)</p>

  <p>기하 문제가 늘 그렇듯 교점을 주의해야 합니다. 다만, 이 문제의 경우 서로 다른 세 직선이 한 점에서 만나지 않는다는 것을 보장하므로, 직선들을 방향벡터에 따라 나누어 관리하면 쉽게 할 수 있습니다.</p>

  <p>Note: 구현해보지는 않았지만, $f(x)$를 모두 구한 다음, 벡터들을 해싱하거나 정렬해서 비교하기만 한다면 훨씬 쉽게 구현할 수 있을 것입니다.</p>
</details>

<hr />

<h3 id="cerc-2012-b-who-wants-to-live-forever">CERC 2012 B Who Wants to Live Forever?</h3>
<p><code class="language-plaintext highlighter-rouge">BOJ 3406 / ICPC Central European Regional Contest 2012 B</code><br />
난이도: <span style="color: rgb(0, 158, 229);">Diamond V</span></p>

<p><strong>문제 요약:</strong> 0 또는 1의 값을 갖는 수열 $x_1 \dots x_N$이 다음 recurrence에 의해 시간에 따라 변화한다. ($\oplus$ 는 XOR)
\(x_i(t) = x_{i-1}(t-1) \oplus x_{i+1}(t-1)\) 
(단, 맨 왼쪽과 오른쪽에 벽으로 막힌 값은 0으로 간주) 무한히 많은 시간이 지났을 때, 수열 전체가 $x_i = 0$이 되는가?</p>

<details>
  <summary><b>풀이 보기:</b></summary>
  <p>일종의 콘웨이 생명 게임 같은 문제입니다. 한참 고민하다가 풀이를 보고서야 정말 멋진 문제였음을 알게 되었습니다.</p>

  <ul>
    <li>먼저, 자명한 경우로 시작할 때 모두 0이면 정답은 True입니다.</li>
    <li>그렇지 않을 때, $x_1, x_2, x_3, x_4$ 에서 1초가 지난 상황에서 수열은 다음과 같습니다. 
\(x_2 \quad (x_1 \oplus x_3) \quad (x_2 \oplus x_4) \quad x_3\)
이제, 여기서 더 시간이 지나면 
\((x_1 \oplus x_3) \quad x_4 \quad x_1 \quad(x_2 \oplus x_4)\)
\(x_4 \quad x_3 \quad x_2 \quad x_1\)
이렇게 진행되게 되는데, 여기서 중요한 점은 수열의 길이가 짝수이기 때문에, 돌리다보면 $x_1, x_2, x_3, x_4$ 가 무한히 많이 반복된다는 점입니다.</li>
    <li>따라서, 짝수 길이 수열의 경우, 단 하나라도 0이 아닌 값이 있다면 이 0이 아닌 값을 지울수 없습니다.</li>
    <li>홀수의 경우 약간 다릅니다. $x_1, x_2, x_3, x_4, x_5$ 에 대해 위 예시처럼 진행해 보면, 
\(x_2 \quad (x_1 \oplus x_3) \quad (x_2 \oplus x_4) \quad (x_3 \oplus x_5) \quad x_4\)
\((x_1 \oplus x_3) \quad x_4 \quad (x_1 \oplus x_5) \quad x_2 \quad (x_3 \oplus x_5)\)
\(x_4 \quad (x_3 \oplus x_5) \quad (x_2 \oplus x_4) \quad (x_1 \oplus x_3) \quad x_2\)
\((x_3 \oplus x_5) \quad x_2 \quad (x_1 \oplus x_5) \quad x_4 \quad (x_1 \oplus x_3)\)
이렇게 변화하게 됩니다. 다시 여기서도 관찰해 보면,
      <ul>
        <li>짝수 인덱스들은 시간이 지남에 따라 원래대로 돌아오고,</li>
        <li>홀수 인덱스들은 약간 까다롭지만, 홀수 인덱스를 생각하는 대신 1초가 지난 후의 짝수 인덱스들을 생각하면 이들도 같은 성질을 만족합니다.</li>
        <li>따라서, $t = 0$에서의 $x_2(0), x_4(0) \dots$ 와, $t = 1$에서의 $x_2(1), x_4(1) \dots $를 따로따로 생각하면,</li>
        <li>이들이 둘 모두 0으로 안정화되는 것은 전체가 안정화될 필요충분조건입니다. (이들은 서로 독립적이므로)</li>
      </ul>
    </li>
    <li>즉, 홀수 길이 수열은 두개의 더 작은 수열을 확인하는 것으로 문제를 줄일 수 있고,</li>
    <li>이는 Divide and Conquer로, Master’s Theorem이 적용되므로 복잡도를 $O(n \log n)$ 으로 바운드할수 있습니다.</li>
  </ul>
</details>
<hr />

<p>셋 총평: 문제가 이상하다고 생각했지만 항상 깊이 생각하다보면 이상한건 나라는 사실을 깨닫게 됩니다. 
특히 전혀 생각지 못한 다익스트라의 아이디어를 쓰는 3번이나, 분할정복으로 깔끔하게 맞추는 5번은 풀이가 깔끔하고 아름다운데도 정말 어려웠습니다. :cry:</p>]]></content><author><name></name></author><category term="competitive-programming" /><category term="competitive-programming" /><summary type="html"><![CDATA[예전 ICPC 준비했던 팀원들이 (아직 졸업을 하지 않아서) 올해 ICPC에 출전한다고 하여, 바쁜 일상에서 잠시 벗어나는 겸 해서 연습에 끼어들어 같이 풀었습니다. 오랜만에 (해커컵같은 대회를 제외하고) 순수히 즐길수있는 PS에 참여하니 굉장히 refreshing 한 느낌이었습니다. ㅋㅋ…]]></summary></entry><entry><title type="html">[Reading] Lightning Fast and Space Efficient k-clique Counting</title><link href="https://gratus907.github.io/paper-reviews/DPColorPath/" rel="alternate" type="text/html" title="[Reading] Lightning Fast and Space Efficient k-clique Counting" /><published>2023-02-10T00:00:00+09:00</published><updated>2023-02-10T00:00:00+09:00</updated><id>https://gratus907.github.io/paper-reviews/DPColorPath</id><content type="html" xml:base="https://gratus907.github.io/paper-reviews/DPColorPath/"><![CDATA[<blockquote>
  <p>Xiaowei Ye, Rong-Hua Li, Qiangqiang Dai, Hongzhi Chen, and Guoren Wang. 2022. Lightning Fast and Space Efficient k-clique Counting. In Proceedings of the ACM Web Conference 2022 (WWW ‘22)</p>
</blockquote>

<h3 id="introduction">Introduction</h3>
<ul>
  <li>Analyzing cliques in graph data is critical in many applications, but exact counting or enumeration of these structures can be computationally costly.</li>
  <li><strong>Problem:</strong> Given a graph $G$ and $k \in \N$, estimate the number of $k$-cliques in $G$.</li>
</ul>

<h4 id="sampling-algorithm-for-counting-cliques">Sampling Algorithm for Counting Cliques</h4>
<ul>
  <li><strong>Sampling</strong> is often the way to go when the objective is to count some structure in a large graph.</li>
  <li>The aim is to efficiently gather <strong>samples</strong> from a <strong>sample space</strong> which encapsulates the set we’re interested in.</li>
  <li>Assuming that the set of interest is $\mathcal{A}$ and the sample space is $\Omega$. If it is possible to obtain uniform random samples from $\Omega$, it is natural to take $t$ samples, and count the number of samples that are in $\mathcal{A}$.</li>
  <li>For this simple algorithm, the <strong>Chernoff’s Bound</strong> ensures a probabilisitic guarantee.
<span style="display:block" class="math_item">
  <b class="math_item_title">Chernoff’s Bound for Sampling</b><br />
  Let $\rho = \abs{\mathcal{A}} / \abs{\Omega}$. A uniform sampling algorithm returns a $1 - \epsilon$ approximation of $\abs{\mathcal{A}}$ with probability $1 - 2\sigma$ if more than $\frac{3}{\rho\epsilon^2}\log(1/\sigma)$ samples are taken uniformly at random.
</span></li>
  <li>Hence, the ultimate aim is to maximize $\rho$, which essentially means finding a sample space that closely mirrors $\mathcal{A}$.</li>
</ul>

<h3 id="key-ideas">Key Ideas</h3>
<p>This paper develops an efficient algorithm for $k$-clique estimation via uniform sampling. 
By employing a greedy coloring strategy, the algorithm initially reduces the sample space to the set of $k$-colored sets/paths, which are structures that have a high likelihood of being cliques. The counting of the number of $k$-colored sets/paths is achieved through dynamic programming.</p>

<p>For sparse graphs, <span style="font-family:Helvetica;">PIVOTER (WSDM 20)</span> already performs remarkably well. The authors thereby propose a framework where given graph is split into sparse and dense region, and run PIVOTER on sparse region, while the dense region is dealt with the sampling algorithm authors propose.</p>

<h4 id="dp-based-colored-set-sampling-dpcolor">DP-Based Colored Set Sampling (DPColor)</h4>
<ul>
  <li>Consider the proper graph coloring (no edge should connect vertices with same color).</li>
  <li>A $k$-colored set (set of $k$ vertices with distinct color) is a good candidate for cliques!
<span style="display:block" class="math_item">
  <b class="math_item_title">Correctness (Unbiasedness) of Sample Space</b><br />
  If a set $\set{v_1, \dots, v_k}$ is a $k$-clique in $G$, it must have distinct colors for any given proper coloring.
</span></li>
  <li>To use small number of colors, use degeneracy-ordered greedy coloring</li>
  <li>To sample $k$-colored set, we shall count the number of $k$-colored set via dynamic programming.</li>
  <li>Let $a_i$ be the number of nodes with color $i$, and $F(i, j)$ be the number of $j$-colored sets, considering the vertices with color only up to $i$. The $F(i, j)$ follows the following recurrence.
\(F(i, j) = a_i \times F(i-1, j-1) + F(i-1, j)\)</li>
  <li>Using this as weights, uniform random sampling can be easily done.</li>
</ul>

<h4 id="dp-based-colored-path-sampling-dpcolorpath">DP-Based Colored Path Sampling (DPColorPath)</h4>
<ul>
  <li>How to improve further? Instead of $k$-colored set, consider $k$-colored paths.</li>
  <li>Choose a center node $u$ arbitrarily. From $N(u)$, count and sample $k$-colored path.</li>
  <li>This is much more likely to be a clique than $k$-colored sets.</li>
  <li>Similar to $k$-colored sets, $k$-colored paths (locally on $N(u)$) can be counted via dynamic programming.</li>
</ul>

<h3 id="results">Results</h3>
<ul>
  <li>The DPColorPath method demonstrates significant speed (an order of magnitude faster than the state-of-the-art methods) and negligible (0.1%) error on large-scale real-world graph datasets, including social networks and citation networks.</li>
  <li>The $\rho$ value for $k$-colored paths are much higher than $k$-colored sets</li>
</ul>

<hr />
<p>Overall, impressive results (accuracy and speed) with relatively simple algorithm. Implementation seems also reasonably doable.
Giving more structure on the graph via proper coloring to reduce the sample space seems like a really nice idea.</p>]]></content><author><name></name></author><category term="paper-reviews" /><category term="paper-review" /><category term="cliques" /><category term="graph-algorithms" /><summary type="html"><![CDATA[Xiaowei Ye, Rong-Hua Li, Qiangqiang Dai, Hongzhi Chen, and Guoren Wang. 2022. Lightning Fast and Space Efficient k-clique Counting. In Proceedings of the ACM Web Conference 2022 (WWW ‘22) Introduction Analyzing cliques in graph data is critical in many applications, but exact counting or enumeration of these structures can be computationally costly. Problem: Given a graph $G$ and $k \in \N$, estimate the number of $k$-cliques in $G$. Sampling Algorithm for Counting Cliques Sampling is often the way to go when the objective is to count some structure in a large graph. The aim is to efficiently gather samples from a sample space which encapsulates the set we’re interested in. Assuming that the set of interest is $\mathcal{A}$ and the sample space is $\Omega$. If it is possible to obtain uniform random samples from $\Omega$, it is natural to take $t$ samples, and count the number of samples that are in $\mathcal{A}$. For this simple algorithm, the Chernoff’s Bound ensures a probabilisitic guarantee. Chernoff’s Bound for Sampling Let $\rho = \abs{\mathcal{A}} / \abs{\Omega}$. A uniform sampling algorithm returns a $1 - \epsilon$ approximation of $\abs{\mathcal{A}}$ with probability $1 - 2\sigma$ if more than $\frac{3}{\rho\epsilon^2}\log(1/\sigma)$ samples are taken uniformly at random. Hence, the ultimate aim is to maximize $\rho$, which essentially means finding a sample space that closely mirrors $\mathcal{A}$. Key Ideas This paper develops an efficient algorithm for $k$-clique estimation via uniform sampling. By employing a greedy coloring strategy, the algorithm initially reduces the sample space to the set of $k$-colored sets/paths, which are structures that have a high likelihood of being cliques. The counting of the number of $k$-colored sets/paths is achieved through dynamic programming. For sparse graphs, PIVOTER (WSDM 20) already performs remarkably well. The authors thereby propose a framework where given graph is split into sparse and dense region, and run PIVOTER on sparse region, while the dense region is dealt with the sampling algorithm authors propose. DP-Based Colored Set Sampling (DPColor) Consider the proper graph coloring (no edge should connect vertices with same color). A $k$-colored set (set of $k$ vertices with distinct color) is a good candidate for cliques! Correctness (Unbiasedness) of Sample Space If a set $\set{v_1, \dots, v_k}$ is a $k$-clique in $G$, it must have distinct colors for any given proper coloring. To use small number of colors, use degeneracy-ordered greedy coloring To sample $k$-colored set, we shall count the number of $k$-colored set via dynamic programming. Let $a_i$ be the number of nodes with color $i$, and $F(i, j)$ be the number of $j$-colored sets, considering the vertices with color only up to $i$. The $F(i, j)$ follows the following recurrence. \(F(i, j) = a_i \times F(i-1, j-1) + F(i-1, j)\) Using this as weights, uniform random sampling can be easily done. DP-Based Colored Path Sampling (DPColorPath) How to improve further? Instead of $k$-colored set, consider $k$-colored paths. Choose a center node $u$ arbitrarily. From $N(u)$, count and sample $k$-colored path. This is much more likely to be a clique than $k$-colored sets. Similar to $k$-colored sets, $k$-colored paths (locally on $N(u)$) can be counted via dynamic programming. Results The DPColorPath method demonstrates significant speed (an order of magnitude faster than the state-of-the-art methods) and negligible (0.1%) error on large-scale real-world graph datasets, including social networks and citation networks. The $\rho$ value for $k$-colored paths are much higher than $k$-colored sets Overall, impressive results (accuracy and speed) with relatively simple algorithm. Implementation seems also reasonably doable. Giving more structure on the graph via proper coloring to reduce the sample space seems like a really nice idea.]]></summary></entry><entry><title type="html">Boyer-Moore Heuristic Pattern Matching</title><link href="https://gratus907.github.io/advanced-algorithms/boyer-moore-algorithm/" rel="alternate" type="text/html" title="Boyer-Moore Heuristic Pattern Matching" /><published>2021-10-27T00:00:00+09:00</published><updated>2021-10-27T00:00:00+09:00</updated><id>https://gratus907.github.io/advanced-algorithms/boyer-moore-algorithm</id><content type="html" xml:base="https://gratus907.github.io/advanced-algorithms/boyer-moore-algorithm/"><![CDATA[<h2 id="motivation">Motivation</h2>
<p>Boyer-Moore 알고리즘이 해결하는 문제는 KMP와 똑같이, 어떤 $n$글자의 긴 텍스트 $T$에 대해, 짧은 $m$글자의 패턴 $P$를 매칭하는 것입니다.</p>
<ul>
  <li>가장 Naive하게 $T$의 모든 위치에 대해 $m$글자를 매칭해보는 알고리즘은 $O(nm)$ 입니다.</li>
  <li><a href="https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm">KMP 알고리즘</a> 은 (언젠가 작성할 계획은 있지만 우선순위는 낮습니다) 이를 $O(n + m)$ 으로 줄인 엄청난 성과를 보입니다.</li>
</ul>

<p>Boyer-Moore 알고리즘은 worst case에서는 $O(nm)$이지만, string이 랜덤하게 주어진다면 평균 $O(n / m)$ 복잡도를 보입니다.</p>

<p>기본적으로, 이 알고리즘은 패턴을 <strong>오른쪽부터 왼쪽으로</strong> 매칭하고, 문자열 자체는 (즉 매칭하는 위치 자체는) <strong>왼쪽에서 오른쪽으로</strong> 봅니다. 이 방향의 차이에 주목할 필요가 있습니다. KMP의 경우는 패턴과 텍스트 모두 좌 -&gt; 우 로 매칭합니다. 가능한한 ‘첫’, ‘두번째’ 와 같은 말은 오른쪽에서 왼쪽으로 매칭하는 실제 세팅에, ‘1번’, ‘2번’ 등의 말은 진짜 인덱스를 의미하도록 작성했습니다.</p>

<h2 id="algorithm--bad-character-heuristic">Algorithm : Bad Character Heuristic</h2>
<p>텍스트 <code class="language-plaintext highlighter-rouge">abcacbcadc</code> 에서 패턴 <code class="language-plaintext highlighter-rouge">acbcda</code>를 매칭한다고 생각해 봅시다. 이때, 뒤에서부터 앞으로 매칭을 시도하는 것은 텍스트 <code class="language-plaintext highlighter-rouge">abcacb</code> 와 <code class="language-plaintext highlighter-rouge">acbcda</code>를 매칭하는 것입니다. 여기서 첫 글자 (패턴을 <strong>오른쪽부터</strong> 읽으므로 텍스트와 패턴의 첫 글자는 각각 6번 위치인 b와 a입니다!) 를 매칭하려고 시도했을 때, a를 찾아야 하는데 b를 찾았으므로 실패했습니다.</p>

<p>Naive matching은 여기서 포기하고 다음 위치인 <code class="language-plaintext highlighter-rouge">bcacbc</code>와의 매칭을 시도하겠지만, Boyer-Moore의 알고리즘은 여기서 “그럼 만약, 이 6번위치의 b를 꼭 써야 한다면, 어디까지 내가 패턴을 밀어야 b를 쓸 수 있느냐?” 라는 질문을 던집니다. 생각해보면 텍스트를 기준으로 패턴을 한칸 밀어봤자, 패턴의 5번 글자인 d와 b를 매칭하게 될 것이고 이는 어차피 실패할 것이기 때문입니다. 패턴의 맨 뒤를 기준으로 3글자를 밀어야 b를 텍스트 6번 b에 맞출 수 있으므로, 이만큼을 push해 버릴 수 있습니다. 여기서 이 ‘b’ 를 <strong>Bad Character</strong> 라고 부를 것입니다.</p>

<p>이를 좀더 정리하면…</p>
<ul>
  <li>Bad character가 패턴에 아예 등장하지 않으면, 패턴을 확 밀어서 아예 넘어가도 됩니다.</li>
  <li>Bad character가 패턴에서 <strong>가장 오른쪽에</strong> 등장하는 위치가 현재 보고있는 bad character의 패턴에서의 위치보다 왼쪽이면, 그만큼을 밀어도 됩니다.</li>
  <li>Bad character가 패턴에서 <strong>가장 오른쪽에</strong> 등장하는 위치가 현재 보고있는 bad character의 패턴에서의 위치보다 오른쪽이면 얻을 수 있는 정보가 없습니다.</li>
</ul>

<h2 id="algorithm--good-suffix-heuristic">Algorithm : Good Suffix Heuristic</h2>
<p>이 방법은 자세히 설명하지 않을 것입니다. (이유는 후술합니다) Good suffix란, 어떻게 보면 위 Bad character의 3번 경우에 얻는 정보가 없음을 거꾸로 이용하는 방법인데요. 3번 경우는 아마도 꽤 많은 글자들이 맞은 다음 처음으로 bad character를 만난 상황일 것입니다. 즉 pattern의 꽤 긴 suffix가 이미 맞고 있는 상황이라는 의미가 됩니다. 이 Good suffix를 패턴에서 다시 맞추려면 얼만큼 이동해야 하는지를 미리 모두 precomputation해 두면, 그만큼을 점프할 수 있습니다. 당연히 맞는 suffix가 길수록 이 suffix를 다시 맞추기가 어려울 것이므로, 꽤 멀리 점프할 수 있을 것 같습니다. 이 precomputation은 “Pattern의 길이 k인 suffix가 다시 suffix로 등장하는 pattern의 prefix 위치” 를 마킹하면 되고, KMP의 실패함수와 매우 유사한 방법으로 구할 수 있습니다.</p>

<h2 id="boyer-moore-horspool-algorithm">Boyer-Moore-Horspool Algorithm</h2>
<p>Horspool의 알고리즘은 위 Boyer-Moore에서 Good suffix heuristic을 아예 포기하고, Bad character는 항상 현재 매칭 위치의 마지막 글자만 고려합니다. 이렇게 해도 평균 시간 복잡도 $O(n / m)$ 비슷한 시간을 유지할 수 있음이 알려져 있지만, 그 증명 과정은 엄청난 수식과 고통스러운 증명 (부등식 줄이기)을 요구합니다. 다만, 충분히 랜덤한 텍스트에 대해서는 B-M-H가 굉장히 빠름이 잘 알려져 있습니다.</p>

<p>Horspool은 굉장히 쉽게 구현할 수 있습니다. 먼저 각 character에 대해 패턴의 오른쪽 끝에서 가장 가까운 (하지만 오른쪽 끝은 아닌) 등장 위치를 계산해 두고, bad character에 걸리면 그만큼 push하면 됩니다.</p>

<p>여기서는 정말 러프한 증명…도 아니고 argument를 하나 소개하고 마치겠습니다. 알파벳 $q$글자 중 랜덤하게 생성된 string $P, T$에서의 Horspool 알고리즘을 가정하겠습니다. 이중 한 글자가 패턴의 오른쪽 끝에서 얼마나 멀리 있을지 그 기댓값을 생각해 봅시다. 알파벳 $x$를 이용하여 $k$길이 이상의 jump를 허용하기 위해서는 뒤에서 $k-1$개의 글자는 $x$가 아닌 다른 글자여야 하므로, 점프 길이가 $k$ 이상일 확률은 $\left(1-\frac{1}{q}\right)^{k-1}$ 입니다. $r = \left(1-\frac{1}{q}\right)$ 로 쓰면 편하게 이를 $r^{k-1}$ 로 쓸 수 있습니다.</p>

<p>$\expect{X} = \sum_{k = 1}^{\infty} \P(X \geq k)$ 의 공식을 이용합니다. $k \geq m+1$의 확률은 0이므로, 
\(\expect{\text{jump length}} = \sum_{j = 1}^{m} r^{j-1} = \frac{r^m - 1}{r - 1}= q(1 - r^m)\)</p>

<p>따라서, $m$이 충분히 크면 대충 $q$ 정도의 shift는 기대할 수 있으므로, $O(n / q)$ 정도의 퍼포먼스는 기대해 볼 수 있습니다. 당연히 이는 각 글자가 iid random이라는 이루어지지 않는 가정이 들어갔을 뿐만 아니라, 각 위치에서 bad character를 만나는 데 걸리는 매칭개수도 무시하고 있지만, 간단한 argument로는 그럭저럭 기능합니다.</p>]]></content><author><name></name></author><category term="advanced-algorithms" /><category term="study" /><category term="algorithms" /><category term="string-algorithms" /><summary type="html"><![CDATA[Motivation Boyer-Moore 알고리즘이 해결하는 문제는 KMP와 똑같이, 어떤 $n$글자의 긴 텍스트 $T$에 대해, 짧은 $m$글자의 패턴 $P$를 매칭하는 것입니다. 가장 Naive하게 $T$의 모든 위치에 대해 $m$글자를 매칭해보는 알고리즘은 $O(nm)$ 입니다. KMP 알고리즘 은 (언젠가 작성할 계획은 있지만 우선순위는 낮습니다) 이를 $O(n + m)$ 으로 줄인 엄청난 성과를 보입니다.]]></summary></entry><entry><title type="html">Aho-Corasick Multiple Pattern Matching</title><link href="https://gratus907.github.io/advanced-algorithms/aho-corasick-algorithm/" rel="alternate" type="text/html" title="Aho-Corasick Multiple Pattern Matching" /><published>2021-10-27T00:00:00+09:00</published><updated>2021-10-27T00:00:00+09:00</updated><id>https://gratus907.github.io/advanced-algorithms/aho-corasick-algorithm</id><content type="html" xml:base="https://gratus907.github.io/advanced-algorithms/aho-corasick-algorithm/"><![CDATA[<p>이 글은 KMP 알고리즘과 Trie 자료구조에 대한 이해를 선행으로 요구합니다.</p>

<h2 id="motivation">Motivation</h2>
<p>어떤 $n$글자의 긴 텍스트 $T$에 대해, 짧은 $m$글자의 패턴 $P$를 매칭하는 문제를 생각해 보겠습니다.</p>
<ul>
  <li>가장 Naive하게 $T$의 모든 위치에 대해 $m$글자를 매칭해보는 알고리즘은 $O(nm)$ 입니다.</li>
  <li><a href="https://en.wikipedia.org/wiki/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm">KMP 알고리즘</a> 은 (언젠가 작성할 계획은 있지만 우선순위는 낮습니다) 이를 $O(n + m)$ 으로 줄인 엄청난 성과를 보입니다.</li>
</ul>

<p>KMP가 이를 가능하게 하는 방법은, <code class="language-plaintext highlighter-rouge">T[i..i+L-1]</code> 과 <code class="language-plaintext highlighter-rouge">P</code>를 매칭하다가 중간에 실패했다고 할 때, Naive 매칭은 <code class="language-plaintext highlighter-rouge">T[i+1..i+L]</code> 을 시도하면서 앞서의 정보를 전혀 이용하지 못합니다. 그러나, 패턴이 <code class="language-plaintext highlighter-rouge">abababa</code>인데, ababa까지 맞고 여섯번째 b가 틀렸다면, 앞 다섯글자까지 맞았다는 정보를 최대한 이용하고 싶습니다. 이를 정말 가능한 최대로 이용하는 것이 KMP 알고리즘이며, 위 위키피디아의 링크와 함께 <a href="https://bowbowbow.tistory.com/6">BowBowBow님의 블로그</a> 글을 참고하면 그렇게 어렵지 않게 배울 수 있습니다. 요점은, 앞 몇글자가 맞았음을 이용해서 절대 맞을리가 없는 위치들을 스킵하는 것입니다. 이를 <strong>실패함수</strong> 라고 부릅니다.</p>

<p>이제, 이를 패턴이 여러 개인 경우로 확장하고자 합니다. 패턴이 $m_1, m_2, \dots m_k$ 글자의 $P_1, \dots P_k$ 라고 하겠습니다.</p>

<h2 id="algorithm">Algorithm</h2>
<p>실패함수는 결국 어떤 prefix까지는 맞았다는 것을 알고 있는 데서 오는데, 우리는 여러 개의 패턴에 대해 비슷한 정보를 관리하고 싶습니다.</p>

<p>Prefix 여러개를 동시에 관리하는 것은 Trie를 이용할 수 있습니다. 
<img src="../../images/a9c2c1743cbd0e6d4b5a6ec257e0bd5864552d77867f1eadf2eb9747fb4a87c5.png" alt="picture 1" /></p>

<p>이 그림을 보면, 파란 간선과 함께 빨간 간선이 그려져 있습니다. 파란 간선은 우리가 일반적으로 알고 있는 Trie의 간선이고, 빨간 간선은 Failure function을 의미합니다. 우리는 다음과 같이 Failure function을 정의합니다.</p>

<p>“패턴 $P$에 대해, 그 prefix $P’$ 까지를 현재 매칭했다고 하자. 이때, $P’$에 해당하는 노드의 실패-노드 $f(P’)$ 을 찾는데, 이는 $P’$의 <strong>proper suffix</strong>이면서, <strong>다른 패턴의 prefix</strong> 인 가장 깊은 노드여야 한다”</p>

<p>이 조건이 무슨 뜻인지 생각해보면…</p>
<ul>
  <li>$P’$을 매칭하다가 실패했다고 하겠습니다. 이제 더이상 이 패턴은 진행할 수 없습니다.</li>
  <li>그러면 이제, 무슨 패턴을 노릴지 결정해야 합니다. 그림에서 cacba를 텍스트 <code class="language-plaintext highlighter-rouge">T[i..]</code>에다가 대고 매칭하다가 실패했다면 현재 위치에서 당장 노릴 수 있는 패턴은 acba, cba, ba, a 등으로 시작하는 패턴을 노릴 수 있습니다.</li>
  <li>이들 중 어떤 다른 패턴의 prefix여야 노리는 것이 의미가 있을 것입니다.</li>
  <li>이러한 노드들이 여러 개 있다면, acba 노드와 cba 노드 중에는 acba 노드를 먼저 확인해야 합니다. 이유는, acba… 를 매칭하다가 실패하면 cba… 패턴은 그 다음에 노려도 되기 때문입니다.</li>
  <li>즉, 텍스트를 스캔하면서 트라이를 따라서 움직이다가, 트라이에서 더이상 갈곳이 없으면 최대한 다른 끝점을 노릴 수 있는 곳으로 이동해서 계속 시도한다는 의미가 됩니다.</li>
</ul>

<p>트라이는 빠르게 construct할 수 있으므로, 이러한 실패함수를 어떻게 계산할지만 따로 생각하면 됩니다. 실패함수는 BFS를 이용하여, depth가 얕은 노드부터 깊은 노드로 건설합니다.</p>
<ul>
  <li>지금 노드 $x$를 보고 있다면, 이 $x$보다 깊이가 얕은 노드 중 반드시 $f(x)$ 가 존재합니다. (proper suffix의 길이는 자기자신보다 짧으므로)</li>
  <li>$x$의 바로 위 부모노드 $p(x)$ 와, $p(x)$에서 $x$로 오는 edge의 알파벳 (즉 $x$의 마지막 글자에 해당하는 알파벳)을 알고 있습니다. 이를 <code class="language-plaintext highlighter-rouge">c</code> 라고 하겠습니다.</li>
  <li>또한, 실패함수는 depth가 얕은 노드부터 계산했으므로 $f(p(x))$ 도 알고 있습니다. 만약 $f(p(x))$ 에서 <code class="language-plaintext highlighter-rouge">c</code>를 이용하여 전진하는 edge가 있다면, 이를 따라 전진합니다.</li>
  <li>그렇지 않다면, $f(f(p(x)))$ 에다 대고 시도하고… 를 반복하면 됩니다.</li>
</ul>

<p>만약 트라이를 따라가다가 어떤 패턴의 끝을 만나면, 그 패턴을 찾았다고 report하면 됩니다. 즉 각 노드는 혹시 내가 어떤 패턴의 끝은 아닌지를 미리 기억하고 있어야 합니다. 이 정보는 사실 Trie에 문자열들을 집어넣을때 미리 잡아줄 수 있으므로 크게 문제될 것이 없습니다.</p>

<p>스캔의 과정을 pseudocode로 표현해 보면 다음과 같습니다.
<img src="../../images/6027d1807c7529d3d303be17844021b919f73bbb3ead7fdfbafc7590b459126b.png" alt="picture 2" /></p>

<h2 id="complexity">Complexity</h2>
<p>알파벳 크기를 $q$, 패턴 전체의 글자수의 총합을 $M$, 텍스트의 글자수를 $n$이라고 하겠습니다. 이때,</p>
<ul>
  <li>Pseudocode를 보면 자명하게 스캔은 $O(n)$ 인것 같지만, 실제로는 $n$에 next 함수가 소모하는 시간만큼이 걸립니다.</li>
  <li>트라이를 구성하는 방법은 구현에 따라 다른데, 가장 일반적인 구현인 child pointer array 방식을 쓰는 경우 $O(qM)$ 시간에 트라이를 구성할 수 있으며 (BFS를 돌려야 해서 이만큼이 소모됩니다) $O(qM)$ 메모리를 소비합니다.</li>
  <li>$q$가 크면 이것이 비효율적일 수 있는데, 트라이의 각 노드에 BBST같은걸 쓴다거나 링크드 리스트를 쓰면 복잡도가 달라집니다. 대표적으로 BBST를 쓰면 $O(M \log q)$ 시간에 트라이를 구성할 수 있고, $O(M \log q)$ 메모리를 소비하는 대신, next가 $O(\log q)$ 시간이 들게 되므로 스캔이 $O(n \log q)$ 걸립니다.</li>
</ul>

<p>따라서, 종합하면 간단하게는 $O(qM + n)$ 시간과 $O(qM)$ 공간을 이용하여 multiple pattern matching을 할 수 있게 됩니다.</p>

<h2 id="구현">구현</h2>
<p><a href="http://boj.kr/d7bb9f5436ed4277ae0829584ceb0b4a">구현 링크</a></p>]]></content><author><name></name></author><category term="advanced-algorithms" /><category term="study" /><category term="algorithms" /><category term="string-algorithms" /><summary type="html"><![CDATA[이 글은 KMP 알고리즘과 Trie 자료구조에 대한 이해를 선행으로 요구합니다.]]></summary></entry></feed>