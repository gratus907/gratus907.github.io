---
layout: single
title: "[논문정리] FixMatch: Simplifying Semi-Supervised Learningwith Consistency and Confidence"
categories: paper-reviews
sidebar:
  nav: "sidepost"
comment: true
comments : true
toc : true
orig-authors: 
venue: NeurIPS21
tag: [paper-review, semi-supervised-learning] 
---

> Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel.  **FixMatch: Simplifying Semi-Supervised Learningwith Consistency and Confidence**. NeurIPS 2021.

<div id="toc">
Contents
</div>
* TOC
{:toc}
----------

오늘 정리할 논문은 2021 NeurIPS에 발표된 **FixMatch**   입니다. 
매우 간단한 아이디어로 상당히 높은 성능을 달성하는데, 특히 barely supervised 세팅에서의 성능이 매우 인상적입니다. 

### Introduction
이 논문에서는 Semi-supervised Learning (준지도학습) 세팅을 생각합니다. 
즉, 약간의 labeled data $D_L = (X_L, Y_L)$ 이 있고,
상대적으로 많은 수의 unlabeled data $D_U = (X_U)$ 가 주어져 있는 상황입니다. 
여기서 어떤 모델 $f_\theta$ 를 학습하는 데 있어, unlabeled data를 적절히 활용하는 것을 목표로 합니다. 

### Background
이하, 두 가지 서로 다른 stochastic augmentation function $\mathcal{A}$ 와 $\alpha$를 생각합니다. 여기서 $\mathcal{A}$ 는 **강한** augmentation (strong) 을, $\alpha$ 는 **약한** augmentation (weak) 을 나타냅니다. 
Stochastic하다는 말은, 매번 $\alpha(x)$ 를 계산할 때마다 (확률적으로) 조금씩 다른 결과물을 얻는다는 뜻으로, 
예를들어 50% 확률로 이미지를 x축 반전하는 함수 $\alpha$를 생각할 수 있습니다.

Augmentation이 약하고 강하다는 것은 수학적인 의미보다는, 사람이 보기에 이미지를 더 많이 변화시키는 함수를 강하다고 생각합니다. 예를들어 x축 반전은 매우 약한 augmentation이고, 그에 비해 Gaussian Noise는 더 강한 augmentation입니다. 일정 확률로 30도를 돌리고 색을 반전하는 함수는 이보다 더 강한 augmentation이 될 것입니다. 

이 논문에서는 strong augmentation으로 `RandAugment` {% cite RandAugment %} 와 `CTAugment` {% cite CTAugment %} 를, weak augmentation 으로는 flip과 shift를 사용합니다. RandAugment나 CTAugment는 다양한 augmentation들 중 몇가지를 매번 randomly 선택하여 적용하는 기법으로, 


### Method
이미지 $X$에 대해, 일반적인 

$$Pr(h(\mathbf{X}) \le b) \approx \frac{1}{N} \sum_{i=1}^N I(h(\mathbf{X}) \le b)$$

### References
{% bibliography --cited_in_order --template short_bib --group_by none %}
